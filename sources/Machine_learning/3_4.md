\newpage

## 12. Clustering Algorithms

Clustering algorithms form a critical component of unsupervised learning, providing powerful tools for uncovering hidden structures in unlabelled data. In this chapter, we delve into two foundational clustering techniques: K-Means Clustering and Hierarchical Clusteringâ€”each offering unique approaches to grouping data points based on similarity. K-Means, renowned for its simplicity and efficiency, partitions the dataset into K clusters by minimizing intra-cluster variance. Conversely, Hierarchical Clustering builds nested clusters by recursively merging or splitting them based on distance metrics, enabling the discovery of multi-level data hierarchies. We will discuss the theoretical underpinnings of these algorithms, followed by a detailed exploration of their implementation and optimization in C++, ensuring that you gain practical insights into the nuances of clustering large, complex datasets.

### K-Means Clustering

K-Means Clustering is one of the most fundamental and widely-used clustering algorithms in machine learning. It is particularly useful in scenarios where we seek to partition a dataset into K distinct, non-overlapping clusters such that the internal cohesion of clusters is maximized and the separation between clusters is minimized. This section covers the theoretical foundations, algorithmic intricacies, parameter initialization techniques, convergence criteria, and potential optimization strategies of K-Means Clustering. Furthermore, we will also explore its practical implementation in C++.

#### Theoretical Foundations

K-Means Clustering aims to partition a set of 'n' data points $\{x_1, x_2, ..., x_n\}$ into 'K' clusters $\{C_1, C_2, ..., C_K\}$ such that the cumulative Euclidean distance between data points and their corresponding cluster centroids is minimized. Formally, the objective is to minimize the following cost function or distortion function:

$$ J = \sum_{i=1}^{K} \sum_{x \in C_i} \|x - \mu_i\|^2 $$

where $\mu_i$ is the centroid of cluster $C_i$, and $\|x - \mu_i\|^2$ represents the squared Euclidean distance between a data point $x$ and centroid $\mu_i$.

#### Algorithmic Steps

The K-Means algorithm follows an iterative process comprising the following key steps:

1. **Initialization**: Randomly select 'K' initial centroids from the dataset.
2. **Assignment Step**: Assign each data point to the nearest centroid based on the Euclidean distance.
3. **Update Step**: Recompute the centroids as the mean of all data points assigned to each cluster.
4. **Convergence Check**: Repeat the assignment and update steps until the centroids do not change significantly or a predefined number of iterations is reached.

#### Detailed Breakdown

1. **Initialization**:
   - The choice of initial centroids plays a crucial role in the convergence and final clustering solution of the algorithm. Several initialization methods include:
     - **Random Initialization**: Randomly pick 'K' data points from the dataset as initial centroids.
     - **K-Means++ Initialization**: An enhanced method that improves the chances of finding better initial centroids, thereby accelerating the convergence.

2. **Assignment Step**:
   - For each data point $x$, compute the distance to all centroids $\mu_k$ and assign $x$ to the cluster with the nearest centroid.
   - Mathematically, a data point $x_i$ is assigned to cluster $C_j$ if:
     $$
     C_j = \arg\min_{k} \|x_i - \mu_k\|^2
     $$

3. **Update Step**:
   - Update the centroids to the mean position of all data points in each cluster. For a cluster $C_j$, the new centroid $\mu_j$ is given by:
     $$
     \mu_j = \frac{1}{|C_j|} \sum_{x \in C_j} x
     $$

4. **Convergence Check**:
   - The algorithm converges when there is no significant change in the centroids, which can be measured using a distance threshold $\epsilon$ or when the maximum number of iterations $T$ is reached.

#### Practical Considerations and Optimization Strategies

- **Handling Empty Clusters**: During the iterations, it is possible for some clusters to become empty. One approach to combat this is to reinitialize the empty cluster's centroid to a random data point.
- **Algorithm Complexity**: The computational complexity of K-Means is $O(n \cdot K \cdot t \cdot d)$, where 'n' is the number of data points, 'K' is the number of clusters, 't' is the number of iterations, and 'd' is the dimensionality of the data. Efficiency can be improved through methods like using KD-Trees or Ball Trees for faster nearest centroid searches.
- **Elbow Method for Optimal K**: Determining the optimal number of clusters 'K' is non-trivial. The Elbow Method involves plotting the distortion function $J$ as a function of 'K' and identifying the 'elbow point' where the decrease in distortion starts to diminish.
- **Silhouette Score**: Another metric for assessing the quality of clustering is the Silhouette Score, which takes into account both inter-cluster and intra-cluster distances. A higher Silhouette Score indicates a better-defined clustering.

#### Implementation in C++

Below is an implementation outline for K-Means Clustering in C++:

```cpp
#include <iostream>
#include <vector>
#include <cmath>
#include <limits>
#include <cstdlib> // For rand and srand
#include <ctime>   // For time

// Define a point class to represent data points
class Point {
public:
    std::vector<double> coordinates;
    
    Point(int dimensions) : coordinates(dimensions, 0.0) {}

    double distance(const Point& other) const {
        double sum = 0.0;
        for (size_t i = 0; i < coordinates.size(); ++i) {
            sum += std::pow(coordinates[i] - other.coordinates[i], 2);
        }
        return std::sqrt(sum);
    }

    Point operator+(const Point& other) const {
        Point result(coordinates.size());
        for (size_t i = 0; i < coordinates.size(); ++i) {
            result.coordinates[i] = coordinates[i] + other.coordinates[i];
        }
        return result;
    }

    Point operator/(double divisor) const {
        Point result(coordinates.size());
        for (size_t i = 0; i < coordinates.size(); ++i) {
            result.coordinates[i] = coordinates[i] / divisor;
        }
        return result;
    }
};

// Function to initialize centroids randomly
std::vector<Point> initializeCentroids(const std::vector<Point>& data, int K) {
    std::srand(std::time(0));
    std::vector<Point> centroids(K, Point(data[0].coordinates.size()));
    for (int i = 0; i < K; ++i) {
        centroids[i] = data[std::rand() % data.size()];
    }
    return centroids;
}

// K-Means clustering function
void kMeans(std::vector<Point>& data, int K, int maxIterations) {
    int dimensions = data[0].coordinates.size();
    std::vector<Point> centroids = initializeCentroids(data, K);
    std::vector<int> assignments(data.size());

    for (int iter = 0; iter < maxIterations; ++iter) {
        // Assignment Step
        for (size_t i = 0; i < data.size(); ++i) {
            double min_distance = std::numeric_limits<double>::max();
            int best_cluster = -1;
            for (int j = 0; j < K; ++j) {
                double dist = data[i].distance(centroids[j]);
                if (dist < min_distance) {
                    min_distance = dist;
                    best_cluster = j;
                }
            }
            assignments[i] = best_cluster;
        }

        // Update Step
        std::vector<Point> new_centroids(K, Point(dimensions));
        std::vector<int> points_in_cluster(K, 0);
        for (size_t i = 0; i < data.size(); ++i) {
            int cluster = assignments[i];
            new_centroids[cluster] = new_centroids[cluster] + data[i];
            points_in_cluster[cluster]++;
        }
        
        for (int j = 0; j < K; ++j) {
            if (points_in_cluster[j] > 0) {
                new_centroids[j] = new_centroids[j] / points_in_cluster[j];
            }
        }

        centroids = std::move(new_centroids);
    }
}

int main() {
    // Example data with 2 dimensions
    std::vector<Point> data = { {Point({1.0, 2.0})}, {Point({2.0, 3.0})}, {Point({3.0, 4.0})}, {Point({5.0, 8.0})}, {Point({8.0, 8.0})} };
    int K = 2; // Number of clusters
    int maxIterations = 100; // Maximum number of iterations
    
    kMeans(data, K, maxIterations);
    
    std::cout << "K-Means clustering completed." << std::endl;
    
    return 0;
}
```

This C++ code provides a basic K-Means implementation, focusing on the core steps of initialization, assignment, and update. It uses Euclidean distance for cluster assignments and updates centroids as the mean of points in each cluster.

#### Conclusion

K-Means Clustering, with its simplicity and efficiency, remains a robust choice for partitioning datasets into meaningful clusters. Despite its sensitivity to initial conditions and fixed number of clusters, significant research and practical methods such as K-Means++ and the Elbow Method have enhanced its usability and effectiveness. By understanding its theoretical foundations and engaging with its practical implementation in C++, one can harness the power of K-Means to uncover hidden structures and insights in diverse datasets.

### Hierarchical Clustering

Hierarchical Clustering is a powerful and intuitive method for analyzing and understanding the nested structures in data. Unlike partition-based clustering techniques like K-Means, which require the number of clusters to be predefined, Hierarchical Clustering creates a multilevel hierarchy of clusters, represented as a tree or dendrogram. This subchapter will delve into the theoretical underpinnings, types, distance methods, algorithmic steps, and practical considerations of Hierarchical Clustering, followed by a detailed implementation in C++.

#### Theoretical Foundations

Hierarchical Clustering produces a hierarchy of clusters that range from individual data points to a single cluster containing all data points. The resulting structure is a tree known as a "dendrogram," which visually represents the nested grouping of data points.

The primary objective of Hierarchical Clustering is to discover the underlying cluster structure without requiring a pre-specified number of clusters. This technique can be broadly divided into two types:

1. **Agglomerative Hierarchical Clustering**: This is a "bottom-up" approach where each data point starts in its own cluster, and pairs of clusters are merged sequentially until a single cluster contains all data points.
2. **Divisive Hierarchical Clustering**: This is a "top-down" approach where all data points start in one cluster, and clusters are split recursively until each cluster contains a single data point.

Given its intuitive nature and ability to reveal nested structures, Agglomerative Hierarchical Clustering is more commonly used and will be the focus of this subchapter.

#### Types of Hierarchical Clustering:

1. **Agglomerative Hierarchical Clustering (AHC)**:
   - Starts with each data point as an individual cluster.
   - Merges the closest pair of clusters iteratively.
   - Continues until all data points form a single cluster.

2. **Divisive Hierarchical Clustering (DHC)**:
   - Starts with all data points in a single cluster.
   - Splits the most heterogeneous cluster iteratively.
   - Continues until each data point forms an individual cluster.
   
#### Distance Methods

The measure of similarity or dissimilarity between clusters is a crucial aspect of Hierarchical Clustering. Several methods can be used to determine the distance between clusters:

1. **Single Linkage (Minimum Linkage)**: Uses the smallest distance between any single pair of data points from two clusters. It tends to create long, chain-like clusters.
   $$
   d(C_i, C_j) = \min_{x \in C_i, y \in C_j} \|x - y\|
   $$

2. **Complete Linkage (Maximum Linkage)**: Uses the largest distance between any single pair of data points from two clusters. It tends to create more compact clusters.
   $$
   d(C_i, C_j) = \max_{x \in C_i, y \in C_j} \|x - y\|
   $$

3. **Average Linkage**: Uses the average distance between all pairs of data points from two clusters.
   $$
   d(C_i, C_j) = \frac{1}{|C_i| \times |C_j|} \sum_{x \in C_i} \sum_{y \in C_j} \|x - y\|
   $$

4. **Centroid Linkage**: Uses the distance between the centroids of two clusters.
   $$
   d(C_i, C_j) = \|\mu_i - \mu_j\|
   $$
   where $\mu_i$ and $\mu_j$ are the centroids of clusters $C_i$ and $C_j$, respectively.

5. **Ward's Method**: Minimizes the total within-cluster variance. At each step, the pair of clusters that leads to the minimum increase in total within-cluster variance after merging is chosen.
   $$
   d(C_i, C_j) = \frac{|C_i| \cdot |C_j|}{|C_i| + |C_j|} \| \mu_i - \mu_j \|^2
   $$
   
#### Algorithmic Steps

The general process of Agglomerative Hierarchical Clustering involves the following key steps:

1. **Initialization**:
   - Start with each data point as an individual cluster.
   - Compute the distance matrix for all pairs of clusters.

2. **Iteration**:
   - Identify the pair of clusters with the smallest distance.
   - Merge the identified pair into a single cluster.
   - Update the distance matrix to reflect the new cluster structure.

3. **Termination**:
   - Repeat the iteration step until only a single cluster remains, encompassing all data points.

The resulting dendrogram is then cut at a desired level to obtain the final clustering solution.

#### Detailed Breakdown

1. **Initialization**:
   - Given a dataset $\{x_1, x_2, ..., x_n\}$, initialize each data point as a separate cluster.
   - Compute the initial distance matrix $D$, where $D(i, j)$ represents the distance between data points $x_i$ and $x_j$.

2. **Iteration**:
   - Find the closest pair of clusters based on the selected linkage method.
   - Merge the closest pair of clusters $( C_i, C_j)$ into a single cluster $C_k$.
   - Update the distance matrix:
     - For single linkage:
       $$
       d(C_k, C_l) = \min(d(C_i, C_l), d(C_j, C_l))
       $$
     - For complete linkage:
       $$
       d(C_k, C_l) = \max(d(C_i, C_l), d(C_j, C_l))
       $$
     - For average linkage:
       $$
       d(C_k, C_l) = \frac{|C_i| d(C_i, C_l) + |C_j| d(C_j, C_l)}{|C_i| + |C_j|}
       $$

3. **Termination**:
   - The iteration continues until the distance matrix reflects a single cluster containing all data points.
   - The hierarchical clustering process is represented as a dendrogram, which can be sliced at different levels to obtain the desired number of clusters based on the specified granularity.

#### Practical Considerations and Optimization Strategies

- **Computational Complexity**: The naive implementation of Agglomerative Hierarchical Clustering has a time complexity of $O(n^3)$ and a space complexity of $O(n^2)$. Optimizations using data structures like priority queues and efficient distance matrix updates can reduce the complexity.
- **Scalability**: For very large datasets, traditional Hierarchical Clustering methods may become infeasible. Strategies such as using a truncated dendrogram, sampling, or hybrid methods combining partition-based clustering with hierarchical clustering can help manage scalability issues.
- **Cluster Interpretability**: The dendrogram provides a rich, visual representation of the clustering hierarchy. However, interpreting which level of the dendrogram corresponds to meaningful clusters often requires domain knowledge or criteria such as the inconsistency coefficient.

#### Implementation in C++

Below is an implementation outline for Agglomerative Hierarchical Clustering using Single Linkage in C++:

```cpp
#include <iostream>
#include <vector>
#include <cmath>
#include <limits>
#include <algorithm>
#include <utility>

// Define a point class to represent data points
class Point {
public:
    std::vector<double> coordinates;
    
    Point(int dimensions) : coordinates(dimensions, 0.0) {}

    double distance(const Point& other) const {
        double sum = 0.0;
        for (size_t i = 0; i < coordinates.size(); ++i) {
            sum += std::pow(coordinates[i] - other.coordinates[i], 2);
        }
        return std::sqrt(sum);
    }
};

// Hierarchical Clustering using Single Linkage method
void hierarchicalClustering(std::vector<Point>& data) {
    int n = data.size();
    std::vector<std::vector<double>> distanceMatrix(n, std::vector<double>(n, 0.0));
    std::vector<int> clusterAssignment(n);
    
    // Initialize distance matrix
    for (int i = 0; i < n; ++i) {
        clusterAssignment[i] = i;
        for (int j = i + 1; j < n; ++j) {
            double dist = data[i].distance(data[j]);
            distanceMatrix[i][j] = dist;
            distanceMatrix[j][i] = dist;
        }
    }
    
    // Perform agglomerative clustering
    for (int step = 0; step < n - 1; ++step) {
        double minDistance = std::numeric_limits<double>::max();
        std::pair<int, int> toMerge = {-1, -1};
        
        // Find the closest pair of clusters
        for (int i = 0; i < n; ++i) {
            for (int j = i + 1; j < n; ++j) {
                if (clusterAssignment[i] != clusterAssignment[j] && distanceMatrix[i][j] < minDistance) {
                    minDistance = distanceMatrix[i][j];
                    toMerge = {i, j};
                }
            }
        }

        // Merge the closest pair of clusters
        int cluster1 = clusterAssignment[toMerge.first];
        int cluster2 = clusterAssignment[toMerge.second];
        for (int i = 0; i < n; ++i) {
            if (clusterAssignment[i] == cluster2) {
                clusterAssignment[i] = cluster1;
            }
        }

        // Update distance matrix
        for (int i = 0; i < n; ++i) {
            if (clusterAssignment[i] == cluster1 || clusterAssignment[i] == cluster2) {
                distanceMatrix[toMerge.first][i] = std::min(distanceMatrix[toMerge.first][i], distanceMatrix[toMerge.second][i]);
                distanceMatrix[i][toMerge.first] = distanceMatrix[toMerge.first][i];
            }
        }
    }
    
    // Output cluster assignments
    for (int i = 0; i < n; ++i) {
        std::cout << "Point " << i << " is in cluster " << clusterAssignment[i] << std::endl;
    }
}

int main() {
    // Example data with 2 dimensions
    std::vector<Point> data = { { Point({1.0, 2.0}) }, { Point({2.0, 3.0}) }, { Point({3.0, 4.0}) }, { Point({5.0, 8.0}) }, { Point({8.0, 8.0}) } };
    
    hierarchicalClustering(data);
    
    return 0;
}
```

This C++ code provides a basic implementation for Agglomerative Hierarchical Clustering using the Single Linkage method. It initializes the distance matrix, identifies the closest pair of clusters, merges them, updates the distance matrix, and finally outputs the cluster assignments.

#### Conclusion

Hierarchical Clustering, with its ability to uncover multilevel structures and provide an intuitive representation through dendrograms, is a versatile tool in machine learning. By understanding its theoretical foundations, types of clustering, distance methods, algorithmic steps, and practical considerations, one can effectively utilize hierarchical techniques to analyze complex datasets. The detailed C++ implementation serves as a practical guide to operationalizing these concepts, enabling meaningful insights into the underlying cluster structures.

### Implementation and Optimization in C++

Implementing and optimizing clustering algorithms in C++ involves not just understanding the theoretical aspects but also having a keen eye for practical efficiency and resource management. This subchapter aims to provide a comprehensive guide on the implementation and optimization of both K-Means and Hierarchical Clustering algorithms in C++. We will explore key techniques such as efficient data structures, parallel processing, and distance computations. Additionally, we will delve into profiling and benchmarking to ensure that our implementations are both performant and scalable.

#### K-Means Clustering Implementation

K-Means Clustering divides data into K clusters by iteratively updating cluster centroids to minimize within-cluster variance. The basic steps include initialization, assignment, updating, and checking for convergence.

##### Data Structures

Efficient implementation of K-Means requires choosing the right data structures:
- **Point Class**: Represents data points and supports basic distance calculations.
- **Cluster Class**: Maintains centroid coordinates and assigned data points.
- **Utilities**: Functions for initializing centroids, updating centroids, and computing distances.

Here is a refined Point class:

```cpp
#include <iostream>
#include <vector>
#include <cmath>

class Point {
public:
    std::vector<double> coordinates;
    
    Point(int dimensions) : coordinates(dimensions, 0.0) {}

    double distance(const Point& other) const {
        double sum = 0.0;
        for (size_t i = 0; i < coordinates.size(); ++i) {
            sum += std::pow(coordinates[i] - other.coordinates[i], 2);
        }
        return std::sqrt(sum);
    }
};
```

##### Efficient Initialization

The choice of initial centroids greatly impacts the algorithm's performance. K-Means++ is an improved way to initialize centroids, ensuring they are more widely spread.

```cpp
std::vector<Point> initializeCentroids(const std::vector<Point>& data, int K) {
    std::srand(std::time(0)); // Initialize random seed
    std::vector<Point> centroids;
    centroids.push_back(data[std::rand() % data.size()]);

    for (int k = 1; k < K; ++k) {
        std::vector<double> distances(data.size(), std::numeric_limits<double>::max());

        for (size_t i = 0; i < data.size(); ++i) {
            for (const Point& centroid : centroids) {
                double dist = data[i].distance(centroid);
                if (dist < distances[i]) {
                    distances[i] = dist;
                }
            }
        }

        double sum = 0;
        for (double dist : distances) {
            sum += dist;
        }

        double r = ((double) std::rand() / (RAND_MAX)) * sum;
        sum = 0;

        for (size_t i = 0; i < data.size(); ++i) {
            sum += distances[i];
            if (sum >= r) {
                centroids.push_back(data[i]);
                break;
            }
        }
    }
    return centroids;
}
```

##### Parallel Processing with OpenMP

K-Means involves iterating over large datasets, making it suitable for parallel processing. `OpenMP` can be used to parallelize the assignment and update steps.

```cpp
#include <omp.h>

void kMeans(std::vector<Point>& data, int K, int maxIterations) {
    int dimensions = data[0].coordinates.size();
    std::vector<Point> centroids = initializeCentroids(data, K);
    std::vector<int> assignments(data.size());
    std::vector<int> points_in_cluster(K, 0);
    
    for (int iter = 0; iter < maxIterations; ++iter) {
        // Assignment Step
        #pragma omp parallel for
        for (size_t i = 0; i < data.size(); ++i) {
            double min_distance = std::numeric_limits<double>::max();
            int best_cluster = -1;
            for (int j = 0; j < K; ++j) {
                double dist = data[i].distance(centroids[j]);
                if (dist < min_distance) {
                    min_distance = dist;
                    best_cluster = j;
                }
            }
            assignments[i] = best_cluster;
        }
        
        // Reset cluster data
        std::fill(points_in_cluster.begin(), points_in_cluster.end(), 0);
        std::vector<Point> new_centroids(K, Point(dimensions));
        
        // Update Step
        #pragma omp parallel for
        for (size_t i = 0; i < data.size(); ++i) {
            int cluster = assignments[i];
            #pragma omp critical
            {
                for (int d = 0; d < dimensions; ++d) {
                    new_centroids[cluster].coordinates[d] += data[i].coordinates[d];
                }
                points_in_cluster[cluster]++;
            }
        }

        #pragma omp parallel for
        for (int j = 0; j < K; ++j) {
            if (points_in_cluster[j] > 0) {
                for (int d = 0; d < dimensions; ++d) {
                    new_centroids[j].coordinates[d] /= points_in_cluster[j];
                }
            }
        }
        centroids = new_centroids;
    }
}
```

##### Convergence Criteria

Convergence can be checked by measuring shifts in centroid positions or by setting a maximum number of iterations. This ensures the algorithm does not run indefinitely.

```cpp
bool hasConverged(const std::vector<Point>& oldCentroids, const std::vector<Point>& newCentroids, double tolerance) {
    for (size_t i = 0; i < oldCentroids.size(); i++) {
        if (oldCentroids[i].distance(newCentroids[i]) > tolerance) {
            return false;
        }
    }
    return true;
}
```

#### Hierarchical Clustering Implementation

Hierarchical Clustering produces a nested hierarchy of clusters without needing to predefine the number of clusters. The critical components of its implementation include distance computation, efficient merging, and the construction of a dendrogram.

#### Data Structures

Efficient data handling is crucial. We need to maintain a list of clusters, a distance matrix, and bookkeeping for merged clusters.

```cpp
#include <vector>
#include <cmath>
#include <limits>
#include <iostream>

class Point {
public:
    std::vector<double> coordinates;

    Point(int dimensions) : coordinates(dimensions, 0.0) {}

    double distance(const Point& other) const {
        double sum = 0.0;
        for (size_t i = 0; i < coordinates.size(); ++i) {
            sum += std::pow(coordinates[i] - other.coordinates[i], 2);
        }
        return std::sqrt(sum);
    }
};
```

#### Single-Linkage Method

The Single-Linkage method (minimum distance) is one of the simplest and most intuitive hierarchical clustering methods. It merges clusters based on the smallest distance between any two points in each cluster.

```cpp
void hierarchicalClustering(std::vector<Point>& data) {
    int n = data.size();
    std::vector<std::vector<double>> distanceMatrix(n, std::vector<double>(n, 0.0));
    std::vector<int> clusterAssignment(n);
    
    // Initialize distance matrix
    for (int i = 0; i < n; ++i) {
        clusterAssignment[i] = i;
        for (int j = i + 1; j < n; ++j) {
            double dist = data[i].distance(data[j]);
            distanceMatrix[i][j] = dist;
            distanceMatrix[j][i] = dist;
        }
    }

    // Perform agglomerative clustering
    for (int step = 0; step < n - 1; ++step) {
        double minDistance = std::numeric_limits<double>::max();
        std::pair<int, int> toMerge = {-1, -1};

        // Find the closest pair of clusters
        for (int i = 0; i < n; ++i) {
            for (int j = i + 1; j < n; ++j) {
                if (clusterAssignment[i] != clusterAssignment[j] &&
                    distanceMatrix[i][j] < minDistance) {
                    minDistance = distanceMatrix[i][j];
                    toMerge = {i, j};
                }
            }
        }

        // Merge the closest pair of clusters
        int cluster1 = clusterAssignment[toMerge.first];
        int cluster2 = clusterAssignment[toMerge.second];
        for (int i = 0; i < n; ++i) {
            if (clusterAssignment[i] == cluster2) {
                clusterAssignment[i] = cluster1;
            }
        }

        // Update distance matrix
        for (int i = 0; i < n; ++i) {
            if (clusterAssignment[i] == cluster1 || clusterAssignment[i] == cluster2) {
                distanceMatrix[toMerge.first][i] =
                    std::min(distanceMatrix[toMerge.first][i], distanceMatrix[toMerge.second][i]);
                distanceMatrix[i][toMerge.first] = distanceMatrix[toMerge.first][i];
            }
        }
    }

    // Output cluster assignments
    for (int i = 0; i < n; ++i) {
        std::cout << "Point " << i << " is in cluster " << clusterAssignment[i] << std::endl;
    }
}

int main() {
    // Example data with 2 dimensions
    std::vector<Point> data = { { Point({1.0, 2.0}) }, { Point({2.0, 3.0}) }, { Point({3.0, 4.0}) }, { Point({5.0, 8.0}) }, { Point({8.0, 8.0}) } };
    
    hierarchicalClustering(data);
    
    return 0;
}
```

#### Dendrogram Construction

A crucial part of Hierarchical Clustering is constructing a dendrogram to represent the nested cluster hierarchy visually. Each merge operation creates a new node linking two subclusters.

#### Optimization Techniques

Optimizing clustering algorithms in C++ involves various strategies to enhance efficiency and scalability.

#### Using Efficient Data Structures

1. **Priority Queues**: Efficiently manage and access the smallest distances for clustering operations.
2. **KD-Trees**: Accelerate nearest neighbor searches in high-dimensional spaces, useful in K-Means clustering.

#### Parallel and Distributed Computing

1. **OpenMP**: Parallelize loop iterations in the assignment and update steps of K-Means clustering.
2. **MPI**: Distribute data across multiple processors or machines to handle large datasets.

#### Profiling and Benchmarking

1. **Gprof**: Profile the C++ code to identify bottlenecks and optimize them.
2. **Benchmarking Tools**: Compare different implementations to evaluate performance improvements.

#### Conclusion

Implementing and optimizing clustering algorithms in C++ requires a deep understanding of both the algorithms and efficient coding practices. Using appropriate data structures, parallel processing, and profiling tools can significantly enhance the performance of K-Means and Hierarchical Clustering algorithms. By meticulously implementing and fine-tuning these techniques, we can process large datasets efficiently and uncover valuable insights through clustering.

