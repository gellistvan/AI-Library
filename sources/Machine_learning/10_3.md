\newpage

## Appendix C: Example Code and Exercises

### Sample Programs Demonstrating Key Concepts

In this section, we will delve into several sample programs to concretely demonstrate key machine learning concepts and techniques discussed throughout this book. These examples not only illustrate the implementation details but also provide insights into the underlying mathematical foundations, algorithmic intricacies, and performance considerations. The primary focus will be on the C++ programming language, owing to its fine control over system resources and efficiency, which are critical for large-scale machine learning applications.

#### 1. Linear Regression

Linear regression is one of the most fundamental algorithms in machine learning. It models the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data.

**Mathematical Foundation:**

The linear regression model can be described as:

$$ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_n x_n + \epsilon $$

where:
- $y$ is the dependent variable.
- $x_1, x_2, \ldots, x_n$ are the independent variables.
- $\beta_0, \beta_1, \ldots, \beta_n$ are the coefficients.
- $\epsilon$ is the error term.

The coefficients are typically estimated using the Least Squares method, which minimizes the sum of the squared errors between the observed and predicted values.

**Implementation:**

The following is an implementation of linear regression using C++.

```cpp
#include <iostream>
#include <vector>
#include <numeric>
#include <cmath>

class LinearRegression {
private:
    std::vector<double> X, Y;
    double B0, B1;
    
public:
    LinearRegression(const std::vector<double> &x, const std::vector<double> &y) : X(x), Y(y), B0(0), B1(0) {}

    void fit() {
        double x_mean = std::accumulate(X.begin(), X.end(), 0.0) / X.size();
        double y_mean = std::accumulate(Y.begin(), Y.end(), 0.0) / Y.size();

        double num = 0, denom = 0;
        for(size_t i = 0; i < X.size(); ++i) {
            num += (X[i] - x_mean) * (Y[i] - y_mean);
            denom += std::pow(X[i] - x_mean, 2);
        }
        B1 = num / denom;
        B0 = y_mean - B1 * x_mean;
    }

    double predict(double x) {
        return B0 + B1 * x;
    }
};

int main() {
    std::vector<double> X = {1, 2, 3, 4, 5};
    std::vector<double> Y = {2, 3, 5, 7, 11};

    LinearRegression model(X, Y);
    model.fit();

    double prediction = model.predict(6);
    std::cout << "Prediction for X=6: " << prediction << std::endl;

    return 0;
}
```

**Explanation:**
- We first define a `LinearRegression` class that takes two vectors `X` (independent variable) and `Y` (dependent variable).
- The `fit` method computes the coefficients $\beta_0$ and $\beta_1$ using the least squares method.
- The `predict` method uses the computed coefficients to predict the value for a given input $x$.

#### 2. Logistic Regression

Logistic regression is a classification algorithm that models the probability of a binary outcome based on one or more predictor variables.

**Mathematical Foundation:**

Logistic regression is based on the logistic function (sigmoid function), which can be expressed as:

$$ \sigma(z) = \frac{1}{1 + e^{-z}} $$

For a binary classification problem, the logistic regression model can be defined as:

$$ p(X) = \sigma(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_n x_n) $$

where $p(X)$ represents the probability that the predicted outcome is 1 (and $1 - p(X)$ is the probability that it is 0).

**Implementation:**

Below is a logistic regression implementation in C++.

```cpp
#include <iostream>
#include <vector>
#include <cmath>

class LogisticRegression {
private:
    std::vector<double> X;
    std::vector<int> Y;
    double B0, B1;
    double learning_rate;
    int iterations;

    double sigmoid(double z) {
        return 1.0 / (1.0 + exp(-z));
    }

    void gradient_descent() {
        for (int i = 0; i < iterations; ++i) {
            double gradient_B0 = 0;
            double gradient_B1 = 0;
            for (size_t j = 0; j < X.size(); ++j) {
                double prediction = sigmoid(B0 + B1 * X[j]);
                gradient_B0 += (prediction - Y[j]);
                gradient_B1 += (prediction - Y[j]) * X[j];
            }
            B0 -= learning_rate * gradient_B0;
            B1 -= learning_rate * gradient_B1;
        }
    }

public:
    LogisticRegression(const std::vector<double> &x, const std::vector<int> &y, double lr, int iters) 
        : X(x), Y(y), B0(0), B1(0), learning_rate(lr), iterations(iters) {}

    void fit() {
        gradient_descent();
    }

    double predict(double x) {
        return sigmoid(B0 + B1 * x);
    }
};

int main() {
    std::vector<double> X = {0.5, 1.5, 2.5, 3.5, 4.5};
    std::vector<int> Y = {0, 0, 1, 1, 1};

    LogisticRegression model(X, Y, 0.1, 1000);
    model.fit();

    double prediction = model.predict(2.0);
    std::cout << "Prediction for X=2.0: " << prediction << std::endl;

    return 0;
}
```

**Explanation:**
- We define a `LogisticRegression` class that takes vectors `X` and `Y`, as well as learning rate and number of iterations.
- The `sigmoid` method computes the sigmoid function.
- The `gradient_descent` method updates the coefficients $\beta_0$ and $\beta_1$ using gradient descent.
- The `predict` method computes the probability for a given input value.

#### 3. K-Nearest Neighbors (KNN)

K-Nearest Neighbors is a simple, non-parametric, and lazy learning algorithm used for classification and regression. In classification, it assigns a class to a sample based on the majority class among its k nearest neighbors.

**Mathematical Foundation:**

Given a new data point $x$, the KNN algorithm identifies the k-nearest neighbors from the training set based on a chosen distance metric (e.g., Euclidean distance). The class that is most common among these neighbors is assigned to the data point.

**Implementation:**

Here's an implementation of the KNN algorithm in C++.

```cpp
#include <iostream>
#include <vector>
#include <cmath>
#include <algorithm>

struct DataPoint {
    double x;
    int label;

    DataPoint(double x_val, int y_val) : x(x_val), label(y_val) {}
};

class KNN {
private:
    std::vector<DataPoint> data;
    int k;

    double distance(double a, double b) {
        return std::abs(a - b);
    }

    int classify(const std::vector<int> &neighbor_labels) {
        int count_0 = std::count(neighbor_labels.begin(), neighbor_labels.end(), 0);
        int count_1 = std::count(neighbor_labels.begin(), neighbor_labels.end(), 1);
        return (count_0 > count_1) ? 0 : 1;
    }

public:
    KNN(const std::vector<DataPoint> &training_data, int k_neighbors) 
        : data(training_data), k(k_neighbors) {}
        
    int predict(double x) {
        std::vector<std::pair<double, int>> distances;

        for (const auto &point : data) {
            double dist = distance(x, point.x);
            distances.push_back(std::make_pair(dist, point.label));
        }

        std::sort(distances.begin(), distances.end());

        std::vector<int> neighbor_labels;
        for (int i = 0; i < k; ++i) {
            neighbor_labels.push_back(distances[i].second);
        }

        return classify(neighbor_labels);
    }
};

int main() {
    std::vector<DataPoint> training_data = {
        DataPoint(1.0, 0),
        DataPoint(1.5, 0),
        DataPoint(2.0, 1),
        DataPoint(2.5, 1),
        DataPoint(3.0, 1),
    };

    KNN model(training_data, 3);
    int prediction = model.predict(2.0);
    std::cout << "Prediction for X=2.0: " << prediction << std::endl;

    return 0;
}
```

**Explanation:**
- We create a `DataPoint` struct to hold individual data points and their labels.
- The `KNN` class includes methods to calculate the distance, classify based on neighbor labels, and predict the class for a new data point.
- In the `predict` method, we find the k-nearest neighbors and classify based on the majority class among them.


### Exercises for Practice

The following exercises are designed to reinforce the concepts explained throughout the book and to deepen your understanding of machine learning algorithms and their implementation in C++. Each exercise is accompanied by a detailed description of the task, its objectives, and the expected outcomes. Where applicable, example code snippets in C++ are provided to guide you through the implementation. These exercises range from basic to advanced levels, catering to both beginners and experienced practitioners.

#### 1. Implementing Polynomial Regression

**Objective:**
To extend linear regression to handle non-linear relationships by implementing polynomial regression. 

**Description:**
Polynomial regression models the relationship between a dependent variable and an independent variable as an $n$-th degree polynomial. You are required to implement a polynomial regression model and fit it to a given dataset.

**Steps:**
1. Generate a dataset that exhibits a non-linear relationship. For instance, use a quadratic relationship between $x$ and $y$.
2. Extend the `LinearRegression` class to handle polynomial features. Create additional features such as $x^2, x^3, \ldots$ based on the desired degree of the polynomial.
3. Fit the polynomial regression model to the dataset using the least squares method.
4. Predict values and visualize the fitted polynomial curve.

**Example Code:**

```cpp
#include <iostream>
#include <vector>
#include <algorithm>
#include <cmath>

class PolynomialRegression {
private:
    std::vector<double> X, Y;
    std::vector<double> coefficients;
    int degree;

    std::vector<double> create_polynomial_features(const std::vector<double>& x) {
        std::vector<double> poly_features;
        for (double xi : x) {
            for (int i = 0; i <= degree; ++i) {
                poly_features.push_back(std::pow(xi, i));
            }
        }
        return poly_features;
    }

    void fit() {
        // Implementation to fit polynomial regression
        // This involves solving a set of simultaneous linear equations to find the coefficients
    }

public:
    PolynomialRegression(const std::vector<double>& x, const std::vector<double>& y, int deg) 
        : X(x), Y(y), degree(deg) {}
    
    void train() {
        fit();
    }

    double predict(double x) {
        double result = 0.0;
        for (int i = 0; i <= degree; ++i) {
            result += coefficients[i] * std::pow(x, i);
        }
        return result;
    }
};

int main() {
    std::vector<double> X = {1, 2, 3, 4, 5};
    std::vector<double> Y = {2, 8, 18, 32, 50}; // Example quadratic relationship: Y = X^2 + X + 1

    PolynomialRegression model(X, Y, 2);
    model.train();

    double prediction = model.predict(6);
    std::cout << "Prediction for X=6: " << prediction << std::endl;

    return 0;
}
```

**Expected Outcome:**
The model should accurately predict the values for the given polynomial relationship. Visualize the results to confirm the fit.

#### 2. Implementing K-Means Clustering

**Objective:**
To implement the K-Means clustering algorithm and apply it to a dataset to identify clusters.

**Description:**
K-Means is an unsupervised learning algorithm used for clustering data into $k$ clusters. The challenge here is to implement the algorithm from scratch, execute it on a sample dataset, and visualize the clusters.

**Steps:**
1. Generate or use a sample dataset suitable for clustering.
2. Initialize $k$ centroids randomly.
3. Assign each data point to the nearest centroid to form clusters.
4. Update centroids by calculating the mean of all data points in each cluster.
5. Repeat the assignment and updating steps until convergence.
6. Visualize the final clusters.

**Example Code:**

```cpp
#include <iostream>
#include <vector>
#include <cmath>
#include <algorithm>
#include <random>

class KMeans {
private:
    std::vector<std::pair<double, double>> data;
    std::vector<std::pair<double, double>> centroids;
    int k;
    int max_iterations;

    double distance(std::pair<double, double> a, std::pair<double, double> b) {
        return std::sqrt(std::pow(a.first - b.first, 2) + std::pow(a.second - b.second, 2));
    }

    void initialize_centroids() {
        std::random_device rd;
        std::mt19937 gen(rd());
        std::uniform_int_distribution<> dis(0, data.size() - 1);

        centroids.clear();
        for (int i = 0; i < k; ++i) {
            centroids.push_back(data[dis(gen)]);
        }
    }

    std::vector<int> assign_clusters() {
        std::vector<int> assignments(data.size());
        for (size_t i = 0; i < data.size(); ++i) {
            double min_dist = std::numeric_limits<double>::max();
            int cluster = 0;
            for (int j = 0; j < k; ++j) {
                double dist = distance(data[i], centroids[j]);
                if (dist < min_dist) {
                    min_dist = dist;
                    cluster = j;
                }
            }
            assignments[i] = cluster;
        }
        return assignments;
    }

    void update_centroids(const std::vector<int>& assignments) {
        std::vector<std::pair<double, double>> new_centroids(k, {0.0, 0.0});
        std::vector<int> counts(k, 0);

        for (size_t i = 0; i < data.size(); ++i) {
            int cluster = assignments[i];
            new_centroids[cluster].first += data[i].first;
            new_centroids[cluster].second += data[i].second;
            counts[cluster]++;
        }

        for (int j = 0; j < k; ++j) {
            new_centroids[j].first /= counts[j];
            new_centroids[j].second /= counts[j];
        }

        centroids = new_centroids;
    }

public:
    KMeans(const std::vector<std::pair<double, double>>& data_points, int num_clusters, int max_iters)
        : data(data_points), k(num_clusters), max_iterations(max_iters) {}

    void fit() {
        initialize_centroids();

        for (int i = 0; i < max_iterations; ++i) {
            std::vector<int> assignments = assign_clusters();
            update_centroids(assignments);
        }
    }

    const std::vector<std::pair<double, double>>& get_centroids() const {
        return centroids;
    }
};

int main() {
    std::vector<std::pair<double, double>> data = {
        {1.0, 1.0}, {1.5, 2.0}, {3.0, 4.0}, {5.0, 7.0},
        {3.5, 5.0}, {4.5, 5.0}, {3.5, 4.5}
    };

    KMeans kmeans(data, 2, 100);
    kmeans.fit();

    const auto& centroids = kmeans.get_centroids();
    for (const auto& centroid : centroids) {
        std::cout << "Centroid: (" << centroid.first << ", " << centroid.second << ")\n";
    }

    return 0;
}
```

**Expected Outcome:**
The algorithm should correctly cluster the data points and provide the final positions of the centroids. Visualize the clusters to evaluate performance.

#### 3. Implementing Decision Trees for Classification

**Objective:**
To implement a decision tree algorithm for classification tasks.

**Description:**
Decision trees split the data at various points based on feature values to create branches that lead to classification outcomes. You are required to build a decision tree from scratch, train it on a dataset, and use it to make predictions.

**Steps:**
1. Load a binary classification dataset.
2. Implement functions for calculating information gain and Gini impurity.
3. Implement the recursive splitting process for building the tree.
4. Implement a prediction function.
5. Train the decision tree model on the dataset and evaluate its performance.

**Detailed Explanation:** 

- **Entropy and Information Gain:**
  Entropy measures the level of uncertainty or impurity in a dataset. It is given by:

  $$
  H(D) = -\sum_{i=1}^{c} p_i \log_2(p_i)
  $$

  where $p_i$ is the probability of class $i$ and $c$ is the number of classes.

  Information gain is the reduction in entropy from a split. It is calculated as:

  $$
  IG(D, A) = H(D) - \sum_{v \in A} \frac{|D_v|}{|D|} H(D_v)
  $$

  where $A$ is the attribute used for the split, $D_v$ is the subset of $D$ with attribute $A$ having value $v$.

- **Gini Impurity:**
  Gini impurity measures the probability of incorrect classification of a randomly chosen element if it were randomly labeled according to the distribution of labels in the dataset. It is given by:

  $$
  G(D) = \sum_{i=1}^{c} p_i (1 - p_i)
  $$

**Example Code:**

```cpp
#include <iostream>
#include <vector>
#include <algorithm>
#include <cmath>

struct TreeNode {
    bool is_leaf;
    int classification;
    int split_feature;
    double split_value;
    TreeNode* left;
    TreeNode* right;

    TreeNode() : is_leaf(true), classification(-1), split_feature(-1), split_value(0.0), left(nullptr), right(nullptr) {}
};

class DecisionTree {
private:
    struct DataPoint {
        std::vector<double> features;
        int label;
    };

    std::vector<DataPoint> data;
    TreeNode* root;

    double entropy(const std::vector<DataPoint>& subset) {
        int count1 = 0, count0 = 0;
        for (const auto& point : subset) {
            if (point.label == 1) count1++;
            else count0++;
        }
        double p1 = (double)count1 / subset.size();
        double p0 = 1 - p1;
        if (p1 == 0 || p0 == 0) return 0;
        return -p1 * std::log2(p1) - p0 * std::log2(p0);
    }

    double information_gain(const std::vector<DataPoint>& subset, int feature, double value) {
        std::vector<DataPoint> left, right;
        for (const auto& point : subset) {
            if (point.features[feature] < value) left.push_back(point);
            else right.push_back(point);
        }
        double subset_entropy = entropy(subset);
        double left_entropy = entropy(left);
        double right_entropy = entropy(right);
        return subset_entropy - ((double)left.size() / subset.size()) * left_entropy 
                               - ((double)right.size() / subset.size()) * right_entropy;
    }

    TreeNode* build_tree(const std::vector<DataPoint>& subset) {
        if (subset.empty()) return nullptr;
        double subset_entropy = entropy(subset);
        if (subset_entropy == 0) {
            TreeNode* leaf = new TreeNode();
            leaf->is_leaf = true;
            leaf->classification = subset[0].label;
            return leaf;
        }

        int best_feature = -1;
        double best_value = 0.0, best_ig = 0.0;
        for (size_t i = 0; i < subset[0].features.size(); ++i) {
            for (const auto& point : subset) {
                double value = point.features[i];
                double ig = information_gain(subset, i, value);
                if (ig > best_ig) {
                    best_feature = i;
                    best_value = value;
                    best_ig = ig;
                }
            }
        }

        std::vector<DataPoint> left, right;
        for (const auto& point : subset) {
            if (point.features[best_feature] < best_value) left.push_back(point);
            else right.push_back(point);
        }

        TreeNode* node = new TreeNode();
        node->is_leaf = false;
        node->split_feature = best_feature;
        node->split_value = best_value;
        node->left = build_tree(left);
        node->right = build_tree(right);
        return node;
    }

    int predict(const TreeNode* node, const std::vector<double>& features) {
        if (node->is_leaf) return node->classification;
        if (features[node->split_feature] < node->split_value) return predict(node->left, features);
        else return predict(node->right, features);
    }

public:
    DecisionTree(const std::vector<std::vector<double>>& features, const std::vector<int>& labels) {
        for (size_t i = 0; i < features.size(); ++i) {
            DataPoint point;
            point.features = features[i];
            point.label = labels[i];
            data.push_back(point);
        }
        root = build_tree(data);
    }

    int predict(const std::vector<double>& features) {
        return predict(root, features);
    }
};

int main() {
    std::vector<std::vector<double>> features = {
        {2.7, 2.5}, {1.4, 2.3}, {3.3, 4.4}, {1.3, 1.8}, {3.0, 3.0}, {7.6, 2.7}, {3.1, 4.5}, {5.6, 3.8}
    };
    std::vector<int> labels = {0, 0, 1, 0, 1, 1, 1, 0};

    DecisionTree tree(features, labels);
    std::vector<double> test_feature = {3.5, 2.8};
    int prediction = tree.predict(test_feature);
    std::cout << "Prediction: " << prediction << '\n';

    return 0;
}
```

**Expected Outcome:**
The decision tree should correctly classify the provided dataset and new predictions based on the learned splits. Evaluate its performance using a confusion matrix or other appropriate metrics.
