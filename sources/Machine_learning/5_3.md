\newpage

## 19. Feature Engineering

In the journey of crafting powerful machine learning models, the importance of features—the individual measurable properties or characteristics of a phenomenon being observed—cannot be understated. High-quality features act as the foundations upon which the models are built, often determining the ceiling of the model’s performance. This chapter dives into the quintessential aspect of machine learning known as feature engineering. We will explore techniques to create new features that encapsulate underlying patterns and complexities within data, as well as methods to select the most relevant features that contribute maximally to the model's predictive power. To solidify these concepts, practical examples in C++ will guide you through the implementation of these techniques, providing a robust framework to enhance your machine learning projects. Let's embark on this crucial step to unlock the full potential of your data by mastering the art and science of feature engineering.

### Creating New Features

Creating new features, often referred to as feature engineering, is the process of using domain knowledge to extract or transform raw data into features that better represent the underlying problem for predictive modeling. This chapter will cover the motivations and methods behind creating new features, the theory underpinning their creation, and practical techniques to implement these in C++.

#### Motivation for Creating New Features

The primary motivation for feature engineering is to enhance the predictive power of machine learning models. Raw data is rarely directly usable for machine learning; it often needs to be transformed into a format that highlights the underlying patterns. Effective feature engineering can:
1. **Improve Model Accuracy**: New features can uncover relationships and patterns that were not apparent in the original raw data.
2. **Reduce Overfitting**: By emphasizing the most critical aspects of the data, feature engineering can help models generalize better to unseen data.
3. **Facilitate Model Interpretability**: Well-crafted features can make the model’s predictions more understandable and justifiable.

#### Types of New Features

1. **Polynomial Features**: Creating interaction terms by raising features to a power or multiplying them together.
2. **Logarithmic, Exponential, and Trigonometric Transformations**: Applying mathematical transformations to features to uncover non-linear relationships.
3. **Date and Time Features**: Extracting useful information from date-time data, such as the year, month, day, hour, or even whether it is a weekend or holiday.
4. **Domain-Specific Features**: Features created based on specific domain expertise. For example, in finance, these could be moving averages or technical indicators.
5. **Text Features**: Converting textual data into quantitative features using methods like TF-IDF, word embeddings, or sentiment scores.
6. **Aggregated Features**: Summarizing attributes, such as averages, sums, counts, or other statistical measures.

#### Practical Implementation in C++

Let's walk through the creation of several types of new features in C++.

##### Polynomial Features

Polynomial features can be particularly useful in linear regression when there is a non-linear relationship between the input variables and the target variable.

```cpp
#include <vector>
#include <cmath>
#include <iostream>

// Example function for generating polynomial features
std::vector<double> generatePolynomialFeatures(const std::vector<double>& features, int degree) {
    std::vector<double> polynomialFeatures;
    for (double feature : features) {
        for (int i = 1; i <= degree; ++i) {
            polynomialFeatures.push_back(pow(feature, i));
        }
    }
    return polynomialFeatures;
}

int main() {
    std::vector<double> features = {1.0, 2.0, 3.0};
    int degree = 3;
    std::vector<double> polyFeatures = generatePolynomialFeatures(features, degree);
    
    for (const auto& feature : polyFeatures) {
        std::cout << feature << " ";
    }
    return 0;
}
```

##### Logarithmic and Exponential Transformations

These transformations can linearize exponential relationships and stabilize variance.

```cpp
#include <vector>
#include <cmath>
#include <iostream>

// Logarithmic transformation
std::vector<double> logTransform(const std::vector<double>& features) {
    std::vector<double> logFeatures;
    for (double feature : features) {
        if (feature > 0) { // Log is only defined for positive numbers
            logFeatures.push_back(log(feature));
        } else {
            logFeatures.push_back(feature); // Handle non-positive values
        }
    }
    return logFeatures;
}

int main() {
    std::vector<double> features = {1.0, 2.0, 0.5, -1.0}; // Note the non-positive value
    std::vector<double> logFeatures = logTransform(features);
    
    for (const auto& feature : logFeatures) {
        std::cout << feature << " ";
    }
    return 0;
}
```

##### Date and Time Features

Extracting features from date-time data can be critical in temporal datasets.

```cpp
#include <iostream>
#include <ctime>

// Function to extract day, month, and year from a time_t timestamp
void extractDateFeatures(std::time_t timestamp, int &day, int &month, int &year) {
    std::tm *ltm = localtime(&timestamp);
    day = ltm->tm_mday;
    month = 1 + ltm->tm_mon;
    year = 1900 + ltm->tm_year;
}

int main() {
    std::time_t now = std::time(0); // Current timestamp
    int day, month, year;
    extractDateFeatures(now, day, month, year);

    std::cout << "Day: " << day << "\n";
    std::cout << "Month: " << month << "\n";
    std::cout << "Year: " << year << "\n";
    return 0;
}
```

##### Aggregated Features

Aggregated features like moving averages can provide insights into the data by smoothing out short-term fluctuations and highlighting long-term trends.

```cpp
#include <vector>
#include <numeric>
#include <iostream>

// Function to calculate moving average of a feature set
std::vector<double> calculateMovingAverage(const std::vector<double>& features, int windowSize) {
    std::vector<double> movingAverages;
    int n = features.size();
    for (int i = 0; i <= n - windowSize; ++i) {
        double sum = std::accumulate(features.begin() + i, features.begin() + i + windowSize, 0.0);
        double average = sum / windowSize;
        movingAverages.push_back(average);
    }
    return movingAverages;
}

int main() {
    std::vector<double> features = {1.0, 2.0, 3.0, 4.0, 5.0, 6.0};
    int windowSize = 3;
    std::vector<double> movingAverages = calculateMovingAverage(features, windowSize);
    
    for (const auto& avg : movingAverages) {
        std::cout << avg << " ";
    }
    return 0;
}
```

#### Discussion on Application

The choice of which new features to create depends on the specific problem at hand and the underlying data:

1. **Polynomial Features**: Useful when you suspect an interaction between features.
2. **Logarithmic and Exponential Transformations**: Suitable for stabilizing variance and handling skewed data.
3. **Date and Time Features**: Essential for time-series analysis or any temporal data.
4. **Domain-Specific Features**: Require deep domain expertise but can be the most powerful.

Feature engineering is intrinsically an iterative and experimental process. It's about hypothesis generation, testing, and refinement. Various machine learning heuristics, validation techniques, and domain knowledge should guide the creation of these features.

#### Conclusion

Creating new features is a cornerstone of effective machine learning. By transforming raw data into a more impactful representation, we can significantly boost the performance and interpretability of our models. This chapter gave an overview of various feature creation techniques and illustrated practical implementations in C++. The next step involves selecting these newly created features using feature selection techniques, which will further refine and optimize our models. As you gain experience in feature engineering, you will develop sharper instincts about which features to create and how to transform your data most effectively.

### Feature Selection Techniques

Feature selection techniques are pivotal in the process of machine learning and data mining, as they allow us to identify and retain the most significant features while eliminating redundant or irrelevant ones. This subchapter will delve deeply into the theory, methodology, and practical applications of feature selection techniques. We will explore the rationale behind feature selection, various methods to perform it, and how to execute these methods in C++.

#### Motivation for Feature Selection

The primary goals for feature selection include improving model performance, reducing overfitting, simplifying models for interpretability, and decreasing computational costs. The following points elaborate on why feature selection is crucial:

1. **Enhanced Model Performance**: Including only the most relevant features can improve the model's predictive accuracy.
2. **Reduced Overfitting**: By removing irrelevant or noisy features, we can help the model generalize better to unseen data.
3. **Improved Interpretability**: With fewer features, the model's behavior becomes easier to interpret and trust.
4. **Decreased Computation**: Fewer features reduce the time and resources required for model training and prediction.

#### Types of Feature Selection Techniques

Feature selection techniques can be broadly categorized into three types: filter methods, wrapper methods, and embedded methods.

1. **Filter Methods**: These methods use statistical techniques to evaluate the relevance of features based on intrinsic properties of the data, independent of the machine learning algorithm.
2. **Wrapper Methods**: These methods evaluate the performance of subsets of features by training and testing a specific machine learning model.
3. **Embedded Methods**: These methods perform feature selection as a part of the model training process. They are intrinsic to specific learning algorithms.

#### Filter Methods

Filter methods are usually computationally efficient and are often used as a preprocessing step before applying more complex models.

##### Chi-Square Test

The Chi-Square test measures the dependency between feature and target variable for categorical data.

```python
from sklearn.feature_selection import chi2
from sklearn.feature_selection import SelectKBest

# Example code to perform Chi-Square test
features = ...  # your feature matrix
target = ...   # your target vector

# Apply Chi-Square Test
chi2_values, p_values = chi2(features, target)
selected_features = SelectKBest(chi2, k=10).fit_transform(features, target)
```

##### Pearson Correlation

Pearson Correlation measures the linear correlation between two variables.

```python
import numpy as np

# Example code to calculate Pearson correlation
def pearsonCorr(X, y):
    correlations = [np.corrcoef(X[:,i], y)[0, 1] for i in range(X.shape[1])]
    return correlations

# Example usage
features = ...  # your feature matrix
target = ...    # your target vector

correlations = pearsonCorr(features, target)
```

##### Variance Threshold

This method removes features with variance below a certain threshold, assuming that low-variance features do not carry much information.

```cpp
#include <vector>
#include <cmath>
#include <iostream>
#include <algorithm>

// Calculate variance of a feature vector
double calculateVariance(const std::vector<double>& feature) {
    double mean = std::accumulate(feature.begin(), feature.end(), 0.0) / feature.size();
    double variance = 0.0;
    for (const double val : feature) {
        variance += (val - mean) * (val - mean);
    }
    return variance / feature.size();
}

// Example code to remove low-variance features
std::vector<std::vector<double>> removeLowVarianceFeatures(const std::vector<std::vector<double>>& features, double threshold) {
    std::vector<std::vector<double>> selectedFeatures;
    for (const auto& feature : features) {
        if (calculateVariance(feature) > threshold) {
            selectedFeatures.push_back(feature);
        }
    }
    return selectedFeatures;
}

int main() {
    std::vector<std::vector<double>> features = {{1.0, 2.0, 1.0}, {4.0, 5.0, 4.0}, {7.0, 8.0, 7.0}};
    double threshold = 0.1;
    auto selectedFeatures = removeLowVarianceFeatures(features, threshold);
    
    for (const auto& feature : selectedFeatures) {
        for (const double val : feature) {
            std::cout << val << " ";
        }
        std::cout << std::endl;
    }
    return 0;
}

```

#### Wrapper Methods

Wrapper methods evaluate subsets of features based on model performance.

##### Recursive Feature Elimination (RFE)

RFE recursively trains the model and removes the least important features in each iteration.

```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

# Example code to perform RFE
model = LogisticRegression()
rfe = RFE(model, n_features_to_select=10)
fit = rfe.fit(features, target)
selected_features = fit.support_
print(selected_features)
```

##### Sequential Feature Selection

Sequential feature selection involves searching over feature subsets starting with an empty set and progressively adding (forward selection) or removing (backward selection) features.

```python
from sklearn.feature_selection import SequentialFeatureSelector
from sklearn.linear_model import LinearRegression

# Example code for Sequential Feature Selector
model = LinearRegression()
sfs = SequentialFeatureSelector(model, n_features_to_select=10, direction='forward')
selected_features = sfs.fit_transform(features, target)
print(sfs.get_support())
```

#### Embedded Methods

Embedded methods integrate feature selection into the model training process. These techniques are intrinsic to certain algorithms.

##### Lasso Regression (L1 Regularization)

L1 regularization adds a penalty equal to the absolute value of the magnitude of coefficients, effectively reducing some to zero.

```python
from sklearn.linear_model import Lasso

# Example code for Lasso Regression
model = Lasso(alpha=0.01)
model.fit(features, target)
selected_features = model.coef_ != 0
print(selected_features)
```

##### Tree-Based Methods

Tree-based algorithms such as decision trees, random forests, and gradient boosting inherently perform feature selection by assigning feature importances.

```python
from sklearn.ensemble import RandomForestClassifier

# Example code for Random Forest feature importances
model = RandomForestClassifier()
model.fit(features, target)
importances = model.feature_importances_
selected_features = importances > np.mean(importances)
print(selected_features)
```

#### Practical Considerations

1. **Data Preprocessing**: Ensure that data is properly cleaned and preprocessed before applying feature selection techniques.
2. **Dimensionality Reduction**: Combining feature selection with dimensionality reduction techniques like PCA can sometimes yield better results.
3. **Domain Knowledge**: Leveraging domain expertise can significantly guide the feature selection process, allowing for the incorporation of meaningful features that purely statistical methods might overlook.
4. **Model-Specific Approaches**: Some techniques may work better with specific models; experimentation is often required to find the most effective approach.
5. **Cross-Validation**: Always validate the feature selection process using cross-validation to avoid overfitting.

#### Conclusion

Feature selection is a critical component of the machine learning pipeline that can dramatically improve the performance and interpretability of a model while reducing its complexity and computational burden. This chapter discussed various feature selection techniques, ranging from filter methods, which evaluate features based on their statistical properties, to wrapper and embedded methods that consider the performance impact on a particular model. Through practical examples and detailed explanations, we highlighted the theoretical underpinnings and practical applications of these methods in C++. Integrating feature selection into your machine learning workflow is not just a best practice but a necessity for building high-performing, generalizable models.

### Practical Examples in C++

In this chapter, we will delve into practical examples of feature engineering and feature selection in C++. The objective is to solidify your understanding of these techniques by demonstrating their implementation in a comprehensive and scientific manner. C++ is widely used in performance-critical applications, and its robust feature set makes it a powerful tool for machine learning when combined with appropriate libraries and frameworks.

#### Setting Up the Environment

Before diving into the examples, it is crucial to set up the development environment. For this tutorial, we will use standard C++ and the Eigen library for matrix operations.

1. **Installing Eigen**: The Eigen library is a lightweight C++ template library for linear algebra. It can be downloaded and included in your project. Installation instructions can be found on the [Eigen official website](https://eigen.tuxfamily.org).

2. **Project Structure**: Creating a well-organized project structure is essential.
   ```bash
   mkdir machine_learning_cpp
   cd machine_learning_cpp
   mkdir src include lib
   touch src/main.cpp include/feature_engineering.h include/feature_selection.h
   ```

3. **Compiling the Project**: We will use CMake to manage the build process. Create a `CMakeLists.txt` file with the necessary configuration.
   ```cmake
   cmake_minimum_required(VERSION 3.10)
   project(machine_learning_cpp)
   
   set(CMAKE_CXX_STANDARD 11)
   
   include_directories(include lib/Eigen)
   
   add_executable(machine_learning_cpp src/main.cpp)
   ```

#### Implementing Feature Engineering Techniques

We will implement a few common feature engineering techniques, such as polynomial features, log transformations, and date-time features.

##### Polynomial Features

```cpp
// feature_engineering.h
#include <vector>
#include <cmath>

std::vector<double> generatePolynomialFeatures(const std::vector<double>& features, int degree);

// feature_engineering.cpp
#include "feature_engineering.h"

std::vector<double> generatePolynomialFeatures(const std::vector<double>& features, int degree) {
    std::vector<double> polynomialFeatures;
    for (double feature : features) {
        for (int i = 1; i <= degree; ++i) {
            polynomialFeatures.push_back(pow(feature, i));
        }
    }
    return polynomialFeatures;
}
```

##### Logarithmic and Exponential Transformations

```cpp
// feature_engineering.h
std::vector<double> logTransform(const std::vector<double>& features);

// feature_engineering.cpp
#include "feature_engineering.h"

std::vector<double> logTransform(const std::vector<double>& features) {
    std::vector<double> logFeatures;
    for (double feature : features) {
        if (feature > 0) {
            logFeatures.push_back(log(feature));
        } else {
            logFeatures.push_back(feature); // Handle non-positive values
        }
    }
    return logFeatures;
}
```

##### Date and Time Features

```cpp
// feature_engineering.h
#include <ctime>

void extractDateFeatures(std::time_t timestamp, int &day, int &month, int &year);

// feature_engineering.cpp
#include "feature_engineering.h"

void extractDateFeatures(std::time_t timestamp, int &day, int &month, int &year) {
    std::tm *ltm = localtime(&timestamp);
    day = ltm->tm_mday;
    month = 1 + ltm->tm_mon;
    year = 1900 + ltm->tm_year;
}
```

#### Implementing Feature Selection Techniques

Next, we'll cover some of the key feature selection techniques such as variance threshold, chi-square test, and Recursive Feature Elimination (RFE).

##### Variance Threshold

```cpp
// feature_selection.h
std::vector<int> varianceThreshold(const std::vector<std::vector<double>>& features, double threshold);

// feature_selection.cpp
#include "feature_selection.h"
#include <numeric>

double calculateVariance(const std::vector<double>& feature) {
    double mean = std::accumulate(feature.begin(), feature.end(), 0.0) / feature.size();
    double variance = 0.0;
    for (const double val : feature) {
        variance += (val - mean) * (val - mean);
    }
    return variance / feature.size();
}

std::vector<int> varianceThreshold(const std::vector<std::vector<double>>& features, double threshold) {
    std::vector<int> selectedFeatures;
    for (size_t i = 0; i < features.size(); ++i) {
        if (calculateVariance(features[i]) > threshold) {
            selectedFeatures.push_back(i);
        }
    }
    return selectedFeatures;
}
```

##### Chi-Square Test

The Chi-Square test implementation requires categorical data. Here, we simulate a simplified version, assuming binary target.

```cpp
// feature_selection.h
#include <vector>

std::vector<int> chiSquareTest(const std::vector<std::vector<double>>& features, const std::vector<int>& target, int k);

// feature_selection.cpp
#include "feature_selection.h"
#include <iostream>
#include <algorithm>
#include <functional>

double chiSquare(const std::vector<double>& feature, const std::vector<int>& target) {
    // Placeholders for Chi-Square calculation
    double chi2 = 0.0;
    // Assume a binary target for simplicity, detailed implementation would require a more thorough approach.
    int n = feature.size();
    for (int i = 0; i < n; i++) {
        chi2 += pow(feature[i] - target[i], 2) / target[i];
    }
    return chi2;
}

std::vector<int> chiSquareTest(const std::vector<std::vector<double>>& features, const std::vector<int>& target, int k) {
    std::vector<std::pair<double, int>> chi2Scores;
    for (size_t i = 0; i < features.size(); ++i) {
        chi2Scores.push_back({ chiSquare(features[i], target), i });
    }
    std::sort(chi2Scores.begin(), chi2Scores.end(), std::greater<>());
    std::vector<int> selectedFeatures;
    for (int i = 0; i < k; ++i) {
        selectedFeatures.push_back(chi2Scores[i].second);
    }
    return selectedFeatures;
}
```

##### Recursive Feature Elimination (RFE)

Recursive Feature Elimination is model-specific. Here’s a simplified example for a linear regression model (LRM).

```cpp
// feature_selection.h
#include <Eigen/Dense>

std::vector<int> recursiveFeatureElimination(Eigen::MatrixXd features, Eigen::VectorXd target, int numFeatures);

// feature_selection.cpp
#include "feature_selection.h"
#include <Eigen/Dense>
#include <vector>
#include <algorithm>

double linearModelError(const Eigen::MatrixXd& features, const Eigen::VectorXd& target) {
    Eigen::VectorXd coefficients = features.colPivHouseholderQr().solve(target);
    Eigen::VectorXd predictions = features * coefficients;
    return (target - predictions).norm();
}

std::vector<int> recursiveFeatureElimination(Eigen::MatrixXd features, Eigen::VectorXd target, int numFeatures) {
    int numColumns = features.cols();
    std::vector<int> selectedFeatures(numColumns);
    std::iota(selectedFeatures.begin(), selectedFeatures.end(), 0);
    
    while (selectedFeatures.size() > numFeatures) {
        double minError = std::numeric_limits<double>::infinity();
        int worstFeature = -1;
        
        for (int feature : selectedFeatures) {
            std::vector<int> currentFeatures = selectedFeatures;
            currentFeatures.erase(std::remove(currentFeatures.begin(), currentFeatures.end(), feature), currentFeatures.end());
            
            Eigen::MatrixXd reducedFeatures(features.rows(), currentFeatures.size());
            for (size_t i = 0; i < currentFeatures.size(); ++i) {
                reducedFeatures.col(i) = features.col(currentFeatures[i]);
            }
            
            double error = linearModelError(reducedFeatures, target);
            if (error < minError) {
                minError = error;
                worstFeature = feature;
            }
        }
        
        selectedFeatures.erase(std::remove(selectedFeatures.begin(), selectedFeatures.end(), worstFeature), selectedFeatures.end());
    }
    
    return selectedFeatures;
}
```

#### Putting It All Together

Let's combine these components in a single workflow that demonstrates both feature engineering and feature selection.

```cpp
// main.cpp
#include "feature_engineering.h"
#include "feature_selection.h"
#include <iostream>
#include <vector>
#include <ctime>
#include <Eigen/Dense>

int main() {
    // Example data
    std::vector<double> rawFeatures = {2.0, 3.0, 5.0, 7.0, 11.0, 13.0, 17.0};
    std::vector<int> target = {0, 1, 0, 1, 0, 1, 0}; // Binary target

    // Feature Engineering
    std::vector<double> polyFeatures = generatePolynomialFeatures(rawFeatures, 2);
    std::vector<double> logFeatures = logTransform(rawFeatures);
    
    // Print engineered features
    std::cout << "Polynomial Features: ";
    for (const auto& feature : polyFeatures) {
        std::cout << feature << " ";
    }
    std::cout << "\nLogarithmic Features: ";
    for (const auto& feature : logFeatures) {
        std::cout << feature << " ";
    }

    // Convert vector to Eigen::Matrix for feature selection
    Eigen::MatrixXd featureMatrix(rawFeatures.size(), 2);
    for (size_t i = 0; i < rawFeatures.size(); ++i) {
        featureMatrix(i, 0) = rawFeatures[i];
        featureMatrix(i, 1) = pow(rawFeatures[i], 2);
    }
    Eigen::VectorXd targetVector = Eigen::Map<Eigen::VectorXd>(target.data(), target.size());

    // Feature Selection
    double varianceThresholdValue = 0.5;
    std::vector<int> selectedVarFeatures = varianceThreshold({rawFeatures, logFeatures}, varianceThresholdValue);

    int k = 1;
    std::vector<int> selectedChi2Features = chiSquareTest({rawFeatures, logFeatures}, target, k);

    int numFeatures = 1;
    std::vector<int> selectedRFEFeatures = recursiveFeatureElimination(featureMatrix, targetVector, numFeatures);

    // Print selected features
    std::cout << "\nSelected Features by Variance Threshold: ";
    for (int feature : selectedVarFeatures) {
        std::cout << feature << " ";
    }
    std::cout << "\nSelected Features by Chi-Square Test: ";
    for (int feature : selectedChi2Features) {
        std::cout << feature << " ";
    }
    std::cout << "\nSelected Features by RFE: ";
    for (int feature : selectedRFEFeatures) {
        std::cout << feature << " ";
    }

    return 0;
}
```

#### Conclusion

This chapter provided a thorough exploration of practical examples in C++ for performing feature engineering and feature selection. We implemented various techniques to generate new features such as polynomial and logarithmic transformations, as well as date and time features. Additionally, we demonstrated multiple feature selection methods, including variance threshold, chi-square test, and Recursive Feature Elimination (RFE). 

By integrating these techniques into a workflow, you can better prepare your data for machine learning models, ultimately enhancing their performance, interpretability, and efficiency. Whether you're working with small datasets or large-scale applications, the principles and practices covered here form an essential part of any machine learning pipeline.

