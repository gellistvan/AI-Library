\newpage

# Part II: Fundamental Machine Learning Algorithms

## 3. Linear Regression

Linear regression is one of the most fundamental and widely used algorithms in the field of machine learning. It serves as a powerful tool for understanding the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data. Its simplicity and interpretability make it an excellent starting point for those new to machine learning, while its robustness ensures its continued relevance for experienced practitioners. In this chapter, we will delve into the core concepts of linear regression, explore its mathematical foundations, and walk through a step-by-step implementation in C++. Additionally, we will investigate various optimization techniques to refine our model, ensuring it provides the most accurate predictions possible.

### Introduction to Linear Regression

Linear regression is a statistical technique used in machine learning to model the relationship between a dependent variable (also known as the outcome or response variable) and one or more independent variables (also known as predictors or features). This relationship is modeled using a linear equation with coefficients that represent the weight of each predictor in the contribution to the outcome. The goal is to find the best-fitting line that minimizes the difference between the observed values and the values predicted by the linear model.

#### 1. Historical Context

Linear regression, a cornerstone of statistical analysis and machine learning, dates back centuries. Sir Francis Galton introduced the concept in the late 19th century, building on earlier work by Karl Pearson. Galton used linear regression to study the relationship between parental and offspring traits, which laid the groundwork for the broader application of this method in various fields.

#### 2. Mathematical Foundations

The standard form of a simple linear regression model, which involves one dependent variable $y$ and one independent variable $x$, is expressed as follows:

$$ y = \beta_0 + \beta_1 x + \epsilon $$

Here:
- $y$ is the dependent variable.
- $x$ is the independent variable.
- $\beta_0$ is the y-intercept of the regression line (the value of $y$ when $x = 0$).
- $\beta_1$ is the slope of the regression line, representing the change in $y$ for a one-unit change in $x$.
- $\epsilon$ is the error term, accounting for the variability in $y$ that cannot be explained by the linear relationship with $x$.

For multiple linear regression, where there are multiple independent variables $x_1, x_2, \ldots, x_p$, the model is:

$$ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p + \epsilon $$

In matrix notation, this can be expressed as:

$$ \mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon} $$

Where:
- $\mathbf{y}$ is an $n \times 1$ vector of observed values.
- $\mathbf{X}$ is an $n \times (p+1)$ matrix of observations (with a column of ones for the intercept).
- $\boldsymbol{\beta}$ is a $(p+1) \times 1$ vector of coefficients.
- $\boldsymbol{\epsilon}$ is an $n \times 1$ vector of errors.

#### 3. Assumptions of Linear Regression

Several key assumptions must be satisfied for the linear regression model to be valid:
1. **Linearity**: The relationship between the dependent and independent variables should be linear.
2. **Independence**: Observations should be independent of each other.
3. **Homoscedasticity**: The variance of residuals (errors) should be constant across all levels of the independent variables.
4. **No Multicollinearity**: Independent variables should not be too highly correlated with each other.
5. **Normality**: Residuals of the model should be roughly normally distributed.

Failure to meet these assumptions may result in biased or inefficient estimates, invalid hypothesis tests, and incorrect inferences.

#### 4. Estimation of Coefficients

The coefficients $\boldsymbol{\beta}$ are typically estimated using the method of Ordinary Least Squares (OLS), which minimizes the sum of the squared residuals:

$$ RSS = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = \sum_{i=1}^{n} \left(y_i - \left(\beta_0 + \beta_1 x_{1i} + \cdots + \beta_p x_{pi}\right)\right)^2 $$

In matrix notation, this can be rewritten as:

$$ RSS = (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^T (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) $$

The OLS estimates are obtained by solving the normal equations:

$$ \hat{\boldsymbol{\beta}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y} $$

#### 5. Model Evaluation

Several metrics are commonly used to evaluate the performance of a linear regression model:
- **R-squared ($R^2$)**: Represents the proportion of variance in the dependent variable explained by the independent variables. It ranges from 0 to 1, with higher values indicating better model fit.
  
  $$ R^2 = 1 - \frac{SS_{res}}{SS_{tot}} $$

  Where $SS_{res}$ is the sum of squares of the residuals, and $SS_{tot}$ is the total sum of squares.

- **Adjusted R-squared**: Adjusts the $R^2$ value for the number of predictors in the model, providing a more accurate measure when multiple variables are involved.

  $$ \text{Adjusted } R^2 = 1 - \left(\frac{1 - R^2}{n - 1}\right) (n - p - 1) $$
  
  Where $n$ is the number of observations and $p$ is the number of predictors.

- **Mean Squared Error (MSE)**: The average of the squared differences between the observed and predicted values.

  $$ MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 $$

- **Root Mean Squared Error (RMSE)**: The square root of MSE, providing an error metric on the same scale as the dependent variable.

  $$ RMSE = \sqrt{MSE} $$

- **Mean Absolute Error (MAE)**: The average of the absolute differences between the observed and predicted values.

  $$ MAE = \frac{1}{n} \sum_{i=1}^{n} | y_i - \hat{y}_i | $$

#### 6. Model Diagnostics

It's essential to conduct various diagnostics to assess the validity and reliability of the linear regression model:
- **Residual Plots**: Plotting residuals against fitted values, predictors, or time can reveal patterns indicating non-linearity, heteroscedasticity, or serial correlation.
- **Q-Q Plots**: Plotting the quantiles of residuals against the quantiles of a normal distribution can assess the normality assumption.
- **VIF (Variance Inflation Factor)**: Detecting multicollinearity among predictors by measuring how much the variance of a regression coefficient is inflated due to multicollinearity.

```python
from statsmodels.graphics.gofplots import qqplot
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Example: Q-Q Plot for Residuals
qqplot(residuals, line='s')
plt.show()

# Example: Residual Plot
sns.residplot(predicted_values, residuals)
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.show()

# Example: VIF Calculation
X = df[['x1', 'x2', 'x3', 'x4']]  # Independent Variables
vif_data = pd.DataFrame()
vif_data["feature"] = X.columns
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]
print(vif_data)
```

### Implementation in C++

Implementing linear regression in C++ requires a solid understanding of both the algorithm's mathematical foundations and the programming language's syntax and features. We will break this chapter into several key sections: setup and dependencies, data handling, model creation, parameter estimation, and model evaluation.

#### Setup and Dependencies

Before diving into the implementation, there are a few key prerequisites and dependencies to set up:

1. **Compiler**: Ensure you have a compatible C++ compiler installed, such as GCC or Clang.
2. **Libraries**: For mathematical operations and handling data, we will use the Eigen library, a popular C++ template library for linear algebra.

You can install Eigen by downloading it from its [official website](https://eigen.tuxfamily.org/dox/GettingStarted.html) and including it in your project.

#### Data Handling

Linear regression requires a dataset comprising independent variables (features) and a dependent variable (target). In C++, we can use the Eigen library to handle matrices representing these variables. 

Here is a simplified illustration of reading data into Eigen matrices:

```cpp
#include <iostream>
#include <Eigen/Dense>
#include <fstream>
#include <sstream>

using namespace Eigen;

MatrixXd readCSV(const std::string& file, int rows, int cols) {
    std::ifstream in(file);
    std::string line;
    MatrixXd mat(rows, cols);
    int row = 0;
    int col = 0;

    while (std::getline(in, line)) {
        std::stringstream ss(line);
        std::string value;
        col = 0;
        while (std::getline(ss, value, ',')) {
            mat(row, col) = std::stod(value);
            col++;
        }
        row++;
    }
    return mat;
}
```

#### Model Creation

The linear regression model in matrix form is given by:

$$ \mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon} $$

Where:
- $\mathbf{y}$ is the vector of observed values.
- $\mathbf{X}$ is the matrix of input features (with a column of ones for the intercept).
- $\boldsymbol{\beta}$ is the vector of coefficients.
- $\boldsymbol{\epsilon}$ is the error term.

We can represent this model in C++ using Eigen matrices. 

#### Parameter Estimation (Ordinary Least Squares)

The most common method for estimating the parameters $\boldsymbol{\beta}$ is Ordinary Least Squares (OLS). Using matrix operations, the OLS estimate is given by:

$$ \hat{\boldsymbol{\beta}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y} $$

This can be efficiently computed using Eigen:

```cpp
VectorXd estimateOLS(const MatrixXd& X, const VectorXd& y) {
    return (X.transpose() * X).inverse() * X.transpose() * y;
}
```

#### Implementation Workflow

Let's integrate the above components into a cohesive workflow:

1. **Reading Data**: Load the dataset into Eigen matrices.
2. **Preparing Data**: Add a column of ones to the feature matrix to account for the intercept.
3. **Estimating Parameters**: Compute the OLS estimates for the model parameters.
4. **Making Predictions**: Use the estimated parameters to make predictions on new data.
5. **Evaluating the Model**: Calculate useful metrics (e.g., MSE, R-squared) to evaluate the model's performance.

Here's a complete implementation:

```cpp
#include <iostream>
#include <Eigen/Dense>
#include <fstream>
#include <sstream>

using namespace Eigen;
using namespace std;

// Function to read CSV data into an Eigen matrix
MatrixXd readCSV(const std::string& file, int rows, int cols) {
    std::ifstream in(file);
    std::string line;
    MatrixXd mat(rows, cols);
    int row = 0;
    int col = 0;

    while (std::getline(in, line)) {
        std::stringstream ss(line);
        std::string value;
        col = 0;
        while (std::getline(ss, value, ',')) {
            mat(row, col) = std::stod(value);
            col++;
        }
        row++;
    }
    return mat;
}

// Function to estimate parameters using OLS
VectorXd estimateOLS(const MatrixXd& X, const VectorXd& y) {
    return (X.transpose() * X).inverse() * X.transpose() * y;
}

// Function to calculate Mean Squared Error
double calculateMSE(const VectorXd& y_true, const VectorXd& y_pred) {
    return (y_true - y_pred).array().square().sum() / y_true.rows();
}

// Function to calculate R-squared
double calculateRSquared(const VectorXd& y_true, const VectorXd& y_pred) {
    double SS_tot = (y_true.array() - y_true.mean()).square().sum();
    double SS_res = (y_true - y_pred).array().square().sum();
    return 1 - (SS_res / SS_tot);
}

int main() {
    // Read dataset
    std::string file = "data.csv";
    int rows = 100;  // Adjust based on your data
    int cols = 3;    // Adjust based on your data
    
    MatrixXd data = readCSV(file, rows, cols);
    
    // Split dataset into X (features) and y (target)
    MatrixXd X = data.leftCols(cols - 1);
    VectorXd y = data.col(cols - 1);
    
    // Add a column of ones to X for the intercept term
    MatrixXd X_b(X.rows(), X.cols() + 1);
    X_b << MatrixXd::Ones(X.rows(), 1), X;
    
    // Estimate parameters
    VectorXd theta = estimateOLS(X_b, y);
    
    // Make predictions
    VectorXd y_pred = X_b * theta;
    
    // Calculate and print MSE and R-squared
    double mse = calculateMSE(y, y_pred);
    double r_squared = calculateRSquared(y, y_pred);
    
    cout << "Estimated coefficients: \n" << theta << endl;
    cout << "Mean Squared Error: " << mse << endl;
    cout << "R-squared: " << r_squared << endl;

    return 0;
}
```

#### Model Evaluation

Evaluating a linear regression model involves calculating various metrics to assess its performance:

1. **Mean Squared Error (MSE)**: Measures the average squared difference between observed and predicted values.

$$ \text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 $$

2. **R-squared ($R^2$)**: Represents the proportion of variance in the dependent variable explained by the independent variables.

$$ R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2} $$

#### Advanced Topics

1. **Regularization**: Techniques like Ridge regression (L2) and Lasso regression (L1) add penalties to the loss function to prevent overfitting.

   **Ridge Regression** minimizes the following loss function:

   $$ L(\boldsymbol{\beta}) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda \sum_{j=1}^{p} \beta_j^2 $$

2. **Gradient Descent**: An iterative optimization algorithm used especially for large datasets where OLS becomes computationally expensive.

   The parameter update rule is:

   $$ \beta_j := \beta_j - \alpha \frac{\partial}{\partial \beta_j}L(\boldsymbol{\beta}) $$

   Where $\alpha$ is the learning rate.

3. **Stochastic Gradient Descent (SGD)**: A variant of gradient descent that updates parameters for each training example, improving efficiency and enabling scaling to larger datasets.

#### Conclusion

Implementing linear regression in C++ provides a comprehensive understanding of the algorithm's details and instills a deeper appreciation for optimization techniques. We've traversed the landscape from data handling to model evaluation, ensuring that you are well-equipped to build and refine linear regression models. The journey of mastering machine learning and optimization continues with exploring more advanced models and techniques, leveraging the robust and efficient capabilities of C++.

### Optimization Techniques

#### Introduction

Optimization techniques are fundamental to improving the performance and predictive power of machine learning models, including linear regression. In this chapter, we will cover several optimization techniques ranging from classic methods like Gradient Descent to more advanced techniques such as Regularization. We will explore these methods with scientific rigour, detailing their mathematical foundations, implementation strategies, and advantages and disadvantages. 

#### Gradient Descent

Gradient Descent is one of the most widely used optimization algorithms in machine learning, particularly when dealing with large datasets where exact methods such as Ordinary Least Squares (OLS) become computationally burdensome.

##### Basics of Gradient Descent

The core idea of Gradient Descent is to iteratively adjust the model parameters to minimize a cost function, usually the Mean Squared Error (MSE) for linear regression.

The gradient descent algorithm updates the parameters $\boldsymbol{\beta}$ by moving in the direction opposite to the gradient of the cost function $J(\boldsymbol{\beta})$:

$$ \beta_j := \beta_j - \alpha \frac{\partial J(\boldsymbol{\beta})}{\partial \beta_j} $$

Where:
- $\alpha$ is the learning rate, which determines the step size of each update.
- $\frac{\partial J(\boldsymbol{\beta})}{\partial \beta_j}$ is the partial derivative of the cost function with respect to $\beta_j$.

In the context of linear regression, the cost function $J(\boldsymbol{\beta})$ is the Mean Squared Error (MSE):

$$ J(\boldsymbol{\beta}) = \frac{1}{2n} \sum_{i=1}^{n} (y_i - \mathbf{X}_i \boldsymbol{\beta})^2 $$

The partial derivative with respect to $\beta_j$ is:

$$ \frac{\partial J(\boldsymbol{\beta})}{\partial \beta_j} = -\frac{1}{n} \sum_{i=1}^{n} (y_i - \mathbf{X}_i \boldsymbol{\beta}) X_{ij} $$

##### Implementation in C++

Here's a basic implementation of Gradient Descent for linear regression in C++:

```cpp
#include <iostream>
#include <Eigen/Dense>
#include <vector>

using namespace Eigen;
using namespace std;

VectorXd gradientDescent(const MatrixXd& X, const VectorXd& y, double alpha, int iterations) {
    int m = X.rows();
    int n = X.cols();
    VectorXd beta = VectorXd::Zero(n);
    
    for (int iter = 0; iter < iterations; ++iter) {
        VectorXd gradient = - (X.transpose() * (y - X * beta)) / m;
        beta = beta - alpha * gradient;
    }
    
    return beta;
}

int main() {
    // Assume X_b is the feature matrix with intercept term and y is the target vector
    MatrixXd X_b(100, 3); // Replace with actual data
    VectorXd y(100);      // Replace with actual data

    double alpha = 0.01; // Learning rate
    int iterations = 1000;

    VectorXd beta = gradientDescent(X_b, y, alpha, iterations);
    cout << "Estimated coefficients: \n" << beta << endl;

    return 0;
}
```

##### Variants of Gradient Descent

1. **Batch Gradient Descent**: Uses the entire dataset to compute the gradient at each step. It is computationally expensive but converges smoothly.
2. **Stochastic Gradient Descent (SGD)**: Updates the parameters for each training example individually. It is faster but has more variability in the updates.
3. **Mini-batch Gradient Descent**: A compromise between Batch and SGD, it updates parameters in batches of data. This method balances the convergence speed and smoothness.

#### Regularization Techniques

Regularization adds a penalty term to the cost function to prevent overfitting by discouraging overly complex models. The two most common regularization techniques are Ridge Regression (L2 regularization) and Lasso Regression (L1 regularization).

##### Ridge Regression (L2 Regularization)

Ridge regression adds the L2 norm of the coefficients to the cost function:

$$ J(\boldsymbol{\beta}) = \sum_{i=1}^{n} (y_i - \mathbf{X}_i \boldsymbol{\beta})^2 + \lambda \sum_{j=1}^{p} \beta_j^2 $$

Where $\lambda$ is the regularization parameter controlling the trade-off between fitting the data and keeping the coefficients small.

In matrix form, the Ridge Regression solution is:

$$ \hat{\boldsymbol{\beta}} = (\mathbf{X}^T \mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^T \mathbf{y} $$

##### Lasso Regression (L1 Regularization)

Lasso regression adds the L1 norm of the coefficients to the cost function:

$$ J(\boldsymbol{\beta}) = \sum_{i=1}^{n} (y_i - \mathbf{X}_i \boldsymbol{\beta})^2 + \lambda \sum_{j=1}^{p} |\beta_j| $$

The L1 penalty tends to produce sparse models, where some coefficients are exactly zero, effectively performing feature selection.

##### Implementation of Ridge and Lasso Regression

The objective function for Ridge Regression can be solved analytically as shown above, but for Lasso Regression, an iterative approach such as coordinate descent is often employed.

Here's a simple implementation of Ridge Regression in Python using NumPy:

```python
import numpy as np

def ridge_regression(X, y, lambda_):
    X_b = np.hstack([np.ones((X.shape[0], 1)), X])
    I = np.eye(X_b.shape[1])
    beta = np.linalg.inv(X_b.T.dot(X_b) + lambda_ * I).dot(X_b.T).dot(y)
    return beta

# Example usage
X = np.random.rand(100, 3)
y = np.random.rand(100)
lambda_ = 0.1

beta = ridge_regression(X, y, lambda_)
print("Estimated coefficients:\n", beta)
```

##### Elastic Net

Elastic Net combines both L1 and L2 regularization, offering a balance between Ridge and Lasso regressions:

$$ J(\boldsymbol{\beta}) = \sum_{i=1}^{n} (y_i - \mathbf{X}_i \boldsymbol{\beta})^2 + \lambda_1 \sum_{j=1}^{p} |\beta_j| + \lambda_2 \sum_{j=1}^{p} \beta_j^2 $$

This approach is particularly useful when the number of predictors exceeds the number of observations or when predictors are highly correlated.

#### Advanced Optimization Algorithms

1. **Conjugate Gradient**: An iterative method for optimizing quadratic functions, particularly suited for large-scale problems where matrix inversion is computationally prohibitive.
   
2. **Newton's Method and Quasi-Newton Methods**: These methods use second-order derivatives (Hessian matrix) to achieve faster convergence compared to gradient descent.

3. **BFGS (Broyden–Fletcher–Goldfarb–Shanno) Algorithm**: A popular quasi-Newton method that approximates the Hessian matrix to find the optimum of the cost function efficiently.

#### Practical Considerations

1. **Learning Rate Tuning**: Selecting an appropriate learning rate ($\alpha$) is crucial. A learning rate too high may cause the algorithm to overshoot the minimum, while a learning rate too low may result in slow convergence.
   
2. **Momentum**: Introducing momentum can help accelerate gradient vectors in the right directions, leading to faster converging.

3. **Early Stopping**: To prevent overfitting, training can be stopped early when the performance on a validation set starts to degrade.

4. **Batch Normalization**: Normalizing inputs in mini-batches can stabilize and speed up the training process.

#### Conclusion

Optimization is a critical component of machine learning that involves enhancing model performance and reliability. From Gradient Descent and its variants to Regularization and advanced algorithms like Newton's methods, each technique serves a unique purpose and has its own set of advantages and trade-offs. In practice, the choice of optimization technique depends on the specific problem, dataset characteristics, and computational resources. Mastering these techniques equips practitioners with the necessary tools to tackle a wide range of regression problems and beyond, opening doors to further exploration in the vast domain of machine learning optimization.

