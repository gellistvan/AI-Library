Adatbányászat és gépi tanulás

Az információs társadalom korában az adatok mennyisége és komplexitása robbanásszerű növekedésnek indult, amely kihívásokat és egyben rendkívüli lehetőségeket is teremtett. Az adatok elemzésének és értelmezésének képessége kulcsfontosságúvá vált mind a tudományos kutatás, mind az üzleti döntéshozatal számára. Az adatbányászat és gépi tanulás olyan innovatív technikák és módszerek összessége, amelyek lehetővé teszik ezen hatalmas adatmennyiség strukturált és értékes információvá alakítását. E fejezet célja, hogy bemutassa az adatbányászat és gépi tanulás alapelveit és definícióit, valamint bevezesse az olvasót a felügyelt és felügyelet nélküli tanulás fogalmába. Ezek az alapvető ismeretek képezik a későbbi fejezetek mélyebb és specifikusabb algoritmusainak és adatszerkezeteinek megértésének alapját. Az út az adatból nyert tudásig itt kezdődik.

### Adatbányászat és gépi tanulás alapjai

Az adatbányászat és gépi tanulás két olyan tudományág, amelyek szorosan összekapcsolódnak és gyakran egymás szinonimáiként kezelik őket. Míg az adatbányászat elsősorban az adatok feltárására és mintázatok felfedezésére koncentrál, addig a gépi tanulás a modellépítésre és predikciós algoritmusokra helyezi a hangsúlyt. Ebben a fejezetben mélyebben ismerjük meg ezen tudományágak alapjait és azok kulcsfogalmait.

#### Adatbányászat

##### Definíciók és célok

Az adatbányászat az adatokban rejlő mintázatok, anomáliák és hasznos információk felfedezésének folyamata. Ez a folyamat különböző technikák és eszközök alkalmazásával történik, amelyek képesek feldolgozni és analizálni a nagy mennyiségű adatot. Az adatbányászat fő céljai közé tartozik:

1. **Ismeretszerzés**: Rejtett információk, mintázatok és szabályszerűségek felfedezése az adatokban.
2. **Döntéstámogatás**: Az üzleti döntéshozatal támogatása az adatok alapú elemzések és előrejelzések révén.
3. **Predikció**: Jövőbeli trendek és események előrejelzése az adatok mintázatainak és tendenciáinak elemzése alapján.

##### Adatbányászati folyamatok

Az adatbányászat egy iteratív folyamat, amely az alábbi lépésekből áll:

1. **Adatgyűjtés**: Az adatok összegyűjtése különböző forrásokból, mint például adatbázisok, adatáramlások, webhelyek és szenzorok.
2. **Adatelőkészítés**: Az adatok tisztítása, normalizálása és transzformálása olyan formába, amely alkalmas az elemzésre.
3. **Adatvizualizáció**: A rendelkezésre álló adatok szemléletes megjelenítése, például grafikonok, diagrammok segítségével, hogy az adatok jobb megértéséhez segítsük az elemzőket.
4. **Mintafelismerés**: Különböző algoritmusok és technikák alkalmazása az adatokban rejlő mintázatok és szabályszerűségek felismerésére.
5. **Értékelés és validáció**: Az észlelt minták és felismerések tesztelése és kiértékelése a pontosság és hasznosság szempontjából.
6. **Telepítés és alkalmazás**: Az elemzés eredményeinek alkalmazása a való világban, például az üzleti folyamatok optimalizálásának érdekében.

#### Gépi tanulás

##### Definíciók és célok

A gépi tanulás egy mesterséges intelligencia ág, amely az adatokból való tanulásra és az adatok mintázatainak felismerésére fókuszál. Az alapötlet az, hogy a gépek "tanuljanak" az adatokból anélkül, hogy explicit programozásra lenne szükség. A gépi tanulás céljai közé tartozik:

1. **Osztályozás**: Az adatok osztályokba vagy kategóriákba sorolása.
2. **Regresszió**: Kapcsolatok és korrelációk feltárása az adatok között, különösen az egyik változó függvényében a másik változó előrejelzése érdekében.
3. **Klaszterezés**: Az adatok csoportokba rendezése, ahol a csoporton belüli adatok hasonlóak egymáshoz.
4. **Dimenziócsökkentés**: Az adatok dimenzióinak csökkentése a redundáns változók eliminálása révén.
5. **Anomália detektálás**: Szokatlan adatok vagy adathalmazok azonosítása, amelyek eltérnek a normál mintázattól.

##### Gépi tanulás típusai

A gépi tanulás három fő kategóriába sorolható, ezek a felügyelt tanulás, a felügyelet nélküli tanulás és a megerősítő tanulás:

1. **Felügyelt tanulás (Supervised Learning)**: 
    - Ebben a módszerben a tanítókészlet bemeneti-párokból áll, ahol minden egyes bemenethez tartozik egy kimenet. Az algoritmus célja, hogy megtanulja a bemenet és a kimenet közötti kapcsolatot, és képes legyen új adatbemenetekre prediktív kimenetet adni.
    - Példák: lineáris regresszió, döntési fák, k-nn, neurális hálózatok.
    
    ```cpp
    // C++ példa a lineáris regresszió implementációjára
    #include <iostream>
    #include <vector>
    using namespace std;

    class LinearRegression {
    public:
        LinearRegression() : b0(0), b1(0) {}

        void fit(const vector<double>& x, const vector<double>& y) {
            double sumX = 0, sumY = 0, sumXY = 0, sumX2 = 0;
            int n = x.size();

            for(int i = 0; i < n; i++) {
                sumX += x[i];
                sumY += y[i];
                sumXY += x[i] * y[i];
                sumX2 += x[i] * x[i];
            }

            b1 = (n * sumXY - sumX * sumY) / (n * sumX2 - sumX * sumX);
            b0 = (sumY - b1 * sumX) / n;
        }

        double predict(double x) {
            return b0 + b1 * x;
        }

    private:
        double b0, b1;
    };

    int main() {
        vector<double> x = {1, 2, 3, 4, 5};
        vector<double> y = {2, 3, 5, 7, 11};

        LinearRegression lr;
        lr.fit(x, y);

        double prediction = lr.predict(6);
        cout << "Prediction for x = 6: " << prediction << endl;

        return 0;
    }
    ```

2. **Felügyelet nélküli tanulás (Unsupervised Learning)**:
    - Nincs címkézett kimeneti adat. Az algoritmus célja az adatok belső mintázatainak, struktúráinak és csoportosításainak felismerése.
    - Példák: klaszterezés (K-means), főkomponens-analízis (PCA), asszociációs szabályok.

    ```cpp
    // C++ példa a K-means klaszterezés implementációjára
    #include <iostream>
    #include <vector>
    #include <cmath>
    using namespace std;

    struct Point {
        double x, y;
        int cluster;
    };

    double distance(Point a, Point b) {
        return sqrt((a.x - b.x) * (a.x - b.x) + (a.y - b.y) * (a.y - b.y));
    }

    vector<Point> k_means(vector<Point>& points, int k) {
        vector<Point> centroids(k);
        // Initialize centroids with the first k points
        for (int i = 0; i < k; i++) {
            centroids[i] = points[i];
        }

        bool change = true;
        while(change) {
            change = false;

            // Assign points to the nearest centroid
            for (auto& point : points) {
                double min_dist = distance(point, centroids[0]);
                int cluster = 0;
                for (int i = 1; i < k; i++) {
                    double dist = distance(point, centroids[i]);
                    if (dist < min_dist) {
                        min_dist = dist;
                        cluster = i;
                    }
                }
                if (point.cluster != cluster) {
                    change = true;
                    point.cluster = cluster;
                }
            }

            // Update centroids
            vector<int> counts(k, 0);
            vector<Point> new_centroids(k, {0, 0, 0});
            for (const auto& point : points) {
                new_centroids[point.cluster].x += point.x;
                new_centroids[point.cluster].y += point.y;
                counts[point.cluster]++;
            }
            for (int i = 0; i < k; i++) {
                new_centroids[i].x /= counts[i];
                new_centroids[i].y /= counts[i];
                centroids[i] = new_centroids[i];
            }
        }
        return centroids;
    }

    int main() {
        vector<Point> points = {{1, 1, 0}, {2, 1, 0}, {4, 3, 0}, {5, 4, 0}};
        int k = 2;
        vector<Point> centroids = k_means(points, k);

        cout << "Centroids:\n";
        for (const auto& centroid : centroids) {
            cout << "(" << centroid.x << ", " << centroid.y << ")\n";
        }

        return 0;
    }
    ```

3. **Megerősítő tanulás (Reinforcement Learning)**:
    - Az algoritmus tanul egy környezettel való interakciókon keresztül, egy visszajelzési mechanizmus segítségével. A cél az, hogy maximálisan növelje a "jutalmat" azáltal, hogy megtanulja, mely akciók vezetnek a legjobb eredményekhez.
    - Példák: Q-learning, Deep Q Networks (DQN).

##### Gépi tanulás folyamat

A gépi tanulás folyamatának legfontosabb lépései a következők:

1. **Adatgyűjtés**: Az adatok beszerzése, amelyek szükségesek a modell tréningéhez.
2. **Adatelőkészítés**: Az adatok tisztítása, normalizálása és feldolgozása.
3. **Modellválasztás**: A megfelelő algoritmus kiválasztása az adott probléma megoldására.
4. **Modelltréning**: A modell betanítása az adatok segítségével.
5. **Értékelés**: A modell pontosságának és teljesítményének mérése különböző metrikák segítségével.
6. **Finomhangolás**: A modell finomhangolása, hogy még jobb teljesítményt érjen el.
7. **Deployálás**: A kész modell produkciós környezetbe helyezése és alkalmazása.

#### Összegzés

Az adatbányászat és gépi tanulás közös vonása, hogy mindkettő hatalmas mennyiségű adatot dolgoz fel és analizál. Mindkét területen az alapos adatgyűjtés és adatelőkészítés kulcsfontosságú, mivel az adatok minősége közvetlenül befolyásolja az algoritmusok teljesítményét. Az adatbányászat a felfedezésre és a mintázatok feltárására koncentrál, míg a gépi tanulás a tanulásra és predikcióra összpontosít. Az adatbányászat és gépi tanulás valamennyi területen alkalmazható, beleértve az üzleti döntéshozatalt, képfelismerést, természetes nyelvfeldolgozást és még sok mást. Ezen technikák mélyebb megértése kulcsfontosságú azoknak, akik szeretnék kihasználni az adatközpontú világ adta lehetőségeket.

**1. Felügyelt és felügyelet nélküli tanulás**

A gépi tanulás (Machine Learning, ML) két fő kategóriába sorolható: felügyelt (supervised) és felügyelet nélküli (unsupervised) tanulás. Mindkét megközelítés alapvető fontosságú az adatbányászat és az intelligens rendszerek kialakítása során.

### Felügyelt tanulás (Supervised Learning)

A felügyelt tanulás egy olyan gépi tanulási paradigma, amelyben egy algoritmus egy ismert „igazság” alapján tanul meg összefüggéseket feltárni a bemeneti adatok között. Az adatok két komponensből állnak: a bemeneti változókból (predictors vagy features) és a célváltozóból (label vagy target). Az algoritmus célja, hogy a bemeneti változók és a célváltozó közötti kapcsolatot feltárja úgy, hogy később új, nem látott adatpontokra is megfelelően tudjon predikciót adni.

#### Példák a felügyelt tanulásra:

1. **Regresszió (Regression)**: 
    - A célváltozó értéke folytonos.
    - Példa algoritmusok: Lineáris regresszió, polinomiális regresszió, Ridge és Lasso regresszió, stb.

2. **Klasszifikáció (Classification)**:
    - A célváltozó értéke diszkrét kategória.
    - Példa algoritmusok: Logisztikus regresszió (Logistic Regression), k-legközelebbi szomszéd (K-Nearest Neighbors), döntési fák (Decision Trees), random erdők (Random Forests), támogatói-vektorgépek (Support Vector Machines), ideghálók (Neural Networks).

### Felügyelet nélküli tanulás (Unsupervised Learning)

A felügyelet nélküli tanulás olyan technika, ahol nincs előre definiált célváltozó. Az algoritmusnak az a feladata, hogy a bemeneti adatok közötti rejtett struktúrát vagy mintázatokat találjon meg. A felügyelet nélküli tanulás alkalmazásának egyik fő célja a nagy mennyiségű adat elemzése és strukturálása.

#### Példák a felügyelet nélküli tanulásra:

1. **Klaszterezés (Clustering)**:
    - Az algoritmus az adatokat csoportokba (klaszterekbe) osztja aszerint, hogy melyik adatpontok hasonlítanak egymásra.
    - Példa algoritmusok: k-közép (k-means), hierarchikus klaszterezés (Hierarchical clustering), központi-anomáliás klaszterezés (DBSCAN).

2. **Dimenziócsökkentés (Dimensionality Reduction)**:
    - A magas dimenziójú adatok vizualizálására és kezelésére.
    - Példa algoritmusok: Főkomponens-analízis (Principal Component Analysis, PCA), T-SNE, Isomap.

### Részletes Technikai Elemzés

#### Felügyelt Tanulás

**1. Lineáris Regresszió (Linear Regression)**

A lineáris regresszió az egyik legegyszerűbb és leggyakrabban használt regressziós technika. Az alapfeltevés az, hogy a célváltozó (Y) lineárisan függ a bemeneti változóktól (X). A formális matematikai modellt az alábbi egyenlet adja meg:

$$ Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \ldots + \beta_nX_n + \epsilon $$

ahol:
- $\beta_0, \beta_1, \ldots, \beta_n$ a modell paraméterei (súlyai),
- $\epsilon$ a hiba term (noise).

**2. Döntési Fák (Decision Trees)**

A döntési fák egy modellezési technika, amely egy input változók alapja alapján fa struktúrában hoz döntéseket. Minden belső csomópont egy attribútumot reprezentál, az ágak a lehetséges attribútum értékeket, a levelek (levélcsomópontok) pedig az eredményeket (predikciókat). A fa struktúráját gráfelmélet fogalmakkal is fel lehet írni.

**3. Támogatói-vektorgépek (Support Vector Machines)**

A támogatói-vektorgépek (SVM) célja, hogy egy optimális hipersíkot találjon, amely a legjobban elválasztja a különböző osztályokba tartozó adatpontokat. Az alapvető ötlet az, hogy növelni kell a két osztály közötti távolságot (margin), miközben minimalizálják a hibás besorolások számát.

```cpp
#include <iostream>
#include <vector>
#include <cmath>

// Example of a linear regression model in C++
class LinearRegression {
private:
    std::vector<double> weights;
    double bias;
    
    double predict(const std::vector<double>& x) {
        double prediction = bias;
        for (size_t i = 0; i < x.size(); ++i) {
            prediction += weights[i] * x[i];
        }
        return prediction;
    }
    
public:
    LinearRegression(size_t num_features) {
        weights.resize(num_features, 0.0);
        bias = 0.0;
    }
    
    void fit(const std::vector<std::vector<double>>& X, const std::vector<double>& y, double learning_rate = 0.01, int epochs = 1000) {
        size_t n_samples = X.size();
        size_t n_features = X[0].size();
        
        for (int e = 0; e < epochs; ++e) {
            for (size_t i = 0; i < n_samples; ++i) {
                double y_pred = predict(X[i]);
                double error = y_pred - y[i];
                
                bias -= learning_rate * error;
                for (size_t j = 0; j < n_features; ++j) {
                    weights[j] -= learning_rate * error * X[i][j];
                }
            }
        }
    }
    
    std::vector<double> predict(const std::vector<std::vector<double>>& X) {
        std::vector<double> predictions;
        for (const auto& x : X) {
            predictions.push_back(predict(x));
        }
        return predictions;
    }
};

int main() {
    std::vector<std::vector<double>> X = {{1, 1}, {2, 2}, {3, 3}, {4, 4}};
    std::vector<double> y = {2, 4, 6, 8};
    
    LinearRegression lr(2);
    lr.fit(X, y);
    
    auto predictions = lr.predict(X);
    for (const auto& pred : predictions) {
        std::cout << "Prediction: " << pred << std::endl;
    }
    
    return 0;
}
```

#### Felügyelet Nélküli Tanulás

**1. k-közép algoritmus (k-means algorithm)**

A k-közép klaszterezés célja, hogy az adatokat k számú klaszterbe ossza, ahol a klaszterek középpontjai (centroids) határozzák meg a klaszterek középpontját. Az algoritmus lépései:
1. Válassz k számú kezdeti középpontot.
2. Addig ismételd az alábbi lépéseket, amíg a középpontok konvergálnak:
    a. Rendelje az egyes adatpontokat a legközelebbi középponthoz.
    b. Számolja ki újra az egyes klaszterek középpontjait.

**2. Főkomponens-analízis (Principal Component Analysis, PCA)**

A PCA egy dimenziócsökkentő technika, amely az eredeti adatok varianciáját megőrző új koordinátarendszert hoz létre. A cél az, hogy a főkomponensek (principal components) viszonylag kis számával képesek legyünk az eredeti adatok minél nagyobb hányadát reprezentálni. A PCA során az adatokat egy olyan térben vetítik le, ahol az első főkomponens a legnagyobb varianciát tartalmazza, a második pedig az elsőre merőlegesen, stb.

Összegzésként, a felügyelt és felügyelet nélküli tanulás alapvető elvei és technikái elengedhetetlenek a gépi tanulás és adatbányászat területén. E két módszertani megközelítés széleskörű alkalmazást nyer az automatizált folyamatokban, képalapú felismerésben, természetes nyelv feldolgozásban és sok más területen.

