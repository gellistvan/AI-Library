Az adatbányászat világában rejlő minták és összefüggések feltárása a modern adatelemzés alapja. Ebben a fejezetben arra törekszünk, hogy az adatbányászati algoritmusok mélyére ássunk, bemutatva azok működését, alkalmazását és hatékonyságát. Különös figyelmet fordítunk az asszociációs szabályokra, ahol az Apriori és Eclat algoritmusokat vizsgáljuk meg, amelyek kulcsszerepet játszanak a gyakori elemek és szabályok felfedezésében. Emellett a klaszterezés különböző megközelítéseit is részletezzük, beleértve a K-means és a hierarchikus klaszterezést, melyek lehetővé teszik az adatpontok csoportosítását és a rejtett struktúrák feltárását. Az algoritmusok bemutatása mellett konkrét alkalmazási példákat és részletes elemzéseket is nyújtunk, hogy az olvasó megérthesse ezen módszerek gyakorlati jelentőségét és hozzáadott értékét a különböző adathelyzetekben. Reméljük, hogy ezek az ismeretek hozzájárulnak az adatbányászat iránti érdeklődés növekedéséhez és gyakorlati alkalmazásához.

### 2.1 Asszociációs szabályok (Apriori, Eclat)

#### Bevezetés az Asszociációs Szabályokba

Az asszociációs szabályok az adatbányászat egyik legnépszerűbb és leggyakrabban alkalmazott technikája, amely célja a nagyméretű adatbázisokban található érdekes minták és kapcsolatok felfedezése. Ezek az algoritmusok különösen hasznosak a kereskedelemben, ahol például a vásárlási kosarak elemzésére használják annak érdekében, hogy megértsék, mely termékeket vásárolják meg a vásárlók együtt. 

Az asszociációs szabályok létrehozásának tipikus példája a következő. Tegyük fel, hogy van egy szupermarket, ahol különböző termékeket forgalmazunk. Az asszociációs szabályok segítségével kideríthetjük, hogy azok a vásárlók, akik kenyeret vásárolnak, gyakran vajat is vesznek. Ezt információt felhasználva a bolt optimalizálhatja áruházi elrendezését vagy promóciókat.

#### Az Apriori algoritmus

Az Apriori algoritmus az egyik legismertebb és leggyakrabban használt algoritmus az asszociációs szabályok feltárására. Az algoritmus fundamentális ötlete az, hogy egy elemhalmaz frekvenciája csak akkor lehet nagy, ha minden részhalmaza is nagy frekvenciájú. Ez a „merészebb lépés” elven alapuló megközelítés csökkenti az elemzéshez szükséges számítási kapacitást, mivel nem kell minden lehetséges elemhalmazt megvizsgálni.

##### Az algoritmus lépései:

1. **Kezdeti Halmaz Generálása (Generálj 1-itemset jelölteket):**
   - Minden egyes adatot (tételt) külön elemként kezel.
   - Számítja az egyes tételek támogatását (support), ami az adott tételt tartalmazó tranzakciók számát jelenti az összes tranzakcióhoz viszonyítva.

2. **K lépés (k-itemsetek generálása):**
   - Kombinálja az előző lépésben generált k-1 itemseteket (részhalmazokat), hogy k-itemseteket hozzon létre.
   - Szűrje ki azokat a k-itemseteket, amelyek támogatása kisebb, mint a megadott minimum támogatási küszöb (minimum support threshold).

3. **Asszociációs szabályok létrehozása:**
   - Minden releváns elem, amely megfelel a minimum támogatási küszöbnek, további feldolgozásra kerül.
   - Kisúrozza azokat a szabályokat, amelyek nem felelnek meg az előírt minimum bizalom (confidence) értékének.

##### Pseudocode in C++:

```cpp
#include <iostream>

#include <vector>
#include <map>

#include <set>

// Function to calculate support of itemsets
std::map<std::set<int>, double> calculateSupport(const std::vector<std::set<int>>& transactions, const std::set<std::set<int>>& itemsets) {
    std::map<std::set<int>, double> supportCount;
    for (const auto& transaction : transactions) {
        for (const auto& itemset : itemsets) {
            if (std::includes(transaction.begin(), transaction.end(), itemset.begin(), itemset.end())) {
                supportCount[itemset]++;
            }
        }
    }
    // Convert count to support
    for (auto& pair : supportCount) {
        pair.second /= transactions.size();
    }
    return supportCount;
}

// Function to generate candidate itemsets
std::set<std::set<int>> generateCandidates(const std::set<std::set<int>>& frequentItemsets, int k) {
    std::set<std::set<int>> candidates;
    for (auto it1 = frequentItemsets.begin(); it1 != frequentItemsets.end(); ++it1) {
        for (auto it2 = std::next(it1); it2 != frequentItemsets.end(); ++it2) {
            std::set<int> candidate(*it1);
            candidate.insert(it2->begin(), it2->end());
            if (candidate.size() == k) {
                candidates.insert(candidate);
            }
        }
    }
    return candidates;
}

int main() {
    std::vector<std::set<int>> transactions = {
        {1, 2, 3, 4},
        {2, 3, 4},
        {2, 3},
        {1, 2, 4},
        {1, 2, 3, 4}
    };
    double minSupport = 0.6; // Minimum support threshold
    std::set<std::set<int>> frequentItemsets;
    std::set<std::set<int>> candidates;
    
    // Generate initial itemsets (1-itemsets)
    for (const auto& transaction : transactions) {
        for (int item : transaction) {
            frequentItemsets.insert({item});
        }
    }
    
    int k = 2;
    while (!frequentItemsets.empty()) {
        // Calculate support for current itemsets
        auto supportCount = calculateSupport(transactions, frequentItemsets);
        
        // Filter itemsets by support threshold
        frequentItemsets.clear();
        for (const auto& pair : supportCount) {
            if (pair.second >= minSupport) {
                frequentItemsets.insert(pair.first);
            }
        }
        
        if (!frequentItemsets.empty()) {
            // Generate new candidates
            candidates = generateCandidates(frequentItemsets, k);
            ++k;
        }
    }
    
    return 0;
}
```

##### Előnyök:
- Könnyű megérteni és implementálni.
- Hatékony megközelítési mód a gyakori elemhalmazok azonosítására.

##### Hátrányok:
- Számítási költsége magas nagy elemhalmazok esetén.
- Nagy memóriaigény.

#### Az Eclat Algoritmus

Az Eclat (Equivalence Class Clustering and bottom-up Lattice Traversal) algoritmus az Apriori egyik alternatívája, amely más megközelítést alkalmaz a gyakori elemhalmazok azonosítására. Az algoritmus alapja a vertikális adatreprezentáció, ahol az elemek halmazait tranzakciók indexeivel reprezentáljuk.

##### Az algoritmus lépései:

1. **Tranzakciós adatokat vertikálisan reprezentálni:**
   - Minden tételt megjelenít tranzakciós indexek listájával.

2. **Metszet (Intersection) számítás:**
   - Az elemsorok metszetének segítségével generál k-itemseteket.
   - Számolja a kapott itemsetek támogatását és szűri ki azokat, amelyek nem felelnek meg a megadott minimális támogatásnak.

3. **Rekurrens láncolás:**
   - Az algoritmus rekurrenciával tovább folytatja a metszési folyamatokat a jól teljesítő itemsetekre, míg be nem fejeződik az egész processzus.

##### Pseudocode in C++:

```cpp
#include <iostream>

#include <vector>
#include <map>

#include <set>

// Function to intersect sets of transaction indices
std::set<int> intersect(const std::set<int>& set1, const std::set<int>& set2) {
    std::set<int> result;
    std::set_intersection(set1.begin(), set1.end(), set2.begin(), set2.end(), std::inserter(result, result.begin()));
    return result;
}

int main() {
    // Example transactions
    std::vector<std::set<int>> transactions = {
        {1, 2, 3, 4},
        {2, 3, 4},
        {2, 3},
        {1, 2, 4},
        {1, 2, 3, 4}
    };
    
    double minSupport = 0.6;
    
    // Generate initial item-support data using vertical representation
    std::map<int, std::set<int>> verticalDb;
    for (int i = 0; i < transactions.size(); ++i) {
        for (int item : transactions[i]) {
            verticalDb[item].insert(i);
        }
    }
    
    // Generate frequent itemsets
    std::map<std::set<int>, std::set<int>> frequentItemsets;
    for (const auto& pair : verticalDb) {
        if (pair.second.size() / static_cast<double>(transactions.size()) >= minSupport) {
            frequentItemsets[{pair.first}] = pair.second;
        }
    }
    
    int k = 2;
    while (!frequentItemsets.empty()) {
        std::map<std::set<int>, std::set<int>> candidateItemsets;
        
        // Generate candidates by intersecting sets of transaction indices
        for (auto it1 = frequentItemsets.begin(); it1 != frequentItemsets.end(); ++it1) {
            for (auto it2 = std::next(it1); it2 != frequentItemsets.end(); ++it2) {
                std::set<int> candidate(it1->first);
                candidate.insert(it2->first.begin(), it2->first.end());
                if (candidate.size() == k) {
                    std::set<int> intersectedIndices = intersect(it1->second, it2->second);
                    if (intersectedIndices.size() / static_cast<double>(transactions.size()) >= minSupport) {
                        candidateItemsets[candidate] = intersectedIndices;
                    }
                }
            }
        }
        
        if (candidateItemsets.empty()) {
            break;
        }
        
        frequentItemsets = candidateItemsets;
        ++k;
    }

    return 0;
}
```

##### Előnyök:
- Gyorsabb, mint az Apriori nagy, ritka mátrixok esetén.
- Hatékonyabb memóriahasználat vertikálisan tárolt adatstruktúrák miatt.

##### Hátrányok:
- Rekurzív természete miatt mély rekurzió esetén nagy memóriahasználatot igényelhet.

#### Kiválasztás és Alkalmazások

Az Apriori és Eclat algoritmusok különböző kontextusokban való alkalmazhatósága jelentősen eltérhet, az adat halmazától és a konkrét alkalmazási területektől függően.

**Apriori**: 
- Nagyobb adatbázisok, de korlátozott számú tétel esetén hatékony.
- Széles körben használják kereskedelmi adatelemzésekre (pl. kosárelemzés).

**Eclat**:
- Nagy, ritka adatmázia esetén ajánlott.
- Bioinformatikában és szociális háló elemzésben gyakran alkalmazzák.

#### Zárszó

Az asszociációs szabályok, különösen az Apriori és Eclat algoritmusok kulcsfontosságú szerepet játszanak az adatbányászatban. Alkalmazásuk révén mélyebb betekintést nyerhetünk a rejtett mintákba és kapcsolatokba, amelyek hozzájárulhatnak a stratégiai döntésmeghozatalhoz. Az egyes algoritmusok különböző előnyei és hátrányai lehetővé teszik az adatok és az elemzés igényeinek legmegfelelőbb választást. A tudomány és az üzleti intelligencia világában ezek az algoritmusok elengedhetetlen eszközök az összetett adatok megértésében és elemzésében.

### 2. Adatbányászati algoritmusok

#### Algoritmusok és alkalmazások

Az adatbányászati algoritmusok széles skálája található meg a szakirodalomban, és mindegyik különböző alkalmazási területeken használatos. Az alábbiakban két kritikus fontosságú algoritmus családot tárgyalunk részletesen: az **asszociációs szabályok** és a **klaszterezési algoritmusok**.

### Asszociációs Szabályok

#### Apriori Algoritmus

Az asszociációs szabályok célja az, hogy gyakran előforduló mintákat és kapcsolódási szabályokat fedezzen fel az adathalmazban. Az Apriori algoritmus talán a legismertebb és leggyakrabban használt módszer ezen a területen. A működése során rekurzív módon épít fel növekvő méretű elemekből álló halmazokat, majd ezekből generál gyakori elemhalmazokat.

**Az Apriori algoritmus lépései**:

1. **Elemhalmazok generálása**: Inicializálódik a gyakori 1-elemű halmazokkal, azaz az egyedi elemekkel, amelyek adott gyakorisági küszöböt (support threshold) elérnek.
2. **Halmazok bővítése**: Az aktuális halmazokat egy elem hozzáadásával bővítjük, létrehozva az egy szinttel magasabb gyakori halmazokat. 
3. **Pruning (metszés)**: Az új halmazok közül kiszűrjük azokat, amelyek nem felelnek meg a gyakorisági küszöbnek.
4. **Ismétlés**: A folyamat addig ismétlődik, amíg nincs már új, gyakori halmaz generálható.
5. **Asszociációs szabályok generálása**: A gyakori halmazokból asszociációs szabályokat hozunk létre, és értékeljük őket meghatározott mérőszámok (confidence és lift) alapján.

**C++ példakód az Apriori algoritmushoz**:

```cpp
#include <iostream>

#include <vector>
#include <set>

#include <map>
#include <algorithm>

#include <iterator>

// Helper function to print sets
void printSet(const std::set<int>& s) {
    std::cout << "{ ";
    for (const auto& elem : s) {
        std::cout << elem << " ";
    }
    std::cout << "} ";
}

// Function to generate candidate k-itemsets
std::set<std::set<int>> aprioriGen(const std::set<std::set<int>>& Lk_1) {
    std::set<std::set<int>> Ck;
    for (auto it = Lk_1.begin(); it != Lk_1.end(); ++it) {
        auto jt = it;
        ++jt;
        for (; jt != Lk_1.end(); ++jt) {
            std::set<int> c;
            std::merge(it->begin(), it->end(), jt->begin(), jt->end(), std::inserter(c, c.begin()));
            if (c.size() == it->size() + 1) {
                Ck.insert(c);
            }
        }
    }
    return Ck;
}

// Function to count the support of itemsets
std::map<std::set<int>, int> countSupport(const std::vector<std::set<int>>& transactions, const std::set<std::set<int>>& Ck) {
    std::map<std::set<int>, int> supportCount;
    for (const auto& transaction : transactions) {
        for (const auto& candidate : Ck) {
            if (std::includes(transaction.begin(), transaction.end(), candidate.begin(), candidate.end())) {
                supportCount[candidate]++;
            }
        }
    }
    return supportCount;
}

// Apriori algorithm implementation
void apriori(const std::vector<std::set<int>>& transactions, int minSupport) {
    std::set<std::set<int>> L1;
    std::map<std::set<int>, int> supportCount;

    // Count the occurrences of each 1-itemset
    for (const auto& transaction : transactions) {
        for (const auto& item : transaction) {
            std::set<int> itemSet = { item };
            supportCount[itemSet]++;
        }
    }

    // Filter out non-frequent 1-itemsets
    for (const auto& pair : supportCount) {
        if (pair.second >= minSupport) {
            L1.insert(pair.first);
        }
    }

    std::set<std::set<int>> Lk = L1;
    int k = 1;

    while (!Lk.empty()) {
        std::cout << "Frequent " << k << "-itemsets:\n";
        for (const auto& itemset : Lk) {
            printSet(itemset);
            std::cout << "\n";
        }

        auto Ck = aprioriGen(Lk);
        supportCount = countSupport(transactions, Ck);
        Lk.clear();
        
        for (const auto& pair : supportCount) {
            if (pair.second >= minSupport) {
                Lk.insert(pair.first);
            }
        }
        k++;
    }
}

int main() {
    std::vector<std::set<int>> transactions = {
        {1, 2, 3}, 
        {1, 2, 4}, 
        {1, 3, 4}, 
        {2, 3, 4},
        {2, 3}
    };
    int minSupport = 2;

    apriori(transactions, minSupport);

    return 0;
}
```

#### Eclat Algoritmus

Az Eclat algoritmus eltér az Apriori algoritmustól abban, hogy egy keresési fát használ a gyakori elemhalmazok generálására. Ebben a felfogásban minden elem egy elágazásnak tekinthető, és a megadott elemhalmaz kibővítése további elágazásokat eredményez.

**Az Eclat algoritmus lépései**:

1. **Elemhalmazok reprezentációja**: Az adatstruktúra, amit az Eclat használ, inkább vertikális, mint horizontális. Az egyes elemeket azok előfordulási halmazaként (transaction ID set, TID set) képviseli.
2. **Rekurzív feltárás**: Gyakori elemhalmazok keresése rekurzív módon történik a transzformált adatszerkezetben.
3. **Intersekció alapú bővítés**: Az egyes elemhalmazok bővítése a halmazok metszésével történik.

### Klaszterezési Algoritmusok

#### K-means Algoritmus

A K-means algoritmus egyik legismertebb klaszterezési módszer, és az adatok egy előre meghatározott számú klaszterbe történő csoportosítására szolgál. Az algoritmus iteratív nem-felügyelt tanulási módszertanra épül.

**Az K-means algoritmus lépései**:

1. **Centroidok inicializálása**: Random módon kezdjük a `k` klaszter centrumainak (centroids) kiválasztását.
2. **Hozzáadási lépés**: Az egyes adatpontok hozzárendelése ahhoz a centrálódhoz történik, amelyhez a legközelebb helyezkednek el (euklideszi távolság alapján).
3. **Centroidok frissítése**: Minden klaszterhez tartozó adatpontok átlagát kiszámítva frissítjük a centroidok helyzetét.
4. **Ismétlés**: Az iteráció addig folytatódik, amíg a centroidok nem változnak vagy adott számú iterációnál nem állunk meg.

#### Hierarchikus Klaszterezés

A hierarchikus klaszterezési eljárások az adatokat hierarchikus fa struktúrában rendezik el, ami lehetővé teszi az adatok több szinten történő elemzését. Két fő típusa van:

1. **Agglomeratív** (bottom-up approach): Kezdetben minden adatpont egy külön klaszter. Iteratív módon a legközelebbi klaszterek összeolvadnak, amíg egyetlen klaszter marad.
2. **Divizív** (top-down approach): Kezdetben az összes adat egy klaszterben van. Iteratív módon egyes klaszterek szétválnak, amíg minden adat külön klaszterben van.

**Agglomeratív Hierarchikus Klaszterezés lépései**:

1. **Inicializáció**: Minden egyes adatpontot külön klaszternek tekintünk.
2. **Legközelebbi klaszterek kiválasztása**: Az adatpontok közötti távolságok alapján kiválasztjuk a legközelebbi két klasztert.
3. **Klaszterek összevonása**: Összevonjuk ezt a két klasztert egyesítve őket.
4. **Ismétlés**: A folyamat mindaddig folytatódik, amíg csak egyetlen klaszter marad.

Ez a részletes vizsgálat betekintést nyújt két fontos adatbányászati algoritmus családba: az asszociációs szabályok és a klaszterezési algoritmusok világába, különös tekintettel az Apriori, Eclat, K-means és Hierarchikus klaszterezési módszerekre. Minden algoritmus különböző alkalmazási helyzetekben nyújt értékes analitikai eszközöket, melyek segítségével gazdag információt nyerhetünk ki nagy mennyiségű adatból.

## 2. Adatbányászati algoritmusok

### Klaszterezés (K-means, Hierarchikus klaszterezés)

A klaszterezés egy alapvetően fontos feladat az adatbányászat és gépi tanulás területén, amely az adatok csoportokba rendezésére szolgál oly módon, hogy az ugyanazon csoportba tartozó elemek hasonlóbbak egymáshoz, mint más csoportok elemeihez. Ennek a fejezetnek az a célja, hogy részletesen bemutassa két jelentős klaszterezési algoritmust: a K-means klaszterezést és a hierarchikus klaszterezést. Mindkét algoritmusnak megvannak a maga sajátosságai, előnyei és hátrányai.

#### 1. K-means Klaszterezés

##### Algoritmus

A K-means klaszterezés egy népszerű és egyszerű partícionáló algoritmus, amely a megfigyeléseket K klaszterbe rendezi oly módon, hogy minimalizálja a klaszterek közötti varianciát. Az algoritmus a következő lépésekből áll:

1. **Inicializálás**: Válasszunk ki K pontot (kezdeti klaszterközéppontokat) az adatpontok közül. Ezek az elsődleges klaszterközéppontok.
2. **Assignálás (hozzárendelés)**: Rendeljük hozzá minden adatpontot a hozzá legközelebbi klaszterközépponthoz, így K klasztert alkotva.
3. **Frissítés**: A klaszterekhez rendelt adatpontok alapján számoljuk ki az egyes klaszterek új középpontjait.
4. **Iteráció**: Ismételjük meg az előző két lépést addig, amíg a klaszterközéppontok már nem változnak (vagy a változás elhanyagolható).
5. **Konvergencia**: Az algoritmus akkor ér véget, amikor a klaszterek középpontjai már nem változnak az iterációk során.

##### Matematikai Formuláció

A K-means algoritmus célja, hogy minimalizálja a következő célfüggvényt:

$$ J = \sum_{j=1}^{k} \sum_{i=1}^{n} \| x_i^{(j)} - \mu_j \|^2 $$

ahol:
- $k$ a klaszterek száma,
- $n$ az adatpontok száma,
- $x_i^{(j)}$ az i-edik adatpont a j-edik klaszterben,
- $\mu_j$ a j-edik klaszter középpontja.

##### Példakód (C++)

```cpp
#include <iostream>

#include <vector>
#include <cmath>

#include <limits>
#include <cstdlib>

// Helper function to calculate Euclidean distance
double euclidean_distance(const std::vector<double>& point1, const std::vector<double>& point2) {
    double sum = 0.0;
    for(size_t i = 0; i < point1.size(); ++i) {
        sum += std::pow(point1[i] - point2[i], 2);
    }
    return std::sqrt(sum);
}

// Initialize centroids randomly
void initialize_centroids(const std::vector<std::vector<double>>& data, std::vector<std::vector<double>>& centroids, int k) {
    std::srand(std::time(nullptr));
    for(int i = 0; i < k; ++i) {
        centroids.push_back(data[std::rand() % data.size()]);
    }
}

// Assign each data point to the nearest centroid
void assign_clusters(const std::vector<std::vector<double>>& data, const std::vector<std::vector<double>>& centroids, std::vector<int>& labels) {
    for(size_t i = 0; i < data.size(); ++i) {
        double min_distance = std::numeric_limits<double>::max();
        int cluster = 0;
        for(size_t j = 0; j < centroids.size(); ++j) {
            double distance = euclidean_distance(data[i], centroids[j]);
            if(distance < min_distance) {
                min_distance = distance;
                cluster = j;
            }
        }
        labels[i] = cluster;
    }
}

// Update centroids based on current assignments
void update_centroids(const std::vector<std::vector<double>>& data, const std::vector<int>& labels, std::vector<std::vector<double>>& centroids, int k) {
    std::vector<int> count(k, 0);
    centroids.assign(k, std::vector<double>(data[0].size(), 0.0));
    for(size_t i = 0; i < data.size(); ++i) {
        int cluster = labels[i];
        for(size_t j = 0; j < data[i].size(); ++j) {
            centroids[cluster][j] += data[i][j];
        }
        count[cluster]++;
    }
    for(size_t i = 0; i < centroids.size(); ++i) {
        for(size_t j = 0; j < centroids[i].size(); ++j) {
            centroids[i][j] /= count[i];
        }
    }
}

void kmeans(const std::vector<std::vector<double>>& data, int k, int max_iterations) {
    std::vector<std::vector<double>> centroids;
    std::vector<int> labels(data.size(), 0);
    initialize_centroids(data, centroids, k);
    for(int it = 0; it < max_iterations; ++it) {
        assign_clusters(data, centroids, labels);
        update_centroids(data, labels, centroids, k);
    }

    // Output the result
    for(size_t i = 0; i < data.size(); ++i) {
        std::cout << "Point " << i << " is in cluster " << labels[i] << std::endl;
    }
}

int main() {
    std::vector<std::vector<double>> data = {
        {1.0, 2.0},
        {1.5, 1.8},
        {5.0, 8.0},
        {8.0, 8.0},
        {1.0, 0.6},
        {9.0, 11.0},
        {8.0, 2.0},
        {10.0, 2.0},
        {9.0, 3.0}
    };

    int k = 3;
    int max_iterations = 100;
    kmeans(data, k, max_iterations);

    return 0;
}
```

##### Alkalmazások és Teljesítmény

A K-means algoritmus széleskörűen alkalmazható különféle területeken, mint például:

- **Ügyfélszegmentáció**: A marketing területén az ügyfeleket különböző célcsoportokra oszthatjuk.
- **Kép- és videófeldolgozás**: Színtartományok redukálása és képek egyszerűsítése.
- **Bioinformatika**: Genetikai és proteomikai adatok csoportosítása.

A K-means algoritmus hatékonysága nagyban függ a középpontok inicializálásának módjától, és gyakran szükséges több futtatás különböző inicializálásokkal a globális minimum eléréséhez. Az algoritmus legnagyobb hátránya, hogy érzékeny a zajos adatokra és az outlierekre (szélsőséges értékek).

#### 2. Hierarchikus Klaszterezés

##### Algoritmus

A hierarchikus klaszterezés két fő típusa van: az agglomeratív (alsó-felül) és a divizív (felül-alul). Az agglomeratív klaszterezés a leggyakrabban használt, és az alábbi lépésekből áll:

1. **Inicializálás**: Minden adatpont saját klaszterként indul.
2. **Csoportosítás**: Iteratívan egyesítsük a két legközelebbi klasztert.
3. **Frissítés**: Frissítsük a távolságokat az új klaszter és a többi klaszter között.
4. **Konvergencia**: Az algoritmus addig folytatódik, amíg egyetlen klaszterbe nem áll össze az összes pont.

##### Matematikai Formuláció

Az agglomeratív hierarchikus klaszterezés különböző távolságmértékeket használhat, például:

- **Egyszerű Lánc (Single Linkage)**: A két klaszter legközelebbi pontjai közötti távolságot veszi figyelembe.
- **Teljes Lánc (Complete Linkage)**: A két klaszter legtávolabbi pontjai közötti távolságot veszi figyelembe.
- **Átlagos Kapcsolat (Average Linkage)**: A klaszterek összes pontjának átlagos távolságát veszi figyelembe.

##### Példakód (C++)

```cpp
#include <iostream>

#include <vector>
#include <cmath>

#include <limits>

// Helper function to calculate Euclidean distance
double euclidean_distance(const std::vector<double>& point1, const std::vector<double>& point2) {
    double sum = 0.0;
    for(size_t i = 0; i < point1.size(); ++i) {
        sum += std::pow(point1[i] - point2[i], 2);
    }
    return std::sqrt(sum);
}

// Find the two closest clusters
std::pair<int, int> closest_clusters(const std::vector<std::vector<double>>& distance_matrix) {
    double min_distance = std::numeric_limits<double>::max();
    std::pair<int, int> min_pair = {0, 0};
    for(size_t i = 0; i < distance_matrix.size(); ++i) {
        for(size_t j = i + 1; j < distance_matrix[i].size(); ++j) {
            if (distance_matrix[i][j] < min_distance) {
                min_distance = distance_matrix[i][j];
                min_pair = {i, j};
            }
        }
    }
    return min_pair;
}

// Update the distance matrix after merging two clusters
void update_distance_matrix(std::vector<std::vector<double>>& distance_matrix, int cluster1, int cluster2) {
    for(size_t i = 0; i < distance_matrix.size(); ++i) {
        if (i != cluster1 && i != cluster2) {
            distance_matrix[cluster1][i] = distance_matrix[i][cluster1] = std::min(distance_matrix[cluster1][i], distance_matrix[cluster2][i]);
        }
    }
    distance_matrix[cluster1][cluster2] = distance_matrix[cluster2][cluster1] = std::numeric_limits<double>::max();
}

void hierarchical_clustering(const std::vector<std::vector<double>>& data) {
    size_t n = data.size();
    std::vector<std::vector<double>> distance_matrix(n, std::vector<double>(n, 0.0));
    
    for(size_t i = 0; i < n; ++i) {
        for(size_t j = i + 1; j < n; ++j) {
            double distance = euclidean_distance(data[i], data[j]);
            distance_matrix[i][j] = distance_matrix[j][i] = distance;
        }
    }
    
    while(n > 1) {
        auto [cluster1, cluster2] = closest_clusters(distance_matrix);
        update_distance_matrix(distance_matrix, cluster1, cluster2);
        --n;
        std::cout << "Merged cluster " << cluster1 << " and " << cluster2 << std::endl;
    }
}

int main() {
    std::vector<std::vector<double>> data = {
        {1.0, 2.0},
        {1.5, 1.8},
        {5.0, 8.0},
        {8.0, 8.0},
        {1.0, 0.6},
        {9.0, 11.0},
        {8.0, 2.0},
        {10.0, 2.0},
        {9.0, 3.0}
    };

    hierarchical_clustering(data);

    return 0;
}
```

##### Alkalmazások és Teljesítmény

A hierarchikus klaszterezés is számos területen alkalmazható, többek között:

- **Dokumentumosztályozás**: Dokumentumok csoportosítása tartalmuk alapján.
- **Baktériumok osztályozása**: Genetikai adatok alapján történő klaszterezés.
- **Árvizsgálat**: Különböző árucikkek csoportosítása vásárlási szokások alapján.

A hierarchikus klaszterezés előnye, hogy nem szükséges előre meghatározni a klaszterek számát, és az eredmény vizualizálása dendrogrammákkal is lehetséges. Ennek ellenére a számítási komplexitása miatt nagy adathalmazok esetén kevésbé hatékony, mint a K-means algoritmus.

### Összegzés

A klaszterezési algoritmusok alapvető szerepet játszanak az adatok csoportosításában és elemzésében. A K-means és a hierarchikus klaszterezés két jól ismert módszer, amelyek különböző jellemzőkkel és alkalmazásokkal rendelkeznek. Míg a K-means gyors és hatékony, addig a hierarchikus klaszterezés több információt nyújt a klaszterek közötti kapcsolatról. Mindkét módszer alkalmazása jelentős értékkel bír a gyakorlati adatfeldolgozási feladatokban.

### 2. Adatbányászati algoritmusok

### 2.3 Algoritmusok és elemzések

#### Bevezetés

Az adatbányászati algoritmusok célja, hogy nagy adatmennyiségből hasznos információkat nyerjenek ki. Ez a fejezet az adatbányászati algoritmusok két fontos osztályára koncentrál, az asszociációs szabályokra és a klaszterezési algoritmusokra. Mindkét osztály jelentős szerepet játszik az adatok elemzésében és feldolgozásában különböző területeken, mint például a kereskedelem, orvostudomány, pénzügyi elemzések és sok más.

### Asszociációs szabályok

Az asszociációs szabályok feladata, hogy feltárják az adatbázisban lévő tranzakciók közötti összefüggéseket. Az egyik legismertebb algoritmus az Apriori algoritmus, amelynek célja, hogy gyakran előforduló elemhalmazokat találjon.

#### Apriori Algoritmus

Az Apriori algoritmus alapgondolata a "level-wise search", ami azt jelenti, hogy az algoritmus lépésről lépésre épít fel egyre nagyobb elemhalmazokat, csak azokat az elemeket használva fel, amelyek egy bizonyos gyakorisági küszöböt meghaladnak.

##### Az Apriori algoritmus lépései:

1. **Gyakori elemek azonosítása:** Kezdetben az egyes elemek közül azokat válogatjuk ki, amelyek gyakorisága (support) nagyobb, mint az adott minimum küszöbérték.
2. **Elemhalmazok generálása:** Az első lépésben talált gyakori elemek segítségével generáljuk az összes lehetséges elemhalmazt.
3. **Support számítása:** Minden elemhalmaz gyakoriságát kiszámítjuk.
4. **Pruning:** Azokat az elemhalmazokat, amelyek supportja kisebb a minimum küszöbértéknél, kihagyjuk.
5. **Iteráció:** Ismételjük az elemhalmazok generálását és pruningot, amíg nem találunk új gyakori elemhalmazokat.

##### Példakód (C++)

```cpp
#include <iostream>

#include <vector>
#include <unordered_map>

#include <unordered_set>

using namespace std;

// Function to generate the frequent itemsets of size 1
unordered_map<int, int> generate_candidates(const vector<vector<int>>& transactions, int min_support) {
    unordered_map<int, int> candidates;
    for (const auto& transaction : transactions) {
        for (int item : transaction) {
            candidates[item]++;
        }
    }
    // Remove items that don't meet the minimum support threshold
    for (auto it = candidates.begin(); it != candidates.end(); ) {
        if (it->second < min_support) {
            it = candidates.erase(it);
        } else {
            ++it;
        }
    }
    return candidates;
}

// Example
int main() {
    // Example transactions
    vector<vector<int>> transactions = {
        {1, 2, 3},
        {1, 3, 4},
        {2, 3, 5},
        {1, 2, 3, 5},
        {2, 5}
    };
    int min_support = 2;

    unordered_map<int, int> frequent_itemsets = generate_candidates(transactions, min_support);
    cout << "Frequent Itemsets:\n";
    for (auto& kv : frequent_itemsets) {
        cout << "Item: " << kv.first << " Support: " << kv.second << "\n";
    }

    return 0;
}
```

#### Eclat Algoritmus

Az Eclat algoritmus a vertikális adatszerkezetet használja az elemhalmazok generálásához és időgény megfelelő támogatásának számításához. Az algoritmus keresztülhalad az elemhalmazokon, és iteratív módon bővíti az elemhalmazokat mindaddig, amíg az egyes elemhalmazok támogatása meghaladja a küszöbértéket.

##### Az Eclat algoritmus lépései:

1. **Vertikális adatformátum létrehozása:** Az adathalmaz vertikális formában való tárolása, ahol minden elemhez tartozó tranzakció azonosítók szerepelnek.
2. **Kandidátus elemhalmazok generálása:** Elegendő támogatási értékkel bíró elemkollekciók kezelésével.
3. **Intersect művelet:** Az összes elemhalmazra végrehajtott metszés, amelyen keresztül válik az algoritmus hatékonyabbá.
4. **Iteráció és pruning:** Az elemhalmaz támogatási értékeinek kiszámítása és a nem megfelelő elemhalmazok kihagyása.

##### Eclat algoritmus Vertikális Adatformátumra

```cpp
#include <iostream>

#include <vector>
#include <unordered_map>

#include <unordered_set>

using namespace std;

// Function to generate vertical format transactions
unordered_map<int, unordered_set<int>> generate_vertical_format(const vector<vector<int>>& transactions) {
    unordered_map<int, unordered_set<int>> vertical_format;
    for (int tid = 0; tid < transactions.size(); ++tid) {
        for (int item : transactions[tid]) {
            vertical_format[item].insert(tid);
        }
    }
    return vertical_format;
}

// Eclat algorithm basic function
void eclat(const unordered_map<int, unordered_set<int>>& vertical_format, 
           unordered_set<int> prefix, 
           unordered_set<int> items, 
           int min_support) {
    // Recursive depth-first approach
    while (!items.empty()) {
        auto it = items.begin();
        int item = *it;
        items.erase(it);
        
        // Create new prefix
        unordered_set<int> new_prefix = prefix;
        new_prefix.insert(item);
        
        // Intersection of transactions
        unordered_set<int> new_tids = vertical_format.at(item);
        for (int prefix_item : prefix) {
            unordered_set<int> temp_tids;
            const auto& current_tids = vertical_format.at(prefix_item);
            for (int tid : new_tids) {
                if (current_tids.find(tid) != current_tids.end()) {
                    temp_tids.insert(tid);
                }
            }
            new_tids = move(temp_tids);
        }
        
        // Check support threshold
        if (new_tids.size() >= min_support) {
            // Output the frequent itemset
            cout << "Itemset: ";
            for (int pi : new_prefix) {
                cout << pi << " ";
            }
            cout << "Support: " << new_tids.size() << "\n";
            
            // Recur with remaining items
            eclat(vertical_format, new_prefix, items, min_support);
        }
    }
}

// Example
int main() {
    // Example transactions
    vector<vector<int>> transactions = {
        {1, 2, 3},
        {1, 3, 4},
        {2, 3, 5},
        {1, 2, 3, 5},
        {2, 5}
    };
    int min_support = 2;

    unordered_map<int, unordered_set<int>> vertical_format = generate_vertical_format(transactions);
    
    // Start Eclat algorithm
    unordered_set<int> prefix, items;
    for (const auto& kv : vertical_format) {
        items.insert(kv.first);
    }
    eclat(vertical_format, prefix, items, min_support);

    return 0;
}
```

### Klaszterezési Algoritmusok

A klaszterezési algoritmusok célja, hogy az adatpontokat csoportosítsák, ahol az azonos klaszterbe tartozó pontok hasonlóak egymáshoz, míg különböző klaszterekbe tartozó pontok kevésbé hasonlóak.

#### K-means Klaszterezés

A K-means egy népszerű partíciós klaszterezési algoritmus, amely előre meghatározott számú klaszterbe (k) osztja az adathalmazt.

##### Az K-means algoritmus lépései:

1. **Inicializáció:** Véletlenül kiválasztunk K adatpontot, amelyek az inicializált klaszter centroidok lesznek.
2. **Klaszter tagok hozzárendelése:** Minden adatpontot a legközelebbi centroidhoz rendelünk hozzá.
3. **Centroid frissítése:** Az egyes klaszterekhez tartozó adatpontok átlagaként újra számítjuk a centroidokat.
4. **Ismétlés:** Ismételjük a tagok hozzárendelését és a centroid frissítését, amíg a centroidok nem változnak jelentősen vagy egy előre meghatározott iterációs számot el nem érünk.

##### Példakód (C++)

```cpp
#include <iostream>

#include <vector>
#include <cmath>

using namespace std;

struct Point {
    double x, y;
};

double distance(const Point& p1, const Point& p2) {
    return sqrt((p1.x - p2.x) * (p1.x - p2.x) + (p1.y - p2.y) * (p1.y - p2.y));
}

void kmeans(vector<Point>& points, int k, int max_iterations) {
    vector<Point> centroids(k);
    // Randomly initialize centroids
    for (int i = 0; i < k; ++i) {
        centroids[i] = points[rand() % points.size()];
    }

    vector<int> assignments(points.size(), -1);

    for (int iter = 0; iter < max_iterations; ++iter) {
        // Assign points to nearest centroid
        for (int i = 0; i < points.size(); ++i) {
            double min_dist = numeric_limits<double>::max();
            for (int j = 0; j < k; ++j) {
                double dist = distance(points[i], centroids[j]);
                if (dist < min_dist) {
                    min_dist = dist;
                    assignments[i] = j;
                }
            }
        }

        // Update centroids
        vector<int> counts(k, 0);
        vector<Point> new_centroids(k, {0.0, 0.0});
        for (int i = 0; i < points.size(); ++i) {
            const auto& p = points[i];
            int cluster_id = assignments[i];
            new_centroids[cluster_id].x += p.x;
            new_centroids[cluster_id].y += p.y;
            counts[cluster_id]++;
        }
        for (int j = 0; j < k; ++j) {
            new_centroids[j].x /= counts[j];
            new_centroids[j].y /= counts[j];
        }
        centroids = new_centroids;
    }

    // Output results
    for (int i = 0; i < points.size(); ++i) {
        cout << "Point (" << points[i].x << ", " << points[i].y << ") assigned to cluster " << assignments[i] << "\n";
    }
}

// Example usage
int main() {
    vector<Point> points = {
        {1.0, 2.0}, {1.5, 1.8}, {5.0, 8.0},
        {8.0, 8.0}, {1.0, 0.6}, {9.0, 11.0},
        {8.0, 2.0}, {10.0, 2.0}, {9.0, 3.0}
    };
    int k = 3;
    int max_iterations = 100;
    kmeans(points, k, max_iterations);

    return 0;
}
```

#### Hierarchikus klaszterezés

A hierarchikus klaszterezési algoritmus két típusban létezik: agglomeratív és divizív. Az agglomeratív megközelítés az adatpontok egyesítésével indul, míg a divizív megközelítés nagy klaszterek felosztásával kezdődik.

##### Agglomeratív Megközelítés

1. **Inicializáció:** Minden adatpontot külön klaszterként kezelünk.
2. **Párok egyesítése:** A két legközelebbi klasztert egyesítjük.
3. **Távolság frissítése:** Frissítjük a klaszterek közötti távolságokat.
4. **Ismétlés:** Ismételjük a párok egyesítését, amíg az összes adatpont egy klaszterré nem válik.

##### Divizív Megközelítés

1. **Inicializáció:** Az összes adatpont egy klaszterként indul.
2. **Felosztás:** Megkeressük a klasztert, amelyet a legjobban szét lehet választani két kisebb klaszterre.
3. **Ismétlés:** Ismételjük a felosztási folyamatot, amíg az összes klaszter a kívánt számúra nem csökken.

#### Példakód (Általánosítsa az agglomeratív klaszterezést)

```cpp
#include <iostream>

#include <vector>
#include <cmath>

#include <limits>
#include <algorithm>

using namespace std;

struct Point {
    double x, y;
};

double distance(const Point& p1, const Point& p2) {
    return sqrt((p1.x - p2.x) * (p1.x - p2.x) + (p1.y - p2.y) * (p1.y - p2.y));
}

void agglomerative_clustering(vector<Point>& points, int num_clusters) {
    int n = points.size();
    vector<vector<double>> distances(n, vector<double>(n));
    vector<int> cluster(n);
    
    // Initialize distances and cluster indices
    for (int i = 0; i < n; ++i) {
        cluster[i] = i;
        for (int j = 0; j < n; ++j) {
            distances[i][j] = distance(points[i], points[j]);
        }
    }
    
    // Perform clustering
    int current_clusters = n;
    while (current_clusters > num_clusters) {
        double min_distance = numeric_limits<double>::max();
        pair<int, int> merge_indices;
        
        // Find the closest pair of clusters
        for (int i = 0; i < n-1; ++i) {
            for (int j = i+1; j < n; ++j) {
                if (cluster[i] != cluster[j] && distances[i][j] < min_distance) {
                    min_distance = distances[i][j];
                    merge_indices = {i, j};
                }
            }
        }
        
        // Merge clusters
        int merge_from = merge_indices.second;
        int merge_to = cluster[merge_indices.first];
        for (int i = 0; i < n; ++i) {
            if (cluster[i] == merge_from) {
                cluster[i] = merge_to;
            }
        }
        
        // Update distances
        for (int i = 0; i < n; ++i) {
            if (cluster[i] == merge_to) {
                for (int j = 0; j < n; ++j) {
                    if (cluster[j] != merge_to) {
                        distances[i][j] = distances[j][i] = min(distances[i][merge_indices.first], distances[merge_indices.second][j]);
                    }
                }
            }
        }
        
        --current_clusters;
    }
    
    // Output resulting clusters
    for (int i = 0; i < points.size(); ++i) {
        cout << "Point (" << points[i].x << ", " << points[i].y << ") is in cluster " << cluster[i] << "\n";
    }
}

// Example usage
int main() {
    vector<Point> points = {
        {1.0, 2.0}, {1.5, 1.8}, {5.0, 8.0},
        {8.0, 8.0}, {1.0, 0.6}, {9.0, 11.0},
        {8.0, 2.0}, {10.0, 2.0}, {9.0, 3.0}
    };
    int num_clusters = 3;
    agglomerative_clustering(points, num_clusters);

    return 0;
}
```

#### Elemzések

A különböző algoritmusok különböző jellegű adatokat igényelnek és más jellegű eredményeket adnak. Az asszociációs algoritmusok, mint az Apriori és az Eclat, általában diszkrét tranzakciós adatokat használnak, ahol a halmazok feltárása a cél. A klaszterezési algoritmusok, mint a K-means és a hierarchikus klaszterezés, főként numerikus adatokkal dolgoznak, és az adatpontok csoportosítását célozzák.

Az asszociációs algoritmusok eredményei gyakorisági minták, amelyek hasznosak lehetnek különböző alkalmazási területeken, például kiskereskedelmi elemzésekben vagy hibaelhárításban. A klaszterezési algoritmusok eredményei adatcsoportok formájában jelennek meg, amelyek segíthetnek az adatstruktúrák megértésében, anomáliák azonosításában vagy az adatok tömörítésében.

Röviden, az adatbányászati algoritmusok központi szerepet játszanak az adatok elemzésében és feldolgozásában, és azon belül is az adott algoritmus kiválasztása és alkalmazása kritikus a kívánt eredmények elérése érdekében.


