### 5. Dimenziócsökkentés

Az adatalapú tudományokban és gépi tanulásban egyre gyakoribb probléma a magas dimenziójú adathalmazok kezelése. Az ilyen adathalmazok, amelyek sok változót tartalmaznak, nehezen értelmezhetők és feldolgozhatók, ráadásul gyakran felesleges vagy redundáns információkat is tartalmaznak. A dimenziócsökkentés technikái olyan eszközöket kínálnak, amelyek segítségével a magas dimenziójú adatok kezelhetőbb, alacsonyabb dimenziójú formába hozhatók, miközben minimalizáljuk az információveszteséget. Ebben a fejezetben két kulcsfontosságú dimenziócsökkentő módszert vizsgálunk meg: a Principal Component Analysis (PCA) és a t-distributed Stochastic Neighbor Embedding (t-SNE) algoritmusokat. Megismerjük ezek működési alapjait, algoritmusait és gyakorlati alkalmazásait, amelyek révén ezen módszerek hatékonyan segíthetnek az adatelemzési és gépi tanulási feladatokban. Ezen technikák segíthetnek az adatok vizualizálásában, a zaj csökkentésében, illetve a modellek teljesítményének javításában, különösen akkor, amikor több száz, vagy akár több ezer változót kell figyelembe venni.

### 5. Dimenziócsökkentés

Dimenziócsökkentés alapvető fontosságú technika az adatelemzés és gépi tanulás területén. Fő célja egy nagy dimenziójú adatállomány redukálása oly módon, hogy az eredeti adatok lényegi információja megmaradjon. Ez nemcsak az adatelemzés és vizualizáció szempontjából előnyös, hanem jelentős mértékben javíthatja a gépi tanulási algoritmusok teljesítményét is. Az e fejezetben bemutatott két alapvető módszer a Principal Component Analysis (PCA) és a t-distributed Stochastic Neighbor Embedding (t-SNE).

#### PCA (Principal Component Analysis)

A Principal Component Analysis (Főkomponens-elemzés) az egyik legszélesebb körben használt dimenziócsökkentő módszer. Matematikailag robusztus és sokféle alkalmazásban bizonyított. Lényege, hogy egy ortogonális transzformáció segítségével ábrázolja az adatokat egy új koordináta-rendszerben, amely mentén a variancia maximalizált.

**Algoritmus és gondolatmenet:**

1. **Adatok normalizálása**: Minden változó átmérőjét hasonló mérlegre hozzuk, általában az adatokat az átlagából levonjuk és az eredményt az adatok szórásával osztjuk el. Ezzel biztosítjuk, hogy az összes változó azonos súllyal vegyen részt a következő lépésekben.

2. **Kovariancia mátrix kiszámítása**: A következő lépés a normalizált adatok kovarianciamátrixának kiszámítása. Ez a mátrix megmutatja, hogy hogyan korrelálnak a különböző változók egymással. Matematikailag ezt egy *C* kovariancia mátrix formájában ábrázoljuk:
   $$
   C = \frac{1}{n-1} X^T X
   $$
   ahol $X$ a normalizált adatmátrix, és $n$ az adatpontok száma.

3. **Eigenértékek és eigenvektorok kiszámítása**: A kovarianciamátrixból eigenértékeket és eigenvektorokat számítunk. Az eigenvektorok megadják az új tengelyek irányát, a hozzájuk tartozó eigenértékek pedig megadják, hogy mennyi varianciát magyaráz az adott tengely.

4. **Főkomponensek kiválasztása**: Kiválasztjuk azokat a főkomponenseket (eigenvektorokat), amelyekhez a legnagyobb eigenértékek tartoznak. Az eigenértékek csökkenő sorrendbe rendezésével választhatjuk ki azokat az első néhány komponenst, amelyekkel a variancia legnagyobb részét megmagyarázhatjuk.

5. **Adatprojekció**: Végül az adatok projektálása a kiválasztott főkomponensekre történik. Ennek eredményeként az eredeti n-dimenziós adatállományt csökkentett k-dimenziós térben ábrázoljuk.

**Formális matematikai leírás:**

Az eredeti adatsor $X$ a következőképpen projektálódik az új dimensiók $k$ számú főkomponsensére:
$$
Z = XW
$$
ahol $W$ az a mátrix, amelynek oszlopai a kiválasztott eigenvektorok.

Példa C++ kóddal:

```cpp
#include <iostream>
#include <vector>
#include <Eigen/Dense>
using namespace Eigen;
using namespace std;

// Function to normalize data
MatrixXd normalize(MatrixXd X) {
    RowVectorXd mean = X.colwise().mean();
    MatrixXd centered = X.rowwise() - mean;
    RowVectorXd stdev = centered.array().square().colwise().sum().sqrt() / (X.rows() - 1);
    centered = centered.array().rowwise() / stdev.array();
    return centered;
}

void PCA(MatrixXd X, int k) {
    // Step 1: Normalize the data
    MatrixXd normalizedX = normalize(X);

    // Step 2: Calculate the covariance matrix
    MatrixXd cov = (normalizedX.transpose() * normalizedX) / double(X.rows() - 1);
    SelfAdjointEigenSolver<MatrixXd> eigensolver(cov);

    // Step 3: Extract eigenvalues and eigenvectors
    MatrixXd eigenvectors = eigensolver.eigenvectors();
    VectorXd eigenvalues = eigensolver.eigenvalues();

    // Step 4: Select the top k eigenvectors
    MatrixXd W = eigenvectors.rightCols(k);

    // Step 5: Project data into new space
    MatrixXd Z = normalizedX * W;

    cout << "Projected Data: \n" << Z << endl;
}

int main() {
    MatrixXd X(5, 3);
    X << 2.5, 2.4, 2.8,
         0.5, 0.7, 1.0,
         2.2, 2.9, 3.1,
         1.9, 2.2, 2.6,
         3.1, 3.0, 3.5;

    int k = 2; // Number of principal components
    PCA(X, k);
    return 0;
}
```

**Alkalmazások:**

1. **Adatvizualizáció**: A 2D vagy 3D térbe való projektálás az adatok egyszerűbb vizualizációját teszi lehetővé, különösen nagydimenziós adathalmazok esetén.

2. **Zajcsökkentés**: A PCA segítségével az adatok vetületeinek zaját csökkenthetjük, miközben a lényeges információkat megőrizzük.

3. **Adat előfeldolgozás**: Gépi tanulási modellek, mint például a klasszifikáció vagy regresszió előtt az adatdimenziók csökkenthetők a feldolgozási idő csökkentése és a modellek teljesítményének javítása érdekében.

4. **Adatkompresszió**: A PCA alkalmazható adatkompresszióra, mivel a csökkentett dimenziójú reprezentáció kevesebb helyet igényel, és a rekonstrukciós hiba minimalizálható.

A PCA gyakorlati alkalmazása széleskörű, és számos adattípuson bizonyítottan hatékony. Habár egyszerű és jól érthető, fontos megjegyezni, hogy a módszer lineáris összefüggést feltételez az adatok között, és nem mindig érvényesül jól nemlineáris struktúrák esetén. Ebben az esetben más dimenziócsökkentő technikák, mint például a t-SNE, jobb eredményt hozhatnak.

#### Összefoglalás

A PCA egy széleskörűen alkalmazott módszer a dimenziócsökkentésre, amely számos előnnyel rendelkezik az adatelemzés és a gépi tanulás területén. Azonban fontos szem előtt tartani a korlátait, különösen akkor, amikor az adatok belső szerkezete nemlineáris. A PCA matematikailag jól megalapozott, és hasznos eszközként szolgálhat a nagy dimenziójú adathalmazok kezelésében, árnyaltabb megértést adva az adatok mögött húzódó főbb mintázatokról és összefüggésekről.

### 5. Dimenziócsökkentés

A dimenziócsökkentés olyan technikák halmaza, amelyek célja a nagy dimenziós adatok kisebb dimenziós ábrázolása, miközben megtartják az adat lényeges tulajdonságait. Ez kulcsfontosságú szerepet játszik az adatbányászatban és gépi tanulásban, mivel csökkenti az adatok mennyiségét, ezzel javítva a számítási hatékonyságot és gyakran a modellek teljesítményét is. Ebben a fejezetben két széles körben használt dimenziócsökkentési technikát tárgyalunk részletesen: a főkomponens-elemzést (Principal Component Analysis, PCA) és a t-SNE (t-Distributed Stochastic Neighbor Embedding) algoritmust.

#### 5.1. PCA (Principal Component Analysis)

**Algoritmus és alkalmazások**

A Principal Component Analysis egy lineáris dimenziócsökkentési módszer, amelyet széles körben alkalmaznak az adatelemzésben és gépi tanulásban. A PCA célja, hogy az adat varianciájának maximális megőrzésével az adatot kevesebb dimenzióra transzformálja.

##### Alapötlet
A PCA lényege, hogy az eredeti adathalmazt egy új bázisra vetíti, ahol a bázisvektorok (főkomponensek) egymásra merőlegesek (ortogonálisak), és az első néhány főkomponens megőrzi az adat legnagyobb varianciáját.

##### Algoritmus lépései:
1. **Adat normálása:** Középérték kivonása az adatokból (mean centering).
2. **Kovariancia mátrix számítása:** Az adat kovariancia mátrixának létrehozása.
3. **Eigenvektorok és Eigenértek számítása:** A kovariancia mátrix eigenvektorainak és eigenértékeinek kiszámítása.
4. **Adat transzformáció:** Az eredeti adatok transzformálása az eigenvektorok által képviselt új tengelyek mentén.

**Matematikai Formulák:**

- Középérték kivonása:
  $X_{\text{norm}} = X - \mu$

- Kovariancia mátrix:
  $C = \frac{1}{n-1} X_{\text{norm}}^T X_{\text{norm}}$

- Eigenvektorok és Eigenértek:
  $C v_i = \lambda_i v_i$

##### PCA C++ Kód Példa (Eigen könyvtár segítségével)
```cpp
#include <Eigen/Dense>
#include <iostream>

using namespace Eigen;
using namespace std;

MatrixXd PCA(const MatrixXd& data, int components) {
    // Mean centering
    MatrixXd centered = data.rowwise() - data.colwise().mean();
    
    // Covariance matrix
    MatrixXd cov = (centered.adjoint() * centered) / double(data.rows() - 1);
    
    // Eigen decomposition
    SelfAdjointEigenSolver<MatrixXd> eig(cov);
    
    // Principal Components
    MatrixXd eigenvectors = eig.eigenvectors().rightCols(components);
    
    // Transform data
    MatrixXd transformed = centered * eigenvectors;
    
    return transformed;
}

int main() {
    MatrixXd data(5, 3);
    data << 1, 2, 3,
            2, 3, 4,
            3, 4, 5,
            4, 5, 6,
            5, 6, 7;

    int components = 2;
    MatrixXd reducedData = PCA(data, components);
    
    cout << "Reduced Data: \n" << reducedData << endl;
    return 0;
}
```

##### Alkalmazások
1. **Képfeldolgozás:** A PCA-t gyakran használják képadatok dimenziócsökkentésére az arcfelismerésben.
2. **Genomika:** A PCA segít az egyes gének közötti kapcsolatok felfedésében nagy genomikai adatok elemzésekor.
3. **Adatvizualizáció:** A PCA lehetővé teszi a többdimenziós adatok két- vagy háromdimenziós ábrázolását, megkönnyítve az adatok vizualizációját és értelmezését.

#### 5.2. t-SNE (t-Distributed Stochastic Neighbor Embedding)

**Algoritmus és alkalmazások**

A t-SNE egy nemlineáris dimenziócsökkentési technika, amely különösen hatékony az adatok vizualizációjára. A t-SNE célja, hogy az adatok magas dimenziós reprezentációjából megtartsa a lokális szomszédságokat egy alacsonyabb dimenziós térben.

##### Alapötlet
A t-SNE a magas dimenziós térben lévő pontok közötti valószínűségi eloszlásokat modellezi, majd próbálja ezeket az eloszlásokat megőrizni egy alacsonyabb dimenziós térben.

##### Algoritmus lépései:
1. **Párközi valószínűségek számítása a magas dimenziós térben:** Az $x_i$ és $x_j$ pontpár közötti valószínűség:
   $P_{j|i} = \frac{\exp(- \| x_i - x_j \|^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(- \| x_i - x_k \|^2 / 2\sigma_i^2)}$
   és
   $P_{ij} = \frac{P_{j|i} + P_{i|j}}{2N}$

2. **Párközi valószínűségek számítása az alacsony dimenziós térben:** Az $y_i$ és $y_j$ pontpár közötti valószínűség:
   $Q_{ij} = \frac{(1 + \| y_i - y_j \|^2)^{-1}}{\sum_{k \neq l} (1 + \| y_k - y_l \|^2)^{-1}}$

3. **Kullback-Leibler divergencia minimalizálása:** A t-SNE a Kullback-Leibler divergencia (KL-divergencia) mérőszámot használja az eloszlások közötti különbség minimalizálására:
   $C = \sum_i \sum_j P_{ij} \log \frac{P_{ij}}{Q_{ij}}$

##### t-SNE C++ Kód Példa (harmadik fél könyvtár, például BH-t-SNE használatával)
A t-SNE általában bonyolultabb és több lépésből álló folyamat, ezért ajánlott a harmadik fél könyvtárak használata, mint például a BH-t-SNE, amelynek C++ implementációja megtalálható [itt](https://github.com/lvdmaaten/bhtsne).

##### Alkalmazások
1. **Adatvizualizáció:** A t-SNE gyakran kerül alkalmazásra magas dimenziós adatok két- vagy háromdimenziós vizualizációjához, például szövegtáblák, képek, vagy genomikai adatok elemzésekor.
2. **Klaszterezés:** A t-SNE elősegíti a klaszterek felismerését és azonosítását azáltal, hogy az adatokat egy vizuálisan érthetőbb formába transzformálja.
3. **Anomália detektálás:** Az adatok vizuális ábrázolása segíthet az anomáliák azonosításában, mivel azok gyakran elkülönülnek a fő adathalmaztól.

Mind a PCA, mind a t-SNE erőteljes eszköz a dimenziócsökkentés területén, lehetővé téve a nagy dimenziós adatok kezelhetőségét és értelmezhetőségét. Míg a PCA lineáris és gyors, a t-SNE nemlineáris módszerként az adatok lokális szomszédságait hűen megőrizve komplexebb mintázatokat képes felfedni. A megfelelő módszer kiválasztása mindig az adott feladattól és adatszerkezettől függ.

### 5. Dimenziócsökkentés 

#### t-SNE

t-SNE (t-distributed Stochastic Neighbor Embedding) egy nemlineáris dimenziócsökkentős technika, amelyet kifejezetten a nagy dimenzionalitású adatok vizualizációjára fejlesztettek ki. Ez a módszer különösen hasznos, ha egy magas dimenziójú térben lévő adatpontok közötti kapcsolatokat két- vagy háromdimenziós térben szeretnénk megjeleníteni, hogy meglássuk az adat szerkezetét vagy klasztereit.

##### Algoritmus

A t-SNE algoritmus lényege, hogy a magas dimenziójú térben lévő adatpontok közötti távolságokat és kapcsolódásokat kis dimenziójú térben (általában két vagy három dimenzióban) rekonstruálja úgy, hogy az eredeti struktúra megőrződjön. A t-SNE két fő fázisban működik:

1. **Probabilitás eloszlás felépítése a magas dimenziójú térben:**

   Az első lépésben minden adatponthoz egy Gaussián környezeti eloszlást rendelünk. Az i-edik adatpontjának $x_i$ körüli **conditional probability distribution**-ját úgy határozzuk meg a j-edik adatpontra $x_j$ vonatkoztatva, hogy az tükrözze, mennyire jelentős a j-edik pont a i-edik pont számára. Ez az eloszlás a következő formulával írható le:

   $$
   p_{j|i} = \frac{\exp(-\|x_i - x_j\|^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-\|x_i - x_k\|^2 / 2\sigma_i^2)}
   $$

   ahol $\sigma_i$ az i-edik pont Perplexity alapján választott paramétere. A Perplexity egy olyan parameter, amely a Gaussián eloszlás szélességét szabályozza és alapvetően a neighbors kiválasztási határát definiálja.

2. **Probabilitás eloszlás felépítése a kicsinyített dimenziójú térben:**

   A kis dimenziójú térben (jelöljük az adatpontokat $y_i$-vel) Lehetővé tesszi annak valószínűséggel $q_{j|i}$:

   $$
   q_{ij} = \frac{(1 + \|y_i - y_j\|^2)^{-1}}{\sum_{k \neq l} (1 + \|y_k - y_l\|^2)^{-1}}
   $$

   itt a t-SNE t-eloszlást használ, hogy elősegítse a nagy távolságok kisszámú adatpontjainak befolyását.

3. **A Kullback-Leibler Divergencia minimalizálása:**

   A két eloszlás közötti különbség mérésére a Kullback-Leibler divergencia szolgál:
   
   $$
   KL(P\|Q) = \sum_{i \neq j} p_{ij} \log \frac{p_{ij}}{q_{ij}}
   $$
   
   Az algoritmus célja ennek a divergenciának a minimalizálása. Ezt gradient descent vagy más optimalizációs módszerekkel érjük el.

##### Alkalmazások

A t-SNE széles körben alkalmazható különböző területeken, ahol nagy dimenzionalitású adatok vizualizálására van szükség. Néhány kiemelkedő alkalmazási terület:

1. **Biológiai adatok:**
   - **Genomics:** Génexpressziós adatok (RNA-seq) vizualizálására, genetikai klaszterek és populációs szerkezeti elemzésére.
   - **Proteomics:** Fehérjeminták klaszterizálása és vizualizálása.

2. **Képfeldolgozás:**
   - **Kézi írásos számjegyek azonosítása:** Mint például az MNIST adatbázis klasztervizualizációja.
   - **Arcképek és vonásokat tartalmas képek vizualizálása:** Arcfélrekonstruálás és arcképadatbázisok feldolgozása.

3. **Szövegbányászat és NLP (Natural Language Processing):**
   - **Szavak embedding megjelenítése:** word2vec vagy más embedding metódusokkal készített reprezentációk vizualizálása.
   - **Dokumentum klaszterizáció:** Tematikus klaszterek azonosítása dokumentumgyűjteményekben.

4. **Internethasználati minták:**
   - Felhasználói viselkedési mintázatok elemzése weboldalak, mobilalkalmazások vagy más digitális platformok látogatása alapján.

##### Példakód (C++)

Az alábbiakban bemutatok egy egyszerű példakódot C++ nyelven a t-SNE implementációjára, ami egy adatállomány DIM csökkentésére alkalmazza az algoritmust. A kód tiszta példát nyújt, ami illusztrációként szolgálhat a t-SNE működésének megértéséhez.

```cpp
#include <iostream>
#include <vector>
#include <cmath>
#include <random>
#include <algorithm>

// Function to compute pairwise Euclidean distances
std::vector<std::vector<double>> computePairwiseDistances(const std::vector<std::vector<double>>& data) {
    size_t n = data.size();
    std::vector<std::vector<double>> distances(n, std::vector<double>(n, 0));
    for (size_t i = 0; i < n; ++i) {
        for (size_t j = i + 1; j < n; ++j) {
            double sum = 0;
            for (size_t d = 0; d < data[i].size(); ++d) {
                double diff = data[i][d] - data[j][d];
                sum += diff * diff;
            }
            distances[i][j] = sqrt(sum);
            distances[j][i] = distances[i][j];
        }
    }
    return distances;
}

// Gaussian Kernel Function
std::vector<double> gaussianKernel(const std::vector<double>& distances, double sigma) {
    std::vector<double> kernel(distances.size());
    for (size_t i = 0; i < distances.size(); ++i) {
        kernel[i] = exp(-distances[i] * distances[i] / (2 * sigma * sigma));
    }
    return kernel;
}

void computeProbability(const std::vector<std::vector<double>>& distances, std::vector<std::vector<double>>& p_matrix, double sigma) {
    for (size_t i = 0; i < distances.size(); ++i) {
        std::vector<double> kernel = gaussianKernel(distances[i], sigma);
        double sum = std::accumulate(kernel.begin(), kernel.end(), 0.0);
        for (size_t j = 0; j < distances[i].size(); ++j) {
            p_matrix[i][j] = kernel[j] / sum;
        }
    }
}

int main() {
    // Prepare mock data
    std::vector<std::vector<double>> data = {{0.0, 0.0}, {1.0, 0.0}, {1.0, 1.0}, {0.0, 1.0}};
    size_t n = data.size();
    
    // Compute pairwise distances in the high-dimensional space
    std::vector<std::vector<double>> distances = computePairwiseDistances(data);
    
    // Probability matrix
    std::vector<std::vector<double>> p_matrix(n, std::vector<double>(n, 0.0));
    double sigma = 1.0; // Hypothetical sigma value for simplicity
    computeProbability(distances, p_matrix, sigma);
    
    // Print the probability matrix
    for (const auto& row: p_matrix) {
        for (double val : row) {
            std::cout << val << ' ';
        }
        std::cout << '\n';
    }
    
    return 0;
}
```

Ez a példa csak az egyik lépést (a probabilisztikus mátrix kiszámítását) fedi le, viszont a teljes t-SNE algoritmus teljes implementációja nagyobb terjedelmű. A **probability distribution** kiszámítása és az optimalizáció mindenképp nagyobb figyelmet igényel a teljes algoritmus működéséhez. További figyelmet kell szentelni a gradient descent optimalizálásra és annak végrehajtására.

#### Összegzés

Összefoglalva, a t-SNE egy rendkívül hasznos eszköz a dimenziócsökkentésben, amely lehetővé teszi a komplex, magas dimenziójú adatok vizualizációját. A probabilisztikus megközelítés és a nemlineáris dimenziócsökkentés kombinációja révén a t-SNE képes felfedni a rejtett struktúrákat és klasztereket az adatokban, aminek köszönhetően széles körben használt eszközzé vált az adatelemzés és gépi tanulás területén.

### 5. Dimenziócsökkentés

A dimenziócsökkentés egy alapvető adatbányászati technika, amely a nagy dimenziójú adathalmazok kezelésére és értelmezésére szolgál. Az adatbányászat és a gépi tanulás területén gyakran találkozunk olyan adatokkal, amelyek számos attribútumot vagy jellemzőt tartalmaznak. Ez a magas dimenziószám gyakran negatív hatással van a tanulási algoritmusok teljesítményére és az adatok általános kezelhetőségére is. A dimenziócsökkentés célja, hogy az eredeti, nagy dimenziószámú adathalmazt egy kisebb, de lényeges információt megőrző alacsonyabb dimenziójú térbe vetítse.

Az alábbiakban részletesen tárgyaljuk a dimenziócsökkentés két népszerű technikáját: a Principal Component Analysis (PCA) és a t-distributed Stochastic Neighbor Embedding (t-SNE) módszereit.

#### 5.1. PCA (Principal Component Analysis)

##### Algoritmus és alkalmazások

A Principal Component Analysis (PCA) egy lineáris dimenziócsökkentési technika, amely a maximum variancia elvét használja az adatok releváns jellemzőinek kinyeréséhez. A PCA célja az, hogy az eredeti adathalmaz egy kisebb részhalmazát úgy válasszuk ki, hogy a legtöbb információ, azaz a variancia, megtartásra kerüljön.

##### Algoritmus

1. **Adatközpontozás:** Az adatokat átlagoljuk, így az adatok új középpontját az origóba helyezzük. Ez úgy történik, hogy az egyes adatokból kivonjuk a dátumok átlagát.
   
   $X_{centered} = X - \mu$

   ahol $\mu$ az adatok átlagos értéke minden dimenzióban.

2. **Kovariancia mátrix számítása:** Az adatpontok $d$ dimenziójában számítjuk a kovariancia mátrixot, ami a dimenziók közötti lineáris kapcsolatokat tükrözi.
   
   $\Sigma = \frac{1}{n-1} X_{centered}^T X_{centered}$

3. **Eigenelemek kiszámítása:** A kovariancia mátrix sajátértékeit és sajátvektorait határozzuk meg. A sajátértékek a dimenziók varianciáját mutatják meg, a sajátvektorok pedig az új tengelyeket képviselik.

4. **Dimenziócsökkentés:** A sajátértékek nagysága szerint rendezzük a sajátvektorokat, hogy a legnagyobb varianciával rendelkező komponensek (sajátvektorok) megmaradjanak. Ezek a sajátvektorok alkotják az ún. főkomponenseket. A kisebb jelentőségű (alacsonyabb sajátértékkel rendelkező) komponensek elhagyásával létrehozzuk az alacsonyabb dimenziójú adatteret.

5. **Adattranszformáció:** Az eredeti adatokat átvisszük az új, alacsonyabb dimenziójú térbe az első $k$ főkomponens segítségével:

   $X_{reduced} = X_{centered} W_k$

   ahol $W_k$ az első $k$ sajátvektor oszlopvektorainak mátrixa.

##### Alkalmazások

- **Adatvizualizáció:** A PCA gyakran használatos az adatok 2D vagy 3D térbe vetítéséhez, hogy azok könnyebben vizualizálhatók legyenek.
- **Adatelőkészítés:** A gépi tanulási modellek esetében a PCA elősegítheti az adatok redundanciájának csökkentését és a modellek hatékonyságának javítását.
- **Arcfelismerés:** Az arcfelismerő rendszerek gyakran használják a PCA-t a képek dimenziójának csökkentésére és a legfontosabb jellemzők kinyerésére.

### 5.2. t-SNE (T-distributed Stochastic Neighbor Embedding)

##### Algoritmus és alkalmazások

A t-distributed Stochastic Neighbor Embedding (t-SNE) egy nemlineáris dimenziócsökkentési technika, amely különösen hatékony az adatpontok közötti szerkezeti kapcsolatok megőrzésében az alacsonyabb dimenziójú térben. Elsősorban vizualizációs célokra használatos, mivel képes az adatok szerkezetét természetes módon megjeleníteni.

##### Algoritmus

1. **Páronkénti távolságok kiszámítása:** Az eredeti dimenzióban az adatpontok közötti páronkénti távolságokat számítjuk ki, majd ezek alapján feltételezzük, hogy az adatpontok egy $d$-dimenziós Gauss-eloszlást követnek. 

   $P_{j|i} = \frac{\exp(-||x_i - x_j||^2 / 2\sigma_i^2)}{\sum_{k \neq i} \exp(-||x_i - x_k||^2 / 2\sigma_i^2)}$
   $P_{i|i} = 0$
   ahol $P_{j|i}$ annak a valószínűsége, hogy az $x_i$ pont szomszédos az $x_j$ ponttal.

2. **Szimmetrizálás:** Az $P_{ij}$ szimmetrikus valószínűségi mátrix számítása.

   $P_{ij} = \frac{P_{j|i} + P_{i|j}}{2n}$

3. **Alacsony dimenziós eloszlás létrehozása:** Az adatokat alacsonyabb dimenzióban $q_{ij}$ eloszlás szerint térképezzük fel, amely egy Student-t-eloszlást követ.

   $Q_{ij} = \frac{(1 + ||y_i - y_j||^2)^{-1}}{\sum_{k \neq l} (1 + ||y_k - y_l||^2)^{-1}}$

4. **Kullback-Leibler divergencia minimalizálása:** Az eredeti és a redukált eloszlások közötti különbséget a Kullback-Leibler divergencia minimalizálásával érjük el.

   $\text{KL}(P||Q) = \sum_{i \ne j} P_{ij} \log \frac{P_{ij}}{Q_{ij}}$

   Ezt a célfüggvényt gradiens módszerekkel optimalizáljuk, hogy a lehető legjobban illeszkedjen az alacsony dimenziójú tér az eredeti adatokhoz.

##### Alkalmazások

- **Klaszterelemzés:** A t-SNE kiválóan alkalmazható az adatok klasztereinek vizualizálására és a rejtett szerkezetek feltárására.
- **Adatfelfedezés:** Az adatelemzők gyakran használják a t-SNE-t az adatok gyors megértéséhez és a mintázatok felismeréséhez szűz adatok esetében.
- **Genomikai adatok analizálása:** A t-SNE segít az összetett genetikai adatok vizualizálásában és klasztereinek azonosításában.

### Összegzés

A dimenziócsökkentési technikák, mint a PCA és a t-SNE kulcsfontosságú eszközök az adatbányászat és a gépi tanulás területén, különböző alkalmazások és adathalmazok számára. A PCA lineáris módszerként hatékonyan csökkenti az adatok dimenzióját varianciaveszteséggel járó egyszerűsítési célra, míg a t-SNE nemlineáris módszerként erőteljes eszköz a komplex adatok szerkezeti megjelenítésére és vizualizációjára. Mindkét módszer lehetővé teszi az adatelemzők és adatkutatók számára, hogy mélyebb betekintést nyerjenek az összetett adathalmazokba, és elősegítik a fontos mintázatok és klaszterek felismerését.

