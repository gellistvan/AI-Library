\newpage

# 6. Kódolási algoritmusok

## 6.1. Adatkompresszió alapelvei

Az adatkompresszió olyan technológiák és módszerek összessége, amelyek célja az adatok méretének csökkentése, hogy azok hatékonyabban legyenek tárolhatók és továbbíthatók. Az információs társadalomban, ahol hatalmas mennyiségű adatot generálunk és fogyasztunk, a kompressziós technikák elengedhetetlenek a sávszélesség optimalizálásához és a tárolási kapacitások gazdaságos kihasználásához. Ebben a fejezetben két alapvető kompressziós típusra összpontosítunk: a veszteségmentes és veszteséges kompresszióra. Bemutatjuk azok előnyeit, alkalmazási területeit, valamint alapvető különbségeit. Továbbá megismerkedünk az entropia és információelmélet alapjaival, amelyek nélkülözhetetlen alapot szolgáltatnak a hatékony adatkompressziós algoritmusok megértéséhez és kifejlesztéséhez. Az elméleti alapok megismerése után pedig betekintést nyújtunk különböző kódolási algoritmusok működésébe és alkalmazási lehetőségeibe.

### Veszteségmentes és veszteséges kompresszió

A digitális adatkompresszió két alapvető csoportját különböztetjük meg: a veszteségmentes és a veszteséges kompressziót. Mindkét típusnak sajátos előnyei és hátrányai vannak, valamint különböző alkalmazási területeken használatosak. Ebben a fejezetben részletesen megvizsgáljuk mindkét kompressziós forma működési elvét, gyakorlati alkalmazásait és példákon keresztül bemutatjuk használatukat.

#### Veszteségmentes kompresszió

A veszteségmentes kompresszió célja az adatok úgynevezett redundanciájának kihasználása az adatok tömörítése érdekében, miközben minden eredeti információ megmarad. Ezen technikák alkalmazása után az adatokat veszteségmentesen, azaz pontosan az eredeti formájukban lehet visszaállítani. 

##### Módszerek és algoritmusok

1. **Huffman kódolás**: 
    A Huffman kódolás egy eloszlásfüggő, bináris fák alapú kódoló eljárás, amely a szimbólumok előfordulási gyakoriságát figyelembe véve rendkívül hatékony tömörítést biztosít. Az algoritmus lépései közé tartozik a szimbólumok gyakorisági táblázatának készítése, egy Huffman-fa kialakítása, majd a szimbólumok kódolása a fa leveleiből.

2. **Run-Length Encoding (RLE)**: 
    Az RLE egyszerű, de hatékony algoritmus, amely különösen jól működik olyan adatsorok esetében, ahol azonos értékek ismétlődnek. Az algoritmus összevonja az ismétlődő elemeket, és azok számával együtt tárolja őket.

3. **Lempel-Ziv-Welch (LZW)**: 
    Az LZW egy szekvenciális, szótáralapú tömörítési módszer, amely az adatfolyamot elemek sorozataként ábrázolja és megpróbálja megtalálni a leghosszabb, már létező szekvenciákat, így csökkentve az adatok mennyiségét.

##### Példa C++ kóddal

Az alábbi példa egy egyszerű Huffman kódolás implementációját mutatja be C++ nyelven:

```cpp
#include <iostream>
#include <vector>
#include <queue>
#include <unordered_map>

using namespace std;

// Node for the Huffman tree
struct HuffmanNode {
    char data;
    int freq;
    HuffmanNode* left;
    HuffmanNode* right;
    HuffmanNode(char data, int freq) : data(data), freq(freq), left(NULL), right(NULL) {}
};

// Comparison object to be used in priority queue
struct compare {
    bool operator()(HuffmanNode* l, HuffmanNode* r) {
        return (l->freq > r->freq);
    }
};

// Traverse the Huffman Tree and store Huffman Codes in a map.
void storeCodes(struct HuffmanNode* root, string str, unordered_map<char, string> &huffmanCode) {
    if (!root) return;
    if (root->data != '$') huffmanCode[root->data] = str;
    storeCodes(root->left, str + "0", huffmanCode);
    storeCodes(root->right, str + "1", huffmanCode);
}

// Function to create the Huffman Tree and generate codes
void HuffmanCodes(vector<char>& data, vector<int>& freq, int size) {
    struct HuffmanNode *left, *right, *top;
    priority_queue<HuffmanNode*, vector<HuffmanNode*>, compare> minHeap;
    for (int i = 0; i < size; ++i) {
        minHeap.push(new HuffmanNode(data[i], freq[i]));
    }
    while (minHeap.size() != 1) {
        left = minHeap.top();       
        minHeap.pop();
        right = minHeap.top(); 
        minHeap.pop();
        top = new HuffmanNode('$', left->freq + right->freq);
        top->left = left;
        top->right = right;
        minHeap.push(top);
    }
    unordered_map<char, string> huffmanCode;
    storeCodes(minHeap.top(), "", huffmanCode);
    for (auto pair : huffmanCode) {
        cout << pair.first << " " << pair.second << '\n';
    }
}

int main() {
    vector<char> data = {'a', 'b', 'c', 'd', 'e', 'f'};
    vector<int> freq = {5, 9, 12, 13, 16, 45};
    HuffmanCodes(data, freq, data.size());
    return 0;
}
```

#### Veszteséges kompresszió

A veszteséges kompresszió célja az adatok méretének radikálisan csökkentése, az eredeti információ egy részének feláldozásával. Ez a technika különösen hasznos olyan alkalmazásokban, ahol a tömörített adatok tökéletessége nem kritikus, mint például kép- és audiófeldolgozás. Az ilyen típusú kompresszió gyakran kihasználja az emberi érzékelés korlátaiból adódó előnyöket.

##### Módszerek és algoritmusok

1. **JPEG kompresszió**: 
    A JPEG (Joint Photographic Experts Group) szabvány az egyik legismertebb veszteséges képkompressziós algoritmus. Lényege, hogy a képeket színtereinek külön kezelése után diszkrét koszinusz transzformációval (DCT) dolgozza fel, ahol bizonyos frekvenciakomponenseket eltávolít vagy csökkent, amelyek kevésbé érzékelhetőek az emberi szem számára.

2. **MP3 kompresszió**: 
    Az MP3 (MPEG-1 Audio Layer III) az egyik leggyakrabban használt veszteséges audiókompressziós formátum, amely pszichoakusztikus modellek segítségével csökkenti az audióadat méretét, miközben megőrzi annak elfogadható minőségét.

3. **MPEG kompresszió**: 
    Az MPEG (Moving Picture Experts Group) videókompressziós szabvány egyaránt alkalmaz veszteséges és veszteségmentes technikákat a videók tömörítése érdekében. Az MPEG-4 esetén például a veszteség jelentős része a mozgásvektoris alapú tömörítésből és a blokkok frekvenciaanalíziséből származik.

##### Példa magyarázata

Mivel a veszteséges kompresszió gyakran komplex matematikai transzformációkra épül, a részletes algoritmikus implementációk meghaladnák e fejezet kereteit. Példaként azonban érdemes megvizsgálni a DCT (Diszkrét Koszinusz Transzformáció) egy 1-dimenziós implementációját, amely a JPEG kompresszió egyik alapköve.

```cpp
#include <iostream>
#include <cmath>
#include <vector>

using namespace std;

// Function to calculate the DCT of a vector
void DCT(vector<double>& data) {
    int N = data.size();
    vector<double> result(N);
    for (int k = 0; k < N; ++k) {
        double sum = 0.0;
        for (int n = 0; n < N; ++n) {
            sum += data[n] * cos(M_PI * k * (2 * n + 1) / (2 * N));
        }
        result[k] = sum * sqrt(2.0 / N) * (k == 0 ? 1.0 / sqrt(2.0) : 1.0);
    }
    data = result;
}

int main() {
    vector<double> data = {255, 128, 36, 75, 150, 200, 35, 255};
    DCT(data);
    for (auto value : data) {
        cout << value << " ";
    }
    return 0;
}
```

#### Alkalmazási területek és előnyök

Mindkét kompressziós technika különböző alkalmazási területeken találja meg a helyét. A veszteségmentes kompresszió kiválóan használható olyan területeken, ahol az adat integritása kritikus, mint például adatbázisok, szövegek, programfájlok és biztonsági mentések. Ezzel szemben a veszteséges kompresszió nagy előnyökkel bír multimédiás alkalmazásokban, mint a képek, videók és audiófájlok tárolása és továbbítása, ahol a fájlok méretének jelentős csökkentése fontosabb, mint az adatok hibátlan megőrzése.

#### Következtetés

Összefoglalva, mind a veszteségmentes, mind a veszteséges kompressziós technológia fontos szerepet játszik a modern informatikában, lehetővé téve az adatok hatékonyabb tárolását és továbbítását. A megfelelő technológia kiválasztása függ az alkalmazási követelményektől és a prioritásként kezelt szempontoktól, mint az adat integritásának megőrzése vagy a tárolási költségek minimalizálása. Az algoritmusok és módszerek részletes megértése és alkalmazása kritikus fontosságú a hatékony adatkompressziós rendszerek tervezésében és fejlesztésében.

### Entropia és információelmélet alapjai

Az adatkompresszió mélyebb megértéséhez elengedhetetlen megismernünk az információelmélet és az entropia alapfogalmait. Az információelmélet egy olyan tudományág, amely a kommunikációs rendszerekben előforduló információ átvitelével, feldolgozásával és tárolásával foglalkozik. Claude Shannon, az információelmélet atyja, 1948-ban publikálta alapvető munkáját, amely forradalmasította a kommunikáció és adatfeldolgozás területét. A következőkben áttekintjük az entrópia és az információelmélet alapfogalmait és azok jelentőségét az adatkompressziós algoritmusok szempontjából.

#### Információ és bizonytalanság

Az információelmélet központi koncepciója az információ mértéke, amelyet a bizonytalanság csökkentésével lehet megfogalmazni. Ha egy esemény valószínűsége $p$, akkor az esemény bekövetkezésének információtartalma egyenesen arányos a valószínűségével. Az egyes események információtartalma így meghatározható a következő pontrendszerrel:

$$ I(x) = -\log_2{p(x)} $$

Ahol $I(x)$ az esemény információtartalma vagy meglepetés értéke, $p(x)$ az esemény bekövetkezési valószínűsége. A negatív logaritmus alkalmazása azért célszerű, mert szeretnénk, hogy a valószínűségi változók bármely értéke, amelyek nagyobb valószínűséggel rendelkeznek, kisebb információtartalmat képviseljenek.

#### Entropia

Az entrópia az egész rendszerről való információ mértéke, vagyis egy forrás átlagos információtartalma egy szimbólumra vonatkoztatva. Matematikailag az entrópia a következő módon definiálható:

$$ H(X) = -\sum_{i=1}^n p(x_i) \log_2{p(x_i)} $$

Itt az $X$ valószínűségi változó, amely $x_1, x_2, \ldots, x_n$ értékeket vehet fel, és $p(x_i)$ az adott érték valószínűsége. Az entrópia tehát az összes szimbólum valószínűség szerinti súlyozott logaritmikus információtartalma.

#### Az információelmélet alapvető tételei

1. **Forráskódolási tétel (Shannon-féle első tézis)**:  
   Azt állítja, hogy egy forrásból származó $X$ véletlen változó entrópiája megadja a forrás optimális bit/szimbólum arányát. Vagyis semmilyen veszteségmentes tömörítés nem lehet hatékonyabb, mint az entrópiával meghatározott érték:

   $$ R \geq H(X) $$

   Ez a tétel alapvető fontosságú a kompressziós algoritmusok tervezésekor, mert meghatározza a teoretikusan lehető legjobb tömörítési hatékonyságot.

2. **Csatornakódolási tétel (Shannon-féle második tézis)**:  
   Ez a tétel azt fekteti le, hogy minden kommunikációs csatornának van egy kapacitása (C), amely meghatározza a maximálisan átvitelre kerülhető információ mennyiségét egy adott zajszint mellett. Ha az átvitel sebessége kisebb vagy egyenlő mint $C$, akkor az átvitel hibamentes lehet.

   $$ \text{Ráta} \leq C $$

#### Gyakorlati következmények

Az entrópia és az információelmélet alapelvei közvetlen hatással vannak a mindennapi életben használt tömörítési algoritmusokra. A pld. Huffman-kódolás és az aritmetikai kódolás mind az entrópia fogalmára alapozva működnek.

Például, a Huffman-kód olcsón kódolja a gyakran használt karaktereket, kisebb redundanciát építve be a közlésekbe. Az aritmetikai kódolás, az entrópia kódolás egy másik formája, kódolja a teljes üzenetet egyetlen, változó hosszúságú kódban, amely hatékonyabb lehet rövid üzeneteknél.

#### Példa számítás C++ kóddal

Az alábbi példában egy egyszerű C++ program implementálja az entrópia kiszámítását egy bemeneti szimbólumforrás alapján:

```cpp
#include <iostream>
#include <unordered_map>
#include <vector>
#include <cmath>
#include <string>

// Function to calculate the entropy
double calculateEntropy(const std::unordered_map<char, int>& frequencies, int totalSymbols) {
    double entropy = 0.0;
    for (const auto& pair : frequencies) {
        if (pair.second > 0) {
            double probability = static_cast<double>(pair.second) / totalSymbols;
            entropy += probability * log2(probability);
        }
    }
    return -entropy;
}

int main() {
    std::string input = "example of entropy calculation";
    std::unordered_map<char, int> frequencies;

    // Calculate frequencies of each character
    for (char c : input) {
        if (c != ' ') {
            frequencies[c]++;
        }
    }

    int totalSymbols = input.length();

    // Calculate entropy
    double entropy = calculateEntropy(frequencies, totalSymbols);
    std::cout << "Entropy of the input string is: " << entropy << std::endl;

    return 0;
}
```

Az eredményül kapott entrópia érték képezi a bemeneti karaktersorozat információ elméleti alapját, megmutatva, hogy mennyi információt tartalmaz az adott bemenet. Ez a mérés fontos szerepet játszik az adatkompresszió hatékonyságának meghatározásában.

#### Következtetés

Összefoglalva, az entrópia és az információelmélet alapfogalmai létfontosságúak az adatkompresszió megértéséhez és alkalmazásához. Az entrópia pontos mérésével megérthetjük az adatforrás információtartalmát és a maximális tömörítési potenciált. Az információelmélet alapvető tézisei segítenek abban, hogy az adatátvitel és tömörítés hatékonyságát optimalizáljuk, maximalizálva a rendszerek teljesítményét a rendelkezésre álló erőforrások határain belül. Az elméleti alapelvek alkalmazásával létrehozhatjuk és fejleszthetjük a különféle tömörítési algoritmusokat, amelyek napjaink digitális világában elengedhetetlenek.

