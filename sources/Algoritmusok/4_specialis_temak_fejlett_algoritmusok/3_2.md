\newpage

## 3.2. Adatbányászati algoritmusok

A modern világban hatalmas mennyiségű adat keletkezik minden pillanatban, és ezek az adatok rengeteg értékes információt hordozhatnak magukban. Az adatbányászat azon technikák és módszerek gyűjteménye, amelyek segítségével a releváns információk kinyerhetők, és a rejtett minták feltárhatók az adatokból. Ebben a fejezetben az adatbányászat néhány kulcsfontosságú algoritmusát tekintjük át, melyek nélkülözhetetlenek a modern adatelemzés és gépi tanulás területén. Először az asszociációs szabályok felfedezésével foglalkozó algoritmusokat, az Apriori és Eclat módszereket mutatjuk be, amelyek segítségével gyakori elemek közötti összefüggéseket és asszociációkat tárhatunk fel nagy adathalmazokban. Ezt követően a klaszterezési algoritmusok, mint a K-means és a hierarchikus klaszterezés kerülnek terítékre, amelyek csoportok, vagy klaszterek kialakítását szolgálják az adatok között, lehetővé téve számunkra a hasonló tulajdonságokat mutató elemek azonosítását és szegmentálását. Ezek az adatbányászati algoritmusok alapvető szerepet játszanak abban, hogy az adatokból értékes és használható információkat nyerjünk ki, és megértsük azok mögöttes struktúráit.

### Asszociációs szabályok (Apriori, Eclat)

Az asszociációs szabályok nagy mennyiségű adatban rejtett kapcsolatokat és összefüggéseket keresnek. Ezen módszerek célja gyakori elemek azonosítása és az elemek közötti összefüggések feltárása. Az asszociációs szabályok felfedezése különösen népszerű az adatbányászatban, mivel gyakran alkalmazzák a vásárlói viselkedés elemzésére, ajánlórendszerek kialakítására és sok más területen. Az asszociációs szabályok két legelterjedtebb algoritmusa az Apriori és az Eclat, amelyeket részletesen bemutatunk ebben a fejezetben.

#### Az asszociációs szabályok alapjai

Az asszociációs szabályokat általában transzakciós adathalmazokon alkalmazzák, mint például a bolti vásárlási adatok. Például, egy szabály formájában megfogalmazható: "Ha valaki tejet vásárol, akkor gyakran kenyérrel együtt vásárolja azt." Ezt a szabályt a következő módon írhatjuk le:
$$ \{tej\} \rightarrow \{kenyér\} $$

Az ilyen szabályok három fontos mérőszáma:
- **Támogatás (support)**: Azon esetek aránya, ahol az elemek együttesen fordulnak elő a teljes adathalmazban.
$$ \text{Támogatás}(\{A \rightarrow B\}) = \frac{\text{Az \{A, B\} tranzakciók száma}}{\text{Összes tranzakció száma}} $$

- **Bizalom (confidence)**: Azon esetek aránya, ahol az A elemet tartalmazó tranzakciók egyúttal B-t is tartalmaznak.
$$ \text{Bizalom}(\{A \rightarrow B\}) = \frac{\text{Az \{A, B\} tranzakciók száma}}{\text{Az \{A\} tranzakciók száma}} $$

- **Emelés (lift)**: Az A és B együttes előfordulásának valószínűsége, megosztva az A és B független előfordulási valószínűségének szorzatával.
$$ \text{Emelés}(\{A \rightarrow B\}) = \frac{\text{Támogatás}(\{A \rightarrow B\})}{\text{Támogatás}(\{A\}) \times \text{Támogatás}(\{B\})} $$

#### Apriori algoritmus

Az Apriori algoritmus az egyik legismertebb és leggyakrabban használt módszer az asszociációs szabályok felfedezésére. Nevét az apriori (latin: előzetesen) elvből kapta, mivel az algoritmus kihasználja azt a tényt, hogy ha egy elemhalmaz gyakori, akkor annak minden részhalmaza is gyakori.

**Algoritmus lépései:**
1. **Gyűjtés:** Minden egyes elemet egyesével gyakoriság szerint összegyűjtjük.
2. **Kombinálás:** A gyakori elemeket kettesével kombináljuk, és megnézzük, hogy ezek kombinációi is gyakoriak-e.
3. **Iteráció:** Az előző lépést iteráljuk többször, egyre nagyobb elemhalmazokat hozva létre, mindaddig, amíg nem tudunk több gyakori elemhalmazt létrehozni.
4. **Szabály generálás:** A gyakori elemhalmazok alapján szabályokat generálunk, amelyek meghatározott támogatási és bizalmi szintet teljesítenek.

**Példakód C++ nyelven:**
```cpp
#include <iostream>
#include <vector>
#include <map>
#include <set>

using namespace std;

// Transaction structure
struct Transaction {
    set<string> items;
};

// Function to get frequent itemsets
vector<set<string>> getFrequentItemsets(const vector<Transaction>& transactions, int min_support) {
    map<set<string>, int> itemset_frequency;
    
    // Step 1: Count the frequency of individual items
    for (const auto& transaction : transactions) {
        for (const auto& item : transaction.items) {
            set<string> single_item_set = {item};
            itemset_frequency[single_item_set]++;
        }
    }
    
    vector<set<string>> frequent_itemsets;
    for (const auto& item : itemset_frequency) {
        if (item.second >= min_support) {
            frequent_itemsets.push_back(item.first);
        }
    }
    
    // Step 2 & 3: Join step and frequent itemset generation for larger itemsets
    while (!frequent_itemsets.empty()) {
        vector<set<string>> new_frequent_itemsets;
        for (size_t i = 0; i < frequent_itemsets.size(); ++i) {
            for (size_t j = i + 1; j < frequent_itemsets.size(); ++j) {
                set<string> candidate_itemset = frequent_itemsets[i];
                candidate_itemset.insert(frequent_itemsets[j].begin(), frequent_itemsets[j].end());
                
                if (candidate_itemset.size() == frequent_itemsets[i].size() + 1) {
                    int frequency = 0;
                    for (const auto& transaction : transactions) {
                        if (includes(transaction.items.begin(), transaction.items.end(),
                                     candidate_itemset.begin(), candidate_itemset.end())) {
                            frequency++;
                        }
                    }
                    if (frequency >= min_support) {
                        new_frequent_itemsets.push_back(candidate_itemset);
                        itemset_frequency[candidate_itemset] = frequency;
                    }
                }
            }
        }
        
        if (new_frequent_itemsets.empty()) break;
        frequent_itemsets = move(new_frequent_itemsets);
    }
    
    vector<set<string>> result;
    for (const auto& item : itemset_frequency) {
        if (item.second >= min_support) {
            result.push_back(item.first);
        }
    }
    return result;
}

int main() {
    vector<Transaction> transactions = {
        {{"bread", "milk"}},
        {{"bread", "diaper", "beer", "eggs"}},
        {{"milk", "diaper", "beer", "cola"}},
        {{"bread", "milk", "diaper", "beer"}},
        {{"bread", "milk", "diaper", "cola"}}
    };
    
    int min_support = 2;
    vector<set<string>> frequent_itemsets = getFrequentItemsets(transactions, min_support);
    
    for (const auto& itemset : frequent_itemsets) {
        for (const auto& item : itemset) {
            cout << item << " ";
        }
        cout << endl;
    }
    
    return 0;
}
```

#### Eclat algoritmus

Az Eclat (Equivalence Class Clustering and bottom-up Lattice Traversal) algoritmus szintén gyakori elemhalmazok keresésére szolgál, de az Apriori algoritmustól eltérően, mélyebben a keresés fáját használja. Az Eclat gyorsabb lehet az Apriorinál, különösen, ha a keresési fa mélyebb szintjeivel dolgozik.

**Algoritmus lépései:**
1. **Vertikális adatformátum:** Az adatokat egy vertikális adatformátumban tárolja, ahol minden elemhez tartozik egy lista, amely megadja, hogy az adott elem mely tranzakciókban fordul elő.
2. **Rekurzív bontás:** Az adatbázis elemeit gyakori elemhalmazokra bontja, rekurzívan kombinálja ezeket az elemeket, és kiszámolja az egyes újonnan létrehozott elemhalmazok gyakoriságát, amíg a további bontás nem eredményez új gyakori elemhalmazokat.

**Példakód C++ nyelven:**
```cpp
#include <iostream>
#include <vector>
#include <map>
#include <set>

using namespace std;

// Transaction structure
struct Transaction {
    vector<string> items;
};

// Helper function to compute intersection of transactions
set<int> intersection(const set<int>& s1, const set<int>& s2) {
    set<int> result;
    for (int i : s1) {
        if (s2.find(i) != s2.end()) {
            result.insert(i);
        }
    }
    return result;
}

void eclat(map<set<string>, set<int>>& itemsets, const set<string>& prefix, const set<string>& items, int min_support, vector<set<string>>& frequent_itemsets) {
    for (auto it = items.begin(); it != items.end(); ++it) {
        set<string> new_prefix = prefix;
        new_prefix.insert(*it);
        
        set<int> tids = intersection(itemsets[prefix], itemsets[ {*it} ]);
        
        if (tids.size() >= min_support) {
            frequent_itemsets.push_back(new_prefix);
            set<string> suffix;
            auto suffix_it = it;
            ++suffix_it;
            for (; suffix_it != items.end(); ++suffix_it) {
                suffix.insert(*suffix_it);
            }
            eclat(itemsets, new_prefix, suffix, min_support, frequent_itemsets);
        }
    }
}

vector<set<string>> getFrequentItemsetsEclat(const vector<Transaction>& transactions, int min_support) {
    map<set<string>, set<int>> itemsets;
    
    // Step 1: Convert transactions to vertical format
    for (int i = 0; i < transactions.size(); ++i) {
        for (const auto& item : transactions[i].items) {
            itemsets[ {item} ].insert(i);
        }
    }
    
    vector<set<string>> frequent_itemsets;
    set<string> items;
    for (const auto& item : itemsets) {
        items.insert(*item.first.begin());
    }
    
    set<string> prefix;
    eclat(itemsets, prefix, items, min_support, frequent_itemsets);
    
    return frequent_itemsets;
}

int main() {
    vector<Transaction> transactions = {
        {{"bread", "milk"}},
        {{"bread", "diaper", "beer", "eggs"}},
        {{"milk", "diaper", "beer", "cola"}},
        {{"bread", "milk", "diaper", "beer"}},
        {{"bread", "milk", "diaper", "cola"}}
    };
    
    int min_support = 2;
    vector<set<string>> frequent_itemsets = getFrequentItemsetsEclat(transactions, min_support);
    
    for (const auto& itemset : frequent_itemsets) {
        for (const auto& item : itemset) {
            cout << item << " ";
        }
        cout << endl;
    }
    
    return 0;
}
```

#### További vizsgálatok és felhasználások

Az asszociációs szabályok alkalmazása és hatékonysága erősen függ az adott problémától, az adathalmaz méretétől és szerkezetétől. Az Apriori és Eclat algoritmusokat gyakran tovább optimalizálják és testre szabják a speciális igények kielégítésére. Ezen algoritmusok kiterjesztéseit különböző területeken alkalmazzák, beleértve a vásárlói kosarak elemzését, ajánlórendszerek fejlesztését, és még sok mást. A mérések, mint a támogatás, bizalom és emelés fontos mérőszámok a szabályok relevanciájának és hasznosságának meghatározásában.

Ezen fejezet részletes elemzés révén betekintést nyújtott az asszociációs szabályok alapelveibe és két legelterjedtebb algoritmusába. A leírt módszerek és példák lehetővé teszik az olvasó számára, hogy megértse az asszociációs szabályok működését, és hogy milyen módszerekkel alkalmazhatók a gyakorlatban. Ezek az algoritmusok kulcsfontosságúak az adatbányászatban, lehetővé téve az adatokban rejlő értékes információk felfedezését és hasznosítását.

### Klaszterezés (K-means, Hierarchikus klaszterezés)

A klaszterezés egy fontos és széles körben alkalmazott technika az adatbányászatban és gépi tanulásban, melynek célja az adatok csoportosítása úgy, hogy az egyes csoportokon belül az adatok hasonlóak legyenek, míg a különböző csoportok között lévő adatok eltérőek. Ez a technika lehetővé teszi az adatok mögött rejlő struktúrák felfedezését és segít az adatok jobb megértésében. Ebben a fejezetben két gyakori klaszterezési algoritmust fogunk részletesen bemutatni: a K-means és a hierarchikus klaszterezést.

#### A klaszterezés alapjai

A klaszterezési feladatok célja az adatok partícionálása homogén csoportokba, ahol az egyes csoportok, azaz klaszterek belső tagjai egymáshoz hasonlóak, de különböznek a más klaszterek tagjaitól. A klaszterezés nem-felügyelt tanulási módszer, mivel az input adatok nincsenek címkézve, és az algoritmusnak az adatok struktúráját kell felfedeznie.

A klaszterezési algoritmusok számos különböző területen alkalmazhatók, többek között:
- Vásárlói szegmentálás
- Bírói ajánlórendszerek
- Kép- és hangfeldolgozás
- Genomikai elemzés

#### K-means algoritmus

A K-means algoritmus az egyik legismertebb és leggyakrabban használt klaszterezési algoritmus. A K-means alapja az adatok előre meghatározott számú klaszterbe történő elosztása úgy, hogy minimalizáljuk az egyes klaszterek belső varianciáját.

**Algoritmus lépései:**
1. **Klaszterközéppontok inicializálása**: Véletlenszerűen választunk ki K adatpontot a bemeneti adatok közül, amelyeket klaszterközéppontként használunk.
2. **Hozzárendelés**: Minden adatpontot hozzárendelünk a hozzá legközelebb eső klaszterközépponthoz.
3. **Klaszterközéppontok frissítése**: A klaszterközéppontokat frissítjük úgy, hogy átlagoljuk az egyes klaszterekhez tartozó adatpontokat.
4. **Iteráció**: Az előző két lépést addig ismételjük, amíg a klaszterközéppontok nem változnak, vagy a változás minimális lesz.

**Kritériumok:**
- **K**: A klaszterek előre meghatározott száma.
- **Távolságmérték**: Leggyakrabban az euklideszi távolságot használjuk az adatpontok és a klaszterközéppontok között.

**Példakód C++ nyelven:**
```cpp
#include <iostream>
#include <vector>
#include <cmath>
#include <limits>
#include <cstdlib>
#include <ctime>

using namespace std;

struct Point {
    double x, y;
    int cluster;

    Point(double x, double y) : x(x), y(y), cluster(-1) {}
};

double distance(const Point& a, const Point& b) {
    return sqrt(pow(a.x - b.x, 2) + pow(a.y - b.y, 2));
}

void kmeans(vector<Point>& points, int K) {
    // Randomly initialize cluster centers
    vector<Point> centroids;
    srand(time(0));
    for (int i = 0; i < K; ++i) {
        centroids.push_back(points[rand() % points.size()]);
    }

    bool changed;
    do {
        changed = false;

        // Assign points to the closest centroid
        for (auto& point : points) {
            int closest_cluster = -1;
            double min_distance = numeric_limits<double>::max();

            for (int i = 0; i < K; ++i) {
                double d = distance(point, centroids[i]);
                if (d < min_distance) {
                    min_distance = d;
                    closest_cluster = i;
                }
            }

            if (point.cluster != closest_cluster) {
                point.cluster = closest_cluster;
                changed = true;
            }
        }

        // Update centroids
        vector<int> count(K, 0);
        vector<double> sum_x(K, 0), sum_y(K, 0);

        for (const auto& point : points) {
            int cluster = point.cluster;
            sum_x[cluster] += point.x;
            sum_y[cluster] += point.y;
            ++count[cluster];
        }

        for (int i = 0; i < K; ++i) {
            if (count[i] > 0) {
                centroids[i] = Point(sum_x[i] / count[i], sum_y[i] / count[i]);
            }
        }
    } while (changed);
}

int main() {
    vector<Point> points = { {1.0, 2.0}, {1.5, 1.8}, {5.0, 8.0}, {8.0, 8.0}, {1.0, 0.6}, {9.0, 11.0} };
    int K = 2;

    kmeans(points, K);

    for (const auto& point : points) {
        cout << "Point (" << point.x << ", " << point.y << ") in cluster " << point.cluster << endl;
    }

    return 0;
}
```

#### Hierarchikus klaszterezés

A hierarchikus klaszterezés szintén széles körben használt módszer, mely két fő típusa az agglomeratív (bottom-up) és a divizív (top-down) módszerek. Az agglomeratív hierarchikus klaszterezés kezdetben minden adatpontot külön klaszterként kezel, majd fokozatosan egyesíti a legközelebbi klasztereket mindaddig, amíg egyetlen klasztert nem kapunk vagy egy előre meghatározott klaszterszámot nem érünk el. A divizív hierarchikus klaszterezés az ellenkező irányban működik: egyetlen klaszterből indul ki, és fokozatosan bontja azt kisebb, al-klaszterekre.

**Algoritmus lépései (agglomeratív hierarchikus klaszterezés):**
1. **Inicializálás**: Minden adatpontot külön klaszterként kezelünk.
2. **Távolságok kiszámítása**: Minden klaszterpár közötti távolságot kiszámoljuk.
3. **Klaszterek egyesítése**: Két legközelebbi klasztert egyesítünk.
4. **Távolságok frissítése**: Az új klaszter és a többi klaszter közötti távolságot újraszámoljuk.
5. **Iteráció**: Az előző két lépést addig ismételjük, amíg a kívánt klaszterszámot el nem érjük.

**Távolságmértékek:**
- **Single-linkage (legközelebbi szomszéd)**: A két klaszter közötti legközelebbi pontok távolságát veszi figyelembe.
- **Complete-linkage (legközelebbi szomszéd)**: A két klaszter között lévő legtávolabbi pontok távolságát méri.
- **Average-linkage**: A két klaszterben lévő összes pontpár közötti átlagos távolságot veszi alapul.

**Példakód C++ nyelven:**
```cpp
#include <iostream>
#include <vector>
#include <tuple>
#include <cmath>
#include <limits>
#include <map>

using namespace std;

struct Point {
    double x, y;

    Point(double x, double y) : x(x), y(y) {}
};

double singleLinkageDistance(const vector<Point>& cluster1, const vector<Point>& cluster2) {
    double minDistance = numeric_limits<double>::max();
    for (const auto& p1 : cluster1) {
        for (const auto& p2 : cluster2) {
            double distance = sqrt(pow(p1.x - p2.x, 2) + pow(p1.y - p2.y, 2));
            if (distance < minDistance) {
                minDistance = distance;
            }
        }
    }
    return minDistance;
}

void hierarchicalClustering(vector<Point>& points, int desiredClusters) {
    vector<vector<Point>> clusters;

    // Initial clusters with single points
    for (const auto& point : points) {
        clusters.push_back({ point });
    }

    while (clusters.size() > desiredClusters) {
        double minDistance = numeric_limits<double>::max();
        int cluster1Index = -1;
        int cluster2Index = -1;

        // Find the closest pair of clusters
        for (size_t i = 0; i < clusters.size(); ++i) {
            for (size_t j = i + 1; j < clusters.size(); ++j) {
                double distance = singleLinkageDistance(clusters[i], clusters[j]);
                if (distance < minDistance) {
                    minDistance = distance;
                    cluster1Index = i;
                    cluster2Index = j;
                }
            }
        }

        // Merge the closest pair of clusters
        clusters[cluster1Index].insert(clusters[cluster1Index].end(),
                                       clusters[cluster2Index].begin(),
                                       clusters[cluster2Index].end());
        clusters.erase(clusters.begin() + cluster2Index);
    }

    // Print clusters
    for (int i = 0; i < clusters.size(); ++i) {
        cout << "Cluster " << i << ":" << endl;
        for (const auto& point : clusters[i]) {
            cout << "(" << point.x << ", " << point.y << ")" << endl;
        }
    }
}

int main() {
    vector<Point> points = { {1.0, 2.0}, {1.5, 1.8}, {5.0, 8.0}, {8.0, 8.0}, {1.0, 0.6}, {9.0, 11.0} };
    int desiredClusters = 2;

    hierarchicalClustering(points, desiredClusters);

    return 0;
}
```

#### Klaszterek validálása és kiértékelése

A klaszterezési eredmények validálása és kiértékelése rendkívül fontos lépés annak biztosítására, hogy a létrehozott klaszterek valóban értelmes és releváns eredményeket adnak. Számos módszer létezik a klaszterek pontosságának mérésére:

1. **Belső mérőszámok**:
   - **Sziluett együttható**: Az egyes adatpontok kohézióját (vagyis mennyire jól illeszkedik a saját klaszterükhöz) és szétválasztását (mennyire különbözik a legközelebbi szomszéd klasztertől) méri.
   - **Dunn-index**: Az egyes klaszterek közötti legkisebb távolságot osztjuk az egyes klaszterek maximális átmérőjével.
   
2. **Külső mérőszámok**:
   - **Rand index**: A klaszterezési eredményeket összehasonlítja egy előre adott címkézett adathalmazzal.
   - **F-mérték**: Az átlagos párhuzamosság és visszahívás mérésére szolgál a címkézett adathalmazokon.

A klaszterezési algoritmusok kiválasztása és alkalmazása nagymértékben függ az adott probléma természetétől, az adathalmaz méretétől és az elvárt eredményektől. A K-means és a hierarchikus klaszterezés két alapvető és rendkívül hasznos módszer, melyek széles körben alkalmazhatók a gyakorlati adatbányászati feladatok megoldására. Ezen algoritmusok megértése és megfelelő alkalmazása lehetővé teszi az értékes információk felfedezését és az adatok strukturáltabb megértését.

