\newpage

## 3.4. Támogatott Vektor Gépek (Support Vector Machines) 

A modern adatbányászat és gépi tanulás területén a Támogatott Vektor Gépek (Support Vector Machines, SVM) kiemelkednek hatékonyságukkal és pontosságukkal. Az SVM-ek egy hatékony felügyelt tanulási algoritmusnak számítanak, melyeket széles körben alkalmaznak különböző osztályozási és regressziós feladatok megoldására. Az SVM-ek alapelve abban áll, hogy a bemeneti adatokat egy magasabb dimenziós térbe transzformálják, ahol céljuk, hogy a legnagyobb marginú hipersíkot találják meg, amely legjobban elválasztja a különböző osztályokhoz tartozó adatpontokat. A gyakorlatban az SVM nyújtotta megoldások sokoldalúsága és robusztussága miatt népszerűek a szöveg- és képosztályozásban, bioinformatikai feladatokban, valamint pénzügyi elemzések során. Ebben a fejezetben részletesen vizsgáljuk meg az SVM-ek elméleti hátterét, működési mechanizmusát, valamint bemutatjuk legfontosabb alkalmazási területeiket.

### Alapelvek és alkalmazások

A Támogatott Vektor Gépek (Support Vector Machines, SVM) a gépi tanulás egyik legfontosabb és legismertebb algoritmusának számítanak, melyeket elsősorban osztályozási és regressziós problémák megoldására alkalmaznak. Az SVM-ek elméleti alapjait Vladimir Vapnik és Alexey Chervonenkis dolgozták ki az 1960-as évek végén, és azóta a módszer számos tudományos és ipari alkalmazásban bizonyította hatékonyságát. Ebben az alfejezetben részletesen bemutatjuk az SVM-ek alapelveit, az elméleti hátterüket, valamint a gyakorlati alkalmazásukat is.

#### 1. Az SVM-ek alapelvei

Az SVM-ek célja egy olyan elkülönítő hipersík (hyperplane) megtalálása, amely a legjobban elválasztja a különböző osztályokhoz tartozó adatokat egy magasabb dimenziós térben. Ez az elkülönítő hipersík az, amely maximalizálja a két osztály között lévő margin-t, vagyis a legközelebbi adatpontok (más néven támogatott vektorok) és a hipersík közötti legkisebb távolságot.

##### 1.1 Az optimalizációs probléma

Az SVM-ek feladata eredetileg egy optimalizációs probléma megoldása. Az alábbiakban bemutatjuk az alapvető matematikai formulázást:

1. **Primal form:**

   Legyenek az adatok $\{(x_i, y_i)\}_{i=1}^n$, ahol $x_i \in \mathbb{R}^d$ a d-dimenziós bemeneti vektor és $y_i \in \{-1, +1\}$ a címke. Az SVM a következő optimalizációs problémát oldja meg:

   Minimizáljuk:

   $$
   \frac{1}{2} \|w\|^2
   $$

   Tárgykorlát:

   $$
   y_i (w \cdot x_i + b) \geq 1, \quad \text{minden} \; i-hez
   $$

   Itt $w$ az a súlyvektor, amely meghatározza a hipersík irányát, míg $b$ az offset (eltolás).

2. **Dual form:**

   A primal formálás kettős (dual) formája a következőképpen írható fel:

   Maximalizáljuk:

   $$
   \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j (x_i \cdot x_j)
   $$

   Tárgykorlát:

   $$
   \sum_{i=1}^n \alpha_i y_i = 0
   $$
   $$
   0 \leq \alpha_i \leq C, \quad \text{minden} \; i-hez
   $$

   Itt $\alpha_i$ olyan Lagrange-multiplikátorok, amelyeket a megoldás során számolunk ki. A „C” hiperparaméter a margin maximalizálása és a hiba tolerálása közötti trade-off-ot szabályozza.

##### 1.2 Kernel trükk

A kernel trükk (kernel trick) lehetővé teszi, hogy az SVM lineáris elválasztó hipersíkját nemlineáris osztályok esetén is alkalmazzuk azáltal, hogy az eredeti adatokat egy magasabb dimenziós térbe transzformáljuk. A kernel függvény $K(x_i, x_j)$ az adatpontok belső szorzatát számítja ki a transzformált térben anélkül, hogy explicite transzformálnánk az adatokat.

Gyakori kernel függvények:

1. **Lineáris kernel:** $K(x_i, x_j) = x_i \cdot x_j$
2. **Polinom kernel:** $K(x_i, x_j) = (\gamma x_i \cdot x_j + r)^d$
3. **Radiális bázisfüggvény (RBF) kernel:** $K(x_i, x_j) = \exp(-\gamma \|x_i - x_j\|^2)$

#### 2. Az SVM-ek alkalmazásai

Az SVM-ek hatékonysága és robusztussága miatt számos területen alkalmazhatók. Az alábbiakban néhány gyakori alkalmazási területet mutatunk be.

##### 2.1 Szövegosztályozás

Az SVM-ek kiválóan alkalmazhatók szövegsorozatok osztályozására, amelyek gyakran nagy dimenziójú jellemzőtérrel rendelkeznek. Például e-mailek SPAM szűréséhez az SVM-ek segítségével nagy pontossággal megkülönböztethetők a kívánt és nem kívánt üzenetek, hiszen a szöveg "zsák modellekkel" (bag-of-words) nagy méretű jellemzővektorrá konvertálható, majd az SVM-ek elválasztó hipersíkot azonosítanak, amely megkülönbözteti a SPAM-et a nem-SPAM-től.

##### 2.2 Képosztályozás

A képfelismerés és osztályozás területén az SVM-ek szintén figyelemre méltó eredményeket érnek el. A különböző kategóriákba tartozó képeken található jellemzők (például szélek, textúrák) alapján az SVM-ek képesek elkülöníteni bizonyos tárgyakat, mint például arcokat, járműveket, stb. A Support Vector Machine algoritmus megfelelő magfüggvény választásával képes feldolgozni a nagyméretű vizuális adatokat és elkülöníteni a célobjektumokat a háttértől.

##### 2.3 Bioinformatika

A bioinformatika területén az SVM-eket gyakran használják génexpressziós adatok elemzésére, ahol az SVM-ek segítségével kategorizálhatók a különböző génminták. Ez segíthet például a rákkutatásban, ahol különböző génexpressziós profilokat tudunk azonosítani, és ez alapján képesek lehetünk kategorizálni a különböző típusú daganatos sejteket.

##### 2.4 Pénzügyi elemzések

A pénzügyi világban az SVM-ek segíthetnek az előrejelzések készítésében és a kockázatkezelésben. Például a hitelkérelmek vagy befektetési portfóliók osztályozása során az SVM-ek képesek azonosítani a potenciálisan veszélyes befektetéseket vagy hitelkérelmeket a történelmi adatokat és más pénzügyi mutatókat elemezve.

#### 3. Írott példa: SVM implementálása C++ nyelven

Noha az SVM-ek implementálása általában magas szintű nyelvekben (mint a Python) történik, itt bemutatunk egy egyszerű példát C++-ban a libsvm könyvtár segítségével.

```cpp
#include <iostream>
#include <svm.h>

int main() {
    // Training data
    svm_problem problem;
    problem.l = 4; // Number of training examples
    double labels[] = {1, -1, 1, -1}; // Example labels
    problem.y = labels;

    // Feature data
    svm_node *x_space = new svm_node[8];
    x_space[0].index = 1; x_space[0].value = 1;
    x_space[1].index = -1; // End of first vector
    x_space[2].index = 1; x_space[2].value = -1;
    x_space[3].index = -1; // End of second vector
    x_space[4].index = 1; x_space[4].value = 2;
    x_space[5].index = -1; // End of third vector
    x_space[6].index = 1; x_space[6].value = -2;
    x_space[7].index = -1; // End of fourth vector
    svm_node* x[] = {&x_space[0], &x_space[2], &x_space[4], &x_space[6]};
    problem.x = x;

    // Set up the SVM parameter
    svm_parameter param;
    param.svm_type = C_SVC;
    param.kernel_type = RBF;
    param.C = 1;
    param.gamma = 0.5;
    param.eps = 0.001;
    param.probability = 1;

    // Train the SVM
    svm_model *model = svm_train(&problem, &param);

    // Test data
    svm_node test_node[2];
    test_node[0].index = 1; test_node[0].value = 0.5;
    test_node[1].index = -1; // End of vector

    // Predict the label
    double prob_estimates[2];
    double label = svm_predict_probability(model, test_node, prob_estimates);
    std::cout << "Predicted label: " << label << std::endl;

    // Clean up
    svm_free_and_destroy_model(&model);
    delete[] x_space;

    return 0;
}
```

Ebben a példában a `libsvm` könyvtárat használjuk, amely lehetővé teszi az SVM-alapú modellek egyszerű implementálását és alkalmazását. A bemutatott példa program négy tanuló adatpontot és egy SVM modellt alkalmaz az adatok betanítására, majd egy új adatpont osztályozására.

#### 4. Előnyök és korlátok

Az SVM-eknek számos előnye van, amelyek miatt széles körben használják őket:

1. **Magas dimenziókban is hatékony:** Az SVM-ek hatékonyak, még akkor is, ha a jellemzőtér magas dimenziójú.
2. **Robusztusság:** Az SVM-ek robusztusak az overfitting ellen, különösen akkor, ha optimálisan választják meg a hiperparamétereket.
3. **Nonlineáris elválasztás:** A kernel trükk révén az SVM-ek képesek nonlineáris osztályozási problémák megoldására is.

Azonban néhány hátránnyal is számolni kell:

1. **Nagy számítási igény:** A nagy adatállományok esetén az SVM-ek számításigényesek lehetnek, különösen a kernel mátrix számítása során.
2. **Optimalizálás komplexitása:** A híperparaméterek (pl. C és gamma) megválasztása kritikus és gyakran nem triviális feladat, amely optimalizációs technikákat igényel.
3. **Nem mindig skálázható:** Nagy méretű adathalmazok esetén az SVM-ek alkalmazása gyakran nehézkes lehet, mivel a képzés és predikció időigényes lehet.

#### Összegzés

A Támogatott Vektor Gépek hangsúlyos szerepet játszanak a modern gépi tanulási és adatbányászati módszerek között. Alapelveik a statisztikai tanulás elméletén alapulnak, és számos alkalmazásban bizonyítottak már hatékonyságukat. A jelen fejezetben részletesen bemutattuk az SVM-ek elméleti hátterét, optimalizációs módszereit és gyakorlati alkalmazásait különféle területeken. Az SVM-ek rugalmasságuk és robusztusságuk révén továbbra is alapvető eszközei maradnak a gépi tanulási megoldásoknak, és számos új alkalmazási területen hozhatnak áttörést.

