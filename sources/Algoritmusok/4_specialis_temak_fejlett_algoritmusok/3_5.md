\newpage

## 3.5. Dimenziócsökkentés 

A modern adatelemzés világában gyakran találkozunk nagyméretű, sokdimenziós adathalmazokkal, amelyek kezelése és elemzése komoly kihívást jelenthet. A dimenziócsökkentés olyan technikák összessége, melyek célja az adatok dimenzióinak számának csökkentése úgy, hogy közben a legfontosabb információkat megőrizzük. Ezen technikák alkalmazásával nem csak az adatok vizsgálhatósága és megértése válik könnyebbé, de az adatfeldolgozás sebessége és hatékonysága is javulhat. Ebben a fejezetben két kiemelkedő dimenziócsökkentési módszerrel foglalkozunk: a Principal Component Analysis-szel (PCA), amely a lineáris dimenziócsökkentés egyik legismertebb módszere, valamint a t-distributed Stochastic Neighbor Embedding-gel (t-SNE), ami egy nem-lineáris technika, különösen alkalmas az adatok vizualizálására. Mindkét módszernek megvan a maga előnye és alkalmazási területe, amit részletesen megvizsgálunk a következő szakaszokban.

### PCA (Principal Component Analysis)

#### Bevezetés

A Principal Component Analysis (főkomponens-analízis, PCA) egy statisztikai eljárás, amelynek célja nagy, többdimenziós adathalmazok komplexitásának csökkentése anélkül, hogy az alapvető információ elveszne. Az eljárás a legfontosabb változók (főkomponensek) kiválasztásával csökkenti az adatdimenziókat. Ez a módszer különösen hasznos az adatok előfeldolgozásában, adatvizualizációban és adatok közötti minták felismerésében.

#### Matematikai háttér

A PCA alapja a valószínűségszámítás és a lineáris algebra. Az eljárás során ortogonális tengelyeket, úgynevezett főkomponenseket azonosítunk az adathalmazon belül, amelyek maximális varianciával rendelkeznek. Az első főkomponens az a vonal, amely az adatok legnagyobb variabilitását magyarázza, a második főkomponens az elsőre merőleges, és az adatok második legnagyobb variabilitását magyarázza, és így tovább.

##### Adatok előkészítése

Az adatok előkészítése szükséges lépés minden PCA alkalmazása előtt. Az adatok standardizálása, azaz minden változó esetében a középérték kivonása és a szórás szerinti osztás, gyakran szükséges, különösen, ha az adatok különböző skálán mozognak. Ez az adatstandardizálás biztosítja, hogy minden változó egyenlő súlyozással induljon a főkomponensek keresésekor.

$$ X_{\text{scaled}} = \frac{X - \mu}{\sigma} $$

ahol $X$ az eredeti adat, $\mu$ az átlag, és $\sigma$ a szórás.

##### Kovariancia mátrix

A PCA lényege, hogy a változók közötti összefüggéseket vizsgáljuk meg a kovariancia mátrixon keresztül. A kovariancia mátrix segítségével megállapíthatjuk, hogy mely változók mozognak együtt, és milyen mértékben. A kovariancia mátrix előállításához szükséges:

$$ \mathbf{C} = \frac{1}{n-1} (X_{\text{scaled}}^T X_{\text{scaled}}) $$

ahol $\mathbf{C}$ a kovariancia mátrix, $X_{\text{scaled}}$ a standardizált adatmátrix, és $n$ az adatminta száma.

##### Sajátértékek és sajátvektorok

A kovariancia mátrix kiszámítását követően meghatározandók annak sajátértékei és sajátvektorai. A sajátvektorok adják meg az új tengelyek irányát (főkomponensek), míg a sajátértékek az ezen tengelyek mentén lévő varianciát (fontosságot). A PCA során az alábbi egyenletrendszert kell megoldani:

$$ \mathbf{C} \mathbf{v} = \lambda \mathbf{v} $$

ahol $\mathbf{v}$ a sajátvektor, és $\lambda$ a sajátérték.

##### Dimenziócsökkentés

Miután meghatároztuk a sajátvektorokat és sajátértékeket, kiválasztunk $k$ sajátvektort az adatok új koordináta-rendszerben történő megjelenítéséhez. Az adatokat ezek mentén vetítjük le, így csökkentve a dimenziókat. Az új, csökkentett dimenziójú adatkészlet így nézhet ki:

$$ X' = X_{\text{scaled}} \cdot \mathbf{W} $$

ahol $\mathbf{W}$ a kiválasztott sajátvektorokból álló mátrix.

#### Példa megvalósítása C++ nyelven

Noha a PCA implementálása különböző programozási nyelveken lehetséges, az alábbiakban egy egyszerű példa látható C++ nyelven az Eigen könyvtár használatával:

```cpp
#include <iostream>
#include <Eigen/Dense>

using namespace Eigen;
using namespace std;

int main() {
    // Example data matrix (4 samples, 3 features)
    MatrixXd data(4, 3);
    data << 2.5, 2.4, 1.9,
            0.5, 0.7, 0.1,
            2.2, 2.9, 0.4,
            1.9, 2.2, 3.1;

    // Mean centering
    VectorXd mean = data.colwise().mean();
    MatrixXd centered = data.rowwise() - mean.transpose();

    // Covariance matrix
    MatrixXd cov = (centered.transpose() * centered) / double(data.rows() - 1);

    // Eigen decomposition
    SelfAdjointEigenSolver<MatrixXd> eig(cov);
    MatrixXd eig_vectors = eig.eigenvectors().rightCols(2); // Select top 2 components

    // Transform data
    MatrixXd transformed = centered * eig_vectors;

    cout << "Original data:\n" << data << "\n";
    cout << "Transformed data:\n" << transformed << "\n";

    return 0;
}
```

#### Alkalmazási területek

A PCA számos területen alkalmazható, többek között:

1. **Adatvizualizáció**: A magas dimenziójú adatok két- vagy háromdimenziós térbe való vetítése segít felfedezni az adatstruktúrát és az összefüggéseket.
2. **Zajcsökkentés**: Képek és jelek zaját csökkenthetjük, ha csak a legfontosabb főkomponenseket tartjuk meg.
3. **Jellemzők kiválasztása**: A legjelentősebb jellemzők kiválogatásával hatékonyabb gépi tanulási modellek készíthetők.
4. **Tömörítés**: Adatok tárolásakor és továbbításakor is hasznos lehet a dimenziók csökkentése.

#### Előnyök és hátrányok

**Előnyök**:
- Csökkenti az adatok komplexitását és könnyen értelmezhető struktúrákat hoz létre.
- Javítja a gépi tanulási algoritmusok teljesítményét a kevesebb adattal való munkavégzés révén.
- Használható zajcsökkentésre és adatvizualizációra is.

**Hátrányok**:
- Lineáris módszer, amely nem feltétlenül hatékony nem-lineáris adatok esetén.
- Információvesztéssel járhat, ha túl sok dimenziót távolítunk el.
- Az eredmény függ a standardizálás minőségétől és az adatok természetétől.

#### Következtetés

A PCA egy erőteljes technika a dimenziócsökkentésben, amely lehetővé teszi az adatok hatékonyabb és értelmezhetőbb formába történő átalakítását. Bár van néhány korlátozása, a PCA széles körben alkalmazható a modern adatbányászat és gépi tanulási feladatok megkönnyítésére és optimalizálására. Az előzőekben bemutatott elméleti magyarázatok és gyakorlati példák segítségével remélhetőleg betekintést nyertél annak fontosságába és alkalmazásába.

### t-SNE (t-Distributed Stochastic Neighbor Embedding)

#### Bevezetés

A t-Distributed Stochastic Neighbor Embedding (t-SNE) egy nemlineáris dimenziócsökkentő algoritmus, amelyet Laurens van der Maaten és Geoffrey Hinton fejlesztett ki 2008-ban. A t-SNE célja a magas dimenziójú adatok két- vagy háromdimenziós térbe történő vetítése, miközben megőrzi az adatok lokális szerkezetét. Ez különösen hasznos az adatok vizualizálásában, mivel az emberi látás korlátozott dimenzióban működik. A t-SNE nagy népszerűségre tett szert a gépi tanulás közösségében különösen a képfeldolgozás, szövegelemzés és egyéb területeken, ahol a nagyméretű, sokdimenziós adatok vizuális elemzésére van szükség.

#### Matematikai háttér

A t-SNE algoritmus a magas dimenziójú adatok közötti lokális szerkezetet próbálja megőrizni egy alacsonyabb dimenziós térben. Ez egy kölcsönhatás-követelmény optimalizálásán alapul, amely a pontok közötti páronkénti távolságok megőrzésén dolgozik.

##### Szemantikai hasonlóság mértéke

Az algoritmus első lépésében a magas dimenziójú térben található adatok közötti hasonlóságot mérjük. Ehhez a t-SNE a feltételes valószínűségeken alapuló lágyszámítású hasonlósági mértéket használja. A hasonlóságot a mindkét irányban valószínűségi sűrűségfüggvény, a Gauss-eloszlás segítségével számítják ki:

$$ p_{j|i} = \frac{\exp \left( - \frac{\| x_i - x_j \|^2}{2 \sigma_i^2} \right)}{\sum_{k \neq i} \exp \left( - \frac{\| x_i - x_k \|^2}{2 \sigma_i^2} \right)} $$

maydém a $\sigma_i$ az adatok helyének adaptív sáv szélességének paramétere, melyet úgy választanak, hogy specifikus perplektitást (azaz speciális információmennyiségi mértéket) fenntartsanak.

A perplektitás a sáv szélességének ésszerű beállítása és az adatok közötti távolság feltérképezésének alapja. Az alkalmazott perplektitás tipikusan a következőképpen definiált:

$$ \text{Perplexity}(P_i) = 2^{H(P_i)} $$

ahol $H(P_i)$ az entrópia:

$$ H(P_i) = - \sum_j p_{j|i} \log_2 p_{j|i} $$

Ezután a hasonlóságokat szimmetrizálják:

$$ p_{ij} = \frac{p_{j|i} + p_{i|j}}{2n} $$

##### Hasonlóság mértéke az alacsony dimenziós térben

Az alacsony dimenziós térben a hasonlóság mértéke Q-eloszlással adható meg, amely t-eloszlást használ, nem Gauss-eloszlást:

$$ q_{ij} = \frac{\left(1 + \| y_i - y_j \|^2 \right)^{-1}}{\sum_{k \neq l} \left(1 + \| y_k - y_l \|^2 \right)^{-1}} $$

Itt $y_i$ és $y_j$ az alacsony dimenziójú térben levő adatpontok, és a t-eloszlás használata lehetővé teszi az adatok jobb elnyújthatóságát az alacsony dimenziós térben.

##### Kölcsönhatás minimalizálása

Az optimalizáció során a cél az adatok közötti távolságok reprodukálása az új térben, ami a Kullback-Leibler divergencia (vagy entropia mértéke) minimalizálásával érhető el a két eloszlás között:

$$ KL(P || Q) = \sum_{i} \sum_{j} p_{ij} \log \frac{p_{ij}}{q_{ij}} $$

Ez az optimalizáció tipikusan egy gradient descent (gradiens leszállás) módszeren keresztül történik.

#### Algoritmus lépései

A t-SNE algoritmus következő lépésekből áll:

1. **Adatok előkészítése**: Standardizálni kell az adatokat, ha szükséges.
2. **Helyi hasonlóság kiszámítása**: A magas dimenziójú térben levő adatok közötti feltételes valószínűségeket kiszámítják.
3. **Perplektitás beállítása**: Az adaptív sáv szélességének beállítása a perplektitás alapján.
4. **Szimmetrizálás**: Szimmetrikus hasonlósági mátrix létrehozása.
5. **Alacsony dimenziójú hasonlósági mátrix**: t-eloszlást használó valószínűségi sűrűségfüggvénnyel.
6. **Optimalizáció**: A Kullback-Leibler divergencia minimalizálása az alacsony dimenziójú koordináták frissítésével.

#### Alkalmazási példák

A t-SNE algoritmust leggyakrabban az alábbi területeken alkalmazzák:

1. **Adatvizualizáció**: Az adatok vizuális reprezentációja, hogy az emberi agy számára is könnyen értelmezhetővé váljon az adatok közötti kapcsolat.
2. **Képfeldolgozás**: Képek közötti hasonlóságok megfigyelése és a vizuális minták felfedezése.
3. **Szövegelemzés**: Szövegbeágyazások dimenzióinak lecsökkentése a hatékonyabb elemzés érdekében.
4. **Bioinformatika**: Genetikai adatok vizualizálása és a különböző genetikai minták felfedezése.
5. **Anomália detekció**: Rendellenes adatpontok azonosítása az alacsony dimenziójú térben.

#### Előnyök és hátrányok

**Előnyök**:
- Nagyszerű eredményeket ad az adatok vizualizálásában.
- Képes kezelni a bonyolult, nemlineáris struktúrákat is.
- Megőrzi az adatok lokális szerkezetét.

**Hátrányok**:
- Magas a számítási költsége, különösen nagy adatkészletek esetén.
- Nem mindig trivialis a paraméterek beállítása (perplektitás, tanulási ráta).
- Az eredmények nem mindig reprodukálhatók egyforma pontossággal, főleg a sztochasztikus természet miatt.

#### Gyakorlati implementáció C++ nyelven

A C++ nyelven a t-SNE algoritmus megvalósítása bonyolultabb lehet, ezért itt csak egy rövid kód részletet mutatok be. Az alábbi példa az Eigen könyvtár segítségével mutatja be a Gauss-eloszlásra jellemző hasonlósági mátrix kiszámításának lépéseit:

```cpp
#include <iostream>
#include <Eigen/Dense>
#include <vector>
#include <cmath>

using namespace Eigen;
using namespace std;

double gauss_similarity(const VectorXd& xi, const VectorXd& xj, double sigma) {
    return exp(-(xi - xj).squaredNorm() / (2 * sigma * sigma));
}

MatrixXd compute_pairwise_similarities(const MatrixXd& data, double sigma) {
    int n = data.rows();
    MatrixXd similarities(n, n);
    for(int i = 0; i < n; ++i) {
        for(int j = 0; j < n; ++j) {
            if(i != j) {
                similarities(i, j) = gauss_similarity(data.row(i), data.row(j), sigma);
            }
        }
    }
    return similarities;
}

MatrixXd compute_conditional_probabilities(const MatrixXd& similarities) {
    MatrixXd probabilities = similarities;
    for(int i = 0; i < similarities.rows(); ++i) {
        probabilities.row(i) /= similarities.row(i).sum();
    }
    return probabilities;
}

int main() {
    MatrixXd data(4, 3);
    data << 2.5, 2.4, 1.9,
            0.5, 0.7, 0.1,
            2.2, 2.9, 0.4,
            1.9, 2.2, 3.1;
    
    double sigma = 1.0; // Example value, usually determined through perplexity

    MatrixXd similarities = compute_pairwise_similarities(data, sigma);
    MatrixXd probabilities = compute_conditional_probabilities(similarities);

    cout << "Pairwise similarities:\n" << similarities << "\n";
    cout << "Conditional probabilities:\n" << probabilities << "\n";

    return 0;
}
```

#### Következtetés

A t-SNE egy rendkívül hasznos és hatékony eszköz nagy adathalmazok vizualizálásához és értelmezéséhez. Bár számos előnnyel jár, amelyek lehetővé teszik az adatok rejtett szerkezetének feltárását, a számítási költségek és a paraméterek beállításának bonyolultsága kihívások elé is állíthatja a felhasználókat. A fent bemutatott elméleti magyarázatok és gyakorlati példák segítségével remélhetőleg világossá vált a t-SNE alkalmazásának módja és jelentősége a dimenziócsökkentés területén.

