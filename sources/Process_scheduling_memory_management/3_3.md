\newpage

## 11. Paging and Swapping

The effective management of memory is critical to the performance and stability of the Linux operating system. This chapter delves into the essential mechanisms of paging and swapping, foundational techniques that enable efficient memory utilization and process management. We will explore the structure and function of page tables, the integral role they play in translating virtual addresses to physical addresses, and the nature of page faults that occur when a requested page is not in memory. Next, we’ll examine the principles and practices behind swapping, where processes' inactive memory pages are moved to disk to free up RAM, and delve into the management of swap space. Finally, we will discuss how the Linux kernel handles out-of-memory situations, ensuring system reliability even when memory resources are critically low. This comprehensive exploration provides the groundwork for understanding the sophisticated memory management strategies that Linux employs to maintain robust and efficient operation.

### Page Tables and Page Faults

In the realm of modern operating systems, one of the most critical tasks is the efficient management of a system's memory. Linux, like many other operating systems, uses a sophisticated memory management scheme called virtual memory. Virtual memory allows the system to provide each process with its own isolated address space, thereby enhancing both security and stability. Central to this scheme are page tables and the mechanisms for handling page faults. This chapter delves deeply into these concepts, elucidating their roles, structures, and the intricate processes that govern their operation.

#### Virtual Memory: A Brief Recap

Virtual memory provides an abstraction that decouples the program's view of memory from the physical memory installed in the computer. This allows processes to use more memory than is physically available, simplifies memory management, and provides a level of protection between processes. At the heart of virtual memory is the page table, which is responsible for translating virtual addresses to physical addresses.

#### Page Tables

A page table is a data structure used by the operating system to manage the mapping between virtual addresses and physical addresses. Each process in a Linux system has its own page table, allowing it to have an isolated address space.

##### Levels of Page Tables

In x86-64 architectures, Linux uses a four-level page table hierarchy:

1. **Page Global Directory (PGD):** The highest level, which points to the Page Upper Directory.
2. **Page Upper Directory (PUD):** The second level, which points to the Page Middle Directory.
3. **Page Middle Directory (PMD):** The third level, which points to the Page Table.
4. **Page Table (PT):** The lowest level, which contains the actual mappings of virtual addresses to physical addresses.

Each entry in a page table points to the base address of the next level of the table or the physical address of a memory page. The hierarchical nature of page tables reduces the amount of memory required to manage large address spaces.

##### Page Table Entries

A page table entry (PTE) is a data structure that contains the physical address of the memory page and various control bits. The typical bits in a PTE include:

- **Present (P) Bit:** Indicates whether the page is currently in physical memory.
- **Write (W) Bit:** Indicates if the page is writable.
- **User/Supervisor (U/S) Bit:** Determines if the page is accessible from user mode or only kernel mode.
- **Accessed (A) Bit:** Set by the hardware when the page is accessed.
- **Dirty (D) Bit:** Set by the hardware when the page is written to.

Here's a quick representation of a page table entry in a simplified form:

```c++
struct PageTableEntry {
  uint64_t present : 1;
  uint64_t write : 1;
  uint64_t user : 1;
  uint64_t reserved : 9;
  uint64_t frame : 52;
};
```

This struct demonstrates how a 64-bit page table entry might be partitioned into various fields, including control bits and the frame number.

#### Page Faults

A page fault occurs when a process attempts to access a page that is not currently mapped to physical memory. Page faults can occur for several reasons:

1. **Page Not Present:** The page is not loaded in physical memory.
2. **Protection Violation:** The access does not comply with the protection bits (e.g., writing to a read-only page).

##### Handling Page Faults

The Linux kernel handles page faults through a series of steps:

1. **Exception Generated:** The CPU detects the invalid memory access and generates a page fault exception.
2. **Page Fault Handler:** The kernel catches the exception and invokes the page fault handler.
3. **Check Validity:** The handler determines whether the fault is legitimate (i.e., the address is within a valid region but not currently mapped) or an error (e.g., accessing an invalid address).
4. **Allocate Page:** If the fault is valid, the kernel allocates a new physical page or swaps in an existing page from disk.
5. **Update Page Tables:** The kernel updates the page tables to map the virtual address to the new physical page.
6. **Resume Execution:** The process is resumed at the instruction that caused the page fault.

Here’s a pseudo-code representation of a simplified page fault handler:

```c++
void page_fault_handler(uint64_t faulting_address, Process *proc) {
    PageTableEntry *pte = find_pte(proc->page_table, faulting_address);
    
    if (!pte->present) {
        PhysicalPage *page = allocate_page();
        if (page) {
            pte->frame = page->frame_number;
            pte->present = 1;
            pte->write = 1; // Example: make the page writable
        } else {
            // Handle out-of-memory situation
        }
    } else if (!pte->write) {
        // Handle write protection violation
    } else {
        // Handle other types of faults
    }
    
    // Continue execution
}
```

This pseudo-code provides a high-level view of how a page fault handler might allocate a new page and update the page table.

#### Swapping and Swap Space Management

While page tables and page faults primarily deal with managing currently active memory, swapping is a mechanism to extend the available memory by using disk space.

##### Swap Space

Swap space is a designated area on the disk where inactive pages can be moved out of physical memory to free up space for active pages. Swap space can be configured in one or more swap partitions or swap files.

##### Swap Operation

When the system runs low on physical memory, the kernel will select pages to move to swap space based on certain criteria, such as how recently or frequently the page has been accessed. This decision is governed by the page replacement algorithm, such as the Least Recently Used (LRU) algorithm.

The process of swapping includes the following steps:

1. **Select a Victim Page:** Choose a page to swap out using the page replacement algorithm.
2. **Write Page to Disk:** Save the contents of the page to the swap space.
3. **Update Page Table:** Mark the page as swapped out and update the page table entry to reflect its location on disk.
4. **Allocate New Page:** Use the freed physical memory to satisfy the current memory request.

When a swapped-out page is accessed again, another page fault occurs, and the kernel must read the page back into physical memory, possibly swapping out another page to make room.

Swapping can dramatically increase the effective amount of memory available but comes at the cost of higher latency, as disk access is significantly slower than access to RAM.

#### Handling Out-of-Memory Situations

Despite the use of paging and swapping, systems can still run into situations where memory is exhausted. Linux handles these out-of-memory (OOM) conditions with a mechanism called the OOM killer.

##### OOM Killer

The OOM killer is part of the Linux kernel that is invoked when the system is critically low on memory. Its purpose is to free up memory by terminating one or more processes.

The OOM killer selects processes to kill based on an *OOM score*, which takes into account factors such as:

- **Memory Use:** Processes using large amounts of memory are prime targets.
- **Process Priority:** Higher priority processes are less likely to be killed.
- **Runtime:** Processes that have been running for a long time are less likely to be killed.

Here’s a simplified flow of how the OOM killer works:

```c++
void out_of_memory() {
    Process *victim = select_victim();
    if (victim) {
        terminate_process(victim);
        free_resources(victim);
    } else {
        // Handle situation where no suitable victim can be found
    }
}
```

This pseudo-code represents the high-level logic of the OOM killer, selecting a process to terminate based on its OOM score and freeing up its memory.

#### Conclusion

In summary, page tables and page faults are fundamental to Linux's ability to manage memory efficiently. By leveraging a hierarchical page table structure, the kernel can translate virtual addresses to physical addresses and handle memory allocation dynamically. Page faults, while initially causing a performance hiccup, are managed in a seamless way to ensure the stability and continuity of running processes. Swapping extends the system's effective memory by using disk space, albeit with some performance trade-offs. Finally, the kernel's handling of out-of-memory situations through the OOM killer ensures that the system can recover from critical low-memory conditions by judiciously terminating processes. These mechanisms collectively form a robust framework that allows Linux to handle memory management challenges in diverse and demanding environments.

### Swapping and Swap Space Management

As modern computing environments continue to demand more effective and efficient memory management, swapping and swap space management have become essential components of the Linux memory management system. Swapping allows the operating system to extend its available memory space by utilizing disk storage to hold inactive memory pages, enabling more effective utilization of physical memory. This chapter explores the principles, mechanisms, and management strategies of swapping and swap space in detail, offering a comprehensive understanding of how Linux achieves robust and scalable memory management.

#### Understanding Swapping

Swapping is a memory management technique wherein pages of memory are copied to a designated space on the disk, known as swap space, to free up physical memory (RAM) for other processes. When those pages are needed again, they are read back into physical memory from the swap space. This process effectively allows the system to use more memory than is physically available, albeit with some trade-offs in performance.

##### Swap Space

Swap space is a dedicated area on the disk used for the purposes of swapping. It can be configured as one or more swap partitions or as swap files within a filesystem. The choice between swap partitions and swap files can depend on specific use cases and requirements:

- **Swap Partitions:** These are distinct disk partitions allocated exclusively for swap space. They generally offer better performance due to reduced fragmentation and certain optimized I/O operations.
- **Swap Files:** Swap files reside within a filesystem and offer more flexibility. They can be resized or moved more easily than partitions and can be used when the disk layout cannot be easily modified to create a new partition.

##### Configuring Swap Space

Creating and attaching swap space involves several steps, whether it be partition-based or file-based. Below are the steps to configure both types of swap space:

###### Creating and Enabling a Swap Partition

1. **Partition the Disk:**
   - Use a disk partitioning tool like `fdisk` or `parted` to create a new swap partition.

   ```bash
   sudo fdisk /dev/sdb
   ```

2. **Format the Partition:**
   - Format the new partition as swap space.

   ```bash
   sudo mkswap /dev/sdb1
   ```

3. **Enable the Swap Partition:**
   - Enable the swap partition for use.

   ```bash
   sudo swapon /dev/sdb1
   ```

4. **Persist the Configuration:**
   - Add the swap partition to `/etc/fstab` to enable it at boot.

   ```bash
   echo '/dev/sdb1 none swap sw 0 0' | sudo tee -a /etc/fstab
   ```

###### Creating and Enabling a Swap File

1. **Create the Swap File:**
   - Create a file that will serve as the swap space.

   ```bash
   sudo dd if=/dev/zero of=/swapfile bs=1M count=2048  # Creates a 2GB swap file
   ```

2. **Set Correct Permissions:**
   - Adjust the file permissions to ensure it is accessible only by the root user.

   ```bash
   sudo chmod 600 /swapfile
   ```

3. **Format the Swap File:**
   - Format the file as swap space.

   ```bash
   sudo mkswap /swapfile
   ```

4. **Enable the Swap File:**
   - Enable the swap file for use.

   ```bash
   sudo swapon /swapfile
   ```

5. **Persist the Configuration:**
   - Add the swap file to `/etc/fstab` to enable it at boot.

   ```bash
   echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab
   ```

##### Swap Management Commands

Linux provides several commands to manage and monitor swap space.

- **swapon and swapoff:** These commands enable and disable swap, respectively. They can be used to manage individual swap partitions or files.

```bash
sudo swapon /dev/sdb1
sudo swapoff /swapfile
```

- **free:** This command displays the amount of free and used memory in the system, including swap space.

```bash
free -h
```

- **swapon -s:** This command shows detailed information about all the swap areas currently in use.

```bash
swapon -s
```

- **/proc/swaps:** This virtual file provides detailed information about the active swap areas in the system.

```bash
cat /proc/swaps
```

#### The Role of the Virtual Memory Manager (VMM)

The Virtual Memory Manager (VMM) in the Linux kernel is responsible for making decisions about memory allocation and swapping. It employs advanced algorithms to maintain a balance between performance and memory efficiency.

##### Page Replacement Algorithms

When the system decides to swap out a page, it must choose which page to swap. This decision is governed by a page replacement algorithm. The most commonly used algorithm in Linux is the Least Recently Used (LRU) algorithm. The LRU algorithm selects the least recently accessed pages for swapping out, based on the assumption that pages accessed more recently are likely to be accessed again soon.

To implement LRU, the kernel maintains a list of active and inactive pages. The active list contains pages that are actively being used, while the inactive list contains pages that have not been used recently. Pages from the inactive list are the primary candidates for swapping.

##### Managing the Swap Cache

Linux uses a structure called the swap cache to optimize swap operations. The swap cache is an in-memory cache of pages that are in the process of being swapped out or have just been swapped out. The swap cache ensures that if a process accesses a page that has been recently swapped out, the page can be retrieved from the swap cache without needing to read it from disk, thus improving performance.

#### Swap Tuning Parameters

Linux provides several tunable parameters that can be adjusted to optimize swap performance and behavior. These parameters can be set using the `/proc/sys/vm` interface or through `sysctl`.

##### Swappiness

The swappiness parameter controls the relative weight given to swapping out pages as opposed to shrinking the filesystem caches. It ranges from 0 to 100:

- **Low Swappiness (0-20):** This setting minimizes swapping, keeping pages in memory as long as possible.
- **High Swappiness (60-100):** This setting makes the kernel more aggressive in swapping out pages to free up memory.

The default value is typically set to 60. To adjust the swappiness:

```bash
sudo sysctl vm.swappiness=30
```

Or add it to `/etc/sysctl.conf` for persistence:

```bash
echo 'vm.swappiness=30' | sudo tee -a /etc/sysctl.conf
```

##### Dirty Ratio and Dirty Background Ratio

These parameters control the behavior of the page writeback, which is the process of writing modified (dirty) pages back to disk:

- **vm.dirty_ratio:** The maximum percentage of system memory that can be filled with dirty pages before the process must write those pages to disk.
- **vm.dirty_background_ratio:** The percentage at which the system triggers the background process to start writing dirty pages to disk.

Reducing these parameters can help ensure that there is more memory available for active processes and reduce the likelihood of swapping.

```bash
sudo sysctl vm.dirty_ratio=10
sudo sysctl vm.dirty_background_ratio=5
```

These changes can also be added to `/etc/sysctl.conf` for persistence:

```bash
echo 'vm.dirty_ratio=10' | sudo tee -a /etc/sysctl.conf
echo 'vm.dirty_background_ratio=5' | sudo tee -a /etc/sysctl.conf
```

#### Performance and Trade-offs

Swapping, while providing the advantage of extending the effective memory, comes with certain trade-offs. Accessing swap space on disk is significantly slower than accessing RAM. Consequently, excessive swapping, known as "swap thrashing," can lead to severe performance degradation. Proper tuning of swappiness, dirty ratios, and efficient page replacement algorithms are crucial to balancing memory use between physical memory and swap space.

##### Mitigating Swap Thrashing

Swap thrashing occurs when the system spends more time swapping pages in and out of memory than executing the processes. Strategies to mitigate swap thrashing include:

- **Increasing Physical Memory:** If swap usage is consistently high, it may indicate a need for more RAM.
- **Optimizing Software:** Ensuring applications are efficiently using memory can reduce the need for swapping.
- **Tuning Swap Parameters:** Adjusting parameters like swappiness and dirty ratios can help control when and how much swapping occurs.

#### Conclusion

Swapping and swap space management are essential components of the Linux memory management system, allowing the operating system to extend its available memory using disk storage. By understanding the mechanisms of swapping, configuring and managing swap space, and utilizing tuning parameters, Linux can achieve efficient and effective memory management. While swapping introduces performance trade-offs, proper management and tuning can mitigate these impacts, ensuring that the system remains responsive and efficient. The exploration of swapping and swap space management in this chapter provides a deep understanding of how Linux handles memory pressure and sustains system performance in variable workloads.

### Handling Out-of-Memory Situations

Effective memory management is crucial for the stability and performance of an operating system. Despite advanced techniques like paging and swapping, there are scenarios where the system may exhaust its physical memory and swap space. In such cases, the operating system must have robust mechanisms to handle out-of-memory (OOM) situations to maintain system integrity and minimize disruption. This chapter explores the strategies and mechanisms employed by the Linux operating system to handle OOM conditions with scientific precision and rigour.

#### Memory Pressure and Out-of-Memory (OOM) Conditions

Memory pressure occurs when the demand for memory exceeds the available physical memory, causing the system to resort to swapping and eventually leading to an out-of-memory condition. Memory pressure is a dynamic state that can be caused by several factors, including:

- **High System Load:** Multiple processes consuming large amounts of memory simultaneously.
- **Memory Leaks:** Processes that continuously allocate memory without releasing it.
- **Insufficient Physical Memory:** Systems with inadequate RAM for their current workload.

When the system is under significant memory pressure and both physical memory and swap space are exhausted, the Linux kernel must take decisive actions to recover from the OOM condition. This is where the OOM killer comes into play.

#### The OOM Killer

The OOM killer is a kernel mechanism designed to free up memory by terminating one or more processes when the system runs critically low on memory. The primary goal of the OOM killer is to relieve memory pressure and allow the system to continue functioning.

##### Invocation of the OOM Killer

The OOM killer is invoked when the Linux kernel detects that it can no longer satisfy memory allocation requests and no recoverable pages are available. This detection is part of the kernel's memory management subsystem, which continuously monitors memory usage and assesses the system's ability to fulfill memory allocation requests.

##### OOM Score and Victim Selection

When invoked, the OOM killer selects one or more processes to terminate based on their OOM score. The OOM score is a heuristic value calculated for each process, reflecting its likelihood of being targeted by the OOM killer. Factors influencing the OOM score include:

- **Memory Usage:** Processes consuming large amounts of memory receive higher OOM scores.
- **Runtime:** Processes running for a longer period tend to have lower OOM scores.
- **Process Priority:** Higher priority processes (e.g., system processes) have lower OOM scores.
- **User Preferences:** Users or system administrators can adjust the OOM score of specific processes using the `oom_score_adj` parameter.

Here's an example of adjusting the OOM score for a specific process:

```bash
# Increase the OOM score adjust value by 100 for a process with PID 1234
echo 100 | sudo tee /proc/1234/oom_score_adj
```

By adjusting the `oom_score_adj` value, administrators can influence the OOM killer’s decision-making process.

##### Process Termination and System Continuation

Once a victim process is selected, the OOM killer forcibly terminates it to free up memory. This involves sending a SIGKILL signal to the selected process, ensuring immediate termination. The memory released by terminating the process is then reallocated to other processes or kernel tasks that were previously unable to obtain memory.

The kernel logs the details of the OOM killer's actions, including the identity of the terminated process and the reasons for its selection, to help administrators diagnose and understand the OOM event:

```bash
# View the system log to examine OOM killer actions
dmesg | grep -i 'killed process'
```

#### Advanced OOM Handling Strategies

While the OOM killer provides a basic mechanism to handle OOM conditions, there are more advanced strategies and tools available to manage memory pressure more gracefully.

##### Memory Overcommitment

Linux supports memory overcommitment, a technique where the system allows processes to allocate more memory than is physically available. Overcommitment relies on the fact that processes often do not use all the memory they allocate. The kernel provides several overcommitment strategies, controlled by the `vm.overcommit_memory` parameter:

- **0 (heuristic overcommitment):** The kernel heuristically decides whether to allow memory allocation based on the available memory and the committed memory.
- **1 (always overcommit):** The system allows all memory allocations, regardless of the available memory.
- **2 (strict overcommitment):** Memory allocations are only allowed if sufficient memory and swap space are available.

Adjusting the overcommitment strategy can help manage memory pressure more effectively:

```bash
# Set the overcommitment strategy to heuristic
sudo sysctl vm.overcommit_memory=0
```

##### Cgroups (Control Groups)

Control Groups (cgroups) provide a mechanism to limit, prioritize, and account for the resource usage of processes. Cgroups can be used to allocate fixed memory limits to groups of processes, preventing any single group from consuming all available memory and causing an OOM condition.

To create and manage a cgroup:

1. **Create a Cgroup:**
   - Create a cgroup directory under the memory subsystem.

   ```bash
   sudo mkdir /sys/fs/cgroup/memory/my_cgroup
   ```

2. **Set Memory Limits:**
   - Set the memory limit for the cgroup.

   ```bash
   echo 512M | sudo tee /sys/fs/cgroup/memory/my_cgroup/memory.limit_in_bytes
   ```

3. **Add Processes to the Cgroup:**
   - Add process IDs (PIDs) to the cgroup.

   ```bash
   echo <PID> | sudo tee /sys/fs/cgroup/memory/my_cgroup/cgroup.procs
   ```

4. **Monitor Memory Usage:**
   - Monitor the memory usage of the cgroup.

   ```bash
   cat /sys/fs/cgroup/memory/my_cgroup/memory.usage_in_bytes
   ```

By using cgroups, administrators can enforce memory limits on specific sets of processes, reducing the likelihood of system-wide OOM conditions.

##### OOM Notifiers

The Linux kernel supports OOM notifiers, which are hooks that notify user-space daemons when an OOM condition is imminent. These notifiers allow custom actions to be taken in response to OOM conditions, such as adjusting memory usage, freeing up resources, or gracefully shutting down non-essential services.

To set up an OOM notifier, user-space daemons can monitor `/proc/meminfo` for low memory warnings or use the `cgroups` memory pressure notification feature.

Here's an example of a simple script that monitors low memory and logs a warning:

```bash
#!/bin/bash

# Threshold for available memory in KB
THRESHOLD=100000

while true; do
    AVAILABLE=$(grep MemAvailable /proc/meminfo | awk '{print $2}')
    if [ "$AVAILABLE" -lt "$THRESHOLD" ]; then
        echo "Warning: Low memory - ${AVAILABLE} KB available" >> /var/log/low_memory.log
    fi
    sleep 10
done
```

This script continuously monitors the available memory and logs a warning if it falls below the specified threshold.

##### User-Space OOM Handling

In addition to the kernel’s OOM killer, custom user-space OOM handling daemons can be implemented to provide more controlled and predictable OOM handling. These daemons can use policies and rules to terminate specific processes, release resources, or take other actions when memory pressure is detected.

Projects like `earlyoom` are examples of user-space OOM handling solutions. `earlyoom` monitors memory usage and triggers early intervention before the kernel OOM killer is invoked, aiming to improve the system’s responsiveness under heavy memory pressure.

#### Conclusion

Out-of-memory situations represent critical conditions that require immediate and effective handling to maintain system stability. The Linux operating system employs a multifaceted approach to manage OOM conditions, from the kernel’s OOM killer to advanced techniques like memory overcommitment, cgroups, OOM notifiers, and user-space OOM handling. By understanding and leveraging these mechanisms, administrators can effectively manage memory pressure and ensure the system remains responsive and stable even under high demand. The detailed exploration in this chapter provides a comprehensive understanding of how Linux handles OOM conditions, contributing to the broader knowledge of memory management strategies in modern operating systems.

