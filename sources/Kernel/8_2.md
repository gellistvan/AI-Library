\newpage

## 26. Packet Handling and Routing

Networking is an integral part of modern operating systems, enabling the seamless transfer of data across diverse systems and networks. In this chapter, we delve into the intricacies of packet handling and routing within the Linux kernel. Understanding how Linux processes incoming and outgoing packets is pivotal for comprehending higher-level networking protocols and services. We commence with packet reception and transmission, exploring how data packets are received by the kernel, processed, and ultimately dispatched to their destinations. Next, we demystify the routing subsystem, elucidating how the kernel determines the optimal paths for packet traversal across interconnected networks. Finally, we shed light on the sophisticated mechanisms of Netfilter and firewall implementations, which offer robust control over packet filtering and network security. This chapter aims to provide a comprehensive understanding of core networking components in the Linux kernel, equipping you with the knowledge to navigate and manipulate the kernel’s networking stack effectively.

### Packet Reception and Transmission

#### Introduction

Packet reception and transmission are cornerstone functionalities of networking subsystems in any operating system. In Linux, these tasks are intricately managed by the kernel to ensure efficient and reliable data transfer across diverse networks. This subchapter examines the comprehensive lifecycle of packets as they traverse the Linux kernel, from their initial reception by network interface controllers (NICs) to their eventual dispatch onto the network. We delve into the underlying data structures, key functions, and processing pathways that facilitate these operations with scientific rigor and exhaustive detail.

#### Packet Reception

##### Network Interface Controllers (NICs)

The journey of a packet within the Linux kernel begins at the network interface controller (NIC). NICs are hardware components responsible for physically interfacing with the network medium (e.g., Ethernet, Wi-Fi). They include various features like Direct Memory Access (DMA), interrupt generation, and buffer management, which are crucial for the efficient handling of network data.

##### Interrupt Handling

When a NIC receives a packet, it typically triggers a hardware interrupt. The Interrupt Service Routine (ISR) associated with the NIC is then executed by the kernel. This ISR is tasked with transferring the packet from the NIC’s buffer to a pre-allocated memory region in the kernel space, often using DMA techniques to minimize CPU overhead. In scenarios where interrupt-driven I/O is insufficient due to high traffic, the kernel can employ NAPI (New API) to mitigate interrupt handling overhead.

##### Allocating Skbuff Structures

In Linux, the fundamental data structure for representing network packets is the `sk_buff` (socket buffer). The `sk_buff` structure encapsulates metadata about the packet, including pointers to the data buffer, protocol headers, and various flags.

```C
struct sk_buff {
   struct sk_buff *next;
   struct sk_buff *prev;
   struct sock *sk;
   struct net_device *dev;
   char cb[48];
   unsigned int len;
   unsigned char *data;
   unsigned char *head;
   unsigned char *end;
   unsigned char *tail;
};
```

When a packet is received, a new `sk_buff` instance is allocated, and the packet data is copied from the DMA buffer into the socket buffer. The socket buffer management subsystem ensures that all received packets are efficiently queued for further processing.

#### Packet Processing Pathways

##### Layer 2 Processing

The first stage of packet processing occurs at the Data Link Layer (Layer 2 of the OSI model). Here, the packet’s Ethernet frame is inspected. The destination MAC address is checked to determine if the packet is intended for the host. If the packet is not filtered out, it is handed over to the appropriate network layer (Layer 3) handler based on its EtherType field, which indicates the protocol encapsulated within the frame (e.g., IPv4, IPv6).

##### Layer 3 Processing

At the Network Layer (Layer 3), the packet undergoes IP processing if it is an IP packet. This involves the following steps:

1. **IP Header Verification**: The IP header is validated for integrity, including checksum verification.
2. **Forwarding Decision**: The destination IP address is checked to determine if the packet is for the local host or needs to be forwarded to another network. If forwarding is required, the routing table is consulted to determine the next-hop address.
3. **Fragmentation Handling**: If the packet is too large for the network’s Maximum Transmission Unit (MTU), it may be fragmented into smaller packets that can traverse the network without issues.

##### Layer 4 Processing

The Transport Layer (Layer 4) comes into play for protocols like TCP, UDP, and ICMP. At this stage, the kernel checks the protocol-specific headers:

1. **UDP Processing**: For User Datagram Protocol (UDP) packets, the kernel verifies the UDP header checksum and delivers the packet to the appropriate socket.
2. **TCP Processing**: For Transmission Control Protocol (TCP) packets, the kernel handles more complex tasks, such as sequence number verification, acknowledgment processing, and, if necessary, reassembly of fragmented segments.
3. **ICMP Handling**: Internet Control Message Protocol (ICMP) packets are processed for network diagnostics and error reporting.

##### Netfilter Hooks

Throughout the packet reception pathway, various Netfilter hooks are invoked. Netfilter is a powerful packet filtering and manipulation framework within the Linux kernel. It allows for the efficient implementation of firewalls, NAT (Network Address Translation), and packet logging.

The primary Netfilter hooks involved in packet reception are:

- **NF_IP_PRE_ROUTING**: Invoked before any routing decisions are made.
- **NF_IP_LOCAL_IN**: Invoked for packets destined for the local host.
- **NF_IP_FORWARD**: Invoked for packets that will be forwarded to another host.

Users can manipulate packet flow at these hooks via iptables or nftables rules.

#### Packet Transmission

The packet transmission pathway in the Linux kernel mirrors the reception pathway but operates in reverse, from higher to lower layers.

##### Socket Layer

Transmission begins at the socket layer, where user-space applications send data using various system calls (`send()`, `sendto()`, `sendmsg()`). The data is encapsulated into `sk_buff` structures and passed down to the appropriate transport layer protocol implementations (TCP, UDP).

##### Transport Layer

At the transport layer, the protocol-specific headers are created:

1. **UDP Transmission**: For UDP, a lightweight header is appended, checksums are computed, and the packet is handed to the IP layer.
2. **TCP Transmission**: For TCP, more extensive operations are performed, including segmenting the data into appropriate sizes, managing the sequence and acknowledgment numbers, ensuring reliability with retransmissions if necessary, and then passing the segments to the IP layer.

##### Network Layer

The Network Layer (Layer 3) is where IP packets are formed. The kernel attaches the necessary IP headers, computes checksums, handles fragmentation if the packet size exceeds the MTU, and determines the next-hop address from the routing table if the packet needs to be forwarded.

##### Data Link Layer

The Data Link Layer (Layer 2) processes the finalized IP packets by encapsulating them within Ethernet frames. This involves adding Ethernet headers and, if required, performing address resolution via the ARP (Address Resolution Protocol) to map IP addresses to MAC addresses on the local network.

##### NIC Buffering and Transmission

The final stage involves transferring the packet from the kernel to the NIC. Leveraging NIC features like DMA, the `sk_buff` data is efficiently copied to the NIC’s transmission buffer. The NIC then handles the actual transmission over the physical network media.

Just as in reception, Netfilter hooks are available in the transmission pathway:

- **NF_IP_LOCAL_OUT**: Invoked for packets generated by the local host.
- **NF_IP_POST_ROUTING**: Invoked after routing decisions have been made but before the packet is handed to the NIC.

#### Transmission Scheduling

Transmission scheduling is managed by the Traffic Control (tc) subsystem, which includes queuing disciplines (qdiscs) and classes. Qdiscs manage how packets are queued for transmission, offering mechanisms for prioritization, shaping, and policing of outgoing traffic.

Common qdiscs include:
- **pfifo_fast**: A basic First-in-First-out (FIFO) queue with three priority bands.
- **htb (Hierarchical Token Bucket)**: A complex qdisc enabling hierarchical bandwidth allocation.

#### Summary

Packet handling within the Linux kernel involves a series of meticulously orchestrated processes, from the moment packets are received by the NICs to their dispatch back onto the network. The kernel's network stack employs well-defined data structures like `sk_buff`, combined with interrupt handling, DMA, and Netfilter hooks, to manage packet flow efficiently and securely. Understanding these processes at a detailed level is pivotal for developing networking functionalities, diagnosing issues, and optimizing performance in Linux-based systems.

### Routing Subsystem

#### Introduction

Routing is a fundamental component of networking that determines the exact path data packets take from their source to their destination across interconnected networks. In the Linux kernel, the routing subsystem is responsible for managing this complex task, leveraging a combination of algorithms, data structures, and routing tables to determine optimal paths. This subchapter offers a granular look into the internal workings of the Linux routing subsystem, exploring its design, key functions, policies, and how it interfaces with other networking components.

#### Fundamental Concepts

##### Routing Tables

Routing decisions in the Linux kernel are primarily based on routing tables, which store a collection of routes that dictate how packets should be forwarded. Each entry in a routing table includes information such as the destination network, gateway, subnet mask, interface, and various metrics.

The routing table can be viewed using the `ip route show` command:

```bash
$ ip route show
default via 192.168.1.1 dev eth0 
192.168.1.0/24 dev eth0  proto kernel  scope link  src 192.168.1.10 
```

##### The RIB and FIB

The Linux kernel utilizes two primary data structures for routing:

1. **Routing Information Base (RIB)**: The RIB is a high-level representation of the routing table, including all routes from various sources like static configurations, dynamic routing protocols, and policy-based routing rules.
2. **Forwarding Information Base (FIB)**: The FIB is a streamlined version of the RIB optimized for fast lookups, used by the kernel to make forwarding decisions for every incoming and outgoing packet.

##### Route Caches

For performance optimization, the Linux routing subsystem employs a route cache mechanism. Route caches store recently used routes, allowing the kernel to quickly retrieve routing information without querying the entire routing table for every packet. This cache improves throughput especially in high-traffic scenarios, although modern Linux kernels have moved towards more efficient direct FIB lookups to avoid cache-related inconsistencies and overheads.

#### Routing Decision Process

The routing decision process involves multiple steps and factors, predominantly focusing on matching the packet’s destination address with the most specific route in the routing table.

##### Matching Criteria

When a packet arrives, the kernel uses several matching criteria to identify the appropriate route:
- **Longest Prefix Match**: The kernel matches the destination IP address of the packet with the most specific network in the routing table, determined by the longest subnet mask.
- **Route Metrics and Priorities**: If multiple routes match the destination address, metrics and priorities are used to select the optimal route. Lower metric values indicate higher preference.

##### Route Lookup and Selection

The `fib_lookup` function plays a central role in the route lookup process:

```C
int fib_lookup(struct flowi *flp, struct fib_result *res)
{
   // Implementation details
   // flp: Flow information including destination address
   // res: Structure to store the resulting route
   
    // Perform the route lookup in the FIB and store the result in res
   // Return 0 on success, non-zero on failure
}
```

The lookup process scrutinizes the flow information, typically including the destination IP address, transport layer ports, network interface, and possibly other criteria defined by routing policies.

#### Policy-Based Routing

Policy-based routing allows the routing decisions to be influenced by additional rules beyond simple destination matching. These policies can consider factors such as source address, TOS (Type of Service) fields, and other packet attributes.

Policy-based routing rules are managed using the `ip rule` command:

```bash
$ ip rule add from 192.168.1.0/24 table 100
$ ip route add default via 10.0.0.1 table 100
```

The `ip rule` command demonstrates how to add a rule that directs packets originating from the 192.168.1.0/24 network to use the routes specified in table 100.

##### Rule Priority

Each policy routing rule has a priority, determining its order of evaluation. Lower priority values indicate higher precedence. The kernel processes these rules in ascending order based on priority until a match is found.

#### Nexthop Resolution

Once a route is selected, the kernel resolves the nexthop address, which specifies the immediate next device the packet should be forwarded to. If the nexthop is specified as an IP address, additional ARP (Address Resolution Protocol) or ND (Neighbor Discovery for IPv6) lookups might be necessary to obtain the corresponding MAC address.

#### Routing Protocols Integration

While the Linux kernel itself does not include dynamic routing protocols, it integrates seamlessly with user-space routing daemons like Quagga, BIRD, and FRR (Free Range Routing). These daemons implement protocols such as OSPF (Open Shortest Path First), BGP (Border Gateway Protocol), and RIP (Routing Information Protocol) to dynamically manage routing tables.

##### Interaction with Routing Daemons

Routing daemons communicate with the kernel using the `rt_netlink` interface, part of the Linux Netlink socket API. This interface allows routing daemons to add, modify, and delete routes in the kernel's routing tables.

```C
struct rtmsg {
   unsigned char rtm_family;
   unsigned char rtm_dst_len;
   unsigned char rtm_src_len;
   unsigned char rtm_tos;
   unsigned char rtm_table;
   unsigned char rtm_protocol;
   unsigned char rtm_scope;
   unsigned char rtm_type;
   unsigned rt_rtm_flags;
};
```

The `rtmsg` structure is utilized in Netlink messages to convey routing information between user-space daemons and the kernel.

#### Load Balancing and Multipath Routing

Linux supports multipath routing and load balancing techniques, where multiple routes to the same destination can coexist. This functionality allows for traffic distribution across multiple paths, enhancing throughput and reliability.

Multipath routes can be configured via `ip route`:

```bash
$ ip route add default nexthop via 192.168.1.1 dev eth0 weight 1 nexthop via 192.168.1.2 dev eth1 weight 2
```

In this example, traffic is distributed between two nexthops, with a higher weight indicating a proportionally greater share of the traffic.

#### Advanced Routing Features

##### Source-Specific Routing

Source-specific routing allows routes to be specified based not only on the destination address but also on the source address. This feature is useful in multi-homed hosts or complex network scenarios.

##### VRF (Virtual Routing and Forwarding)

VRF provides multiple independent routing tables within a single system. Each VRF instance can have its own routes, interfaces, and policies, enabling network segmentation and isolation.

```bash
$ ip link add vrf-blue type vrf table 100
$ ip addr add 192.168.10.1/24 dev eth0
$ ip link set dev eth0 master vrf-blue
$ ip route add default via 192.168.10.254 table 100
```

The above commands configure a VRF named "vrf-blue" and associate an interface with it.

#### Performance Optimization

##### Efficient Data Structures

Recent Linux kernels optimize route lookups using sophisticated data structures like radix trees and hash tables to ensure rapid access times, even in large routing tables.

##### Protocol Offload

Advanced NICs support protocol offloading, where certain routing functions can be offloaded to the NIC hardware, reducing CPU load and enhancing performance.

##### Route Aggregation

Route aggregation or summarization reduces the size of the routing table by combining multiple routes into a single entry, thus speeding up the lookup process and reducing memory usage.

#### Debugging and Monitoring

Efficient debugging and monitoring tools are crucial for diagnosing and resolving routing issues. The `ip route get` command helps evaluate route selection for a given destination:

```bash
$ ip route get 8.8.8.8
8.8.8.8 via 192.168.1.1 dev eth0 src 192.168.1.10
cache
```

Additionally, the `traceroute` utility helps trace the route packets take to their destination, useful for diagnosing network path issues.

#### Summary

The routing subsystem in the Linux kernel is a sophisticated and highly optimized component, essential for the efficient and reliable forwarding of packets across networks. By employing meticulously designed data structures, integration with dynamic routing protocols, and leveraging advanced features like policy-based routing and multipath support, the Linux routing subsystem ensures optimal routing decisions. A comprehensive understanding of this subsystem is pivotal for networking professionals and developers looking to harness Linux's networking capabilities to their fullest extent.

### Netfilter and Firewall Implementation

#### Introduction

Netfilter is a powerful and flexible framework within the Linux kernel designed for packet filtering, network address translation (NAT), and other packet mangling operations. It serves as the underlying infrastructure for firewall utilities such as iptables, nftables, and ufw (Uncomplicated Firewall). This chapter explores the Netfilter architecture, its key components, hook points, connection tracking, and the implementation of firewalls with scientific rigor and detail.

#### Overview of Netfilter

Netfilter operates within the Linux kernel to provide hooks at various points in the network stack. These hooks allow modules (such as iptables rules) to register callback functions that can inspect, modify, or drop packets as they traverse the network stack.

The primary functionalities of Netfilter include:

1. **Packet Filtering**: Controlling packet flow based on predefined rules.
2. **Network Address Translation (NAT)**: Modifying network address information in packet headers.
3. **Connection Tracking**: Maintaining the state of network connections across multiple packets.

Netfilter's architecture is designed to be modular and extensible, supporting a wide range of networking operations.

#### Netfilter Hook Points

Netfilter defines five primary hook points where packet processing functions can be registered. These hooks correspond to specific stages in the packet lifecycle:

1. **NF_INET_PRE_ROUTING**: Invoked before routing decisions are made, applicable to incoming packets.
2. **NF_INET_LOCAL_IN**: Invoked after routing, for packets intended for the local machine.
3. **NF_INET_FORWARD**: Invoked for packets being forwarded to another interface (neither destined for nor originating from the local machine).
4. **NF_INET_LOCAL_OUT**: Invoked for packets generated by the local machine, before routing.
5. **NF_INET_POST_ROUTING**: Invoked after routing, before packets are sent out on the network.

Each hook is associated with specific networking operations and is defined in header files such as `<linux/netfilter.h>`.

#### Core Data Structures

##### nf_hook_ops

The `nf_hook_ops` structure defines the hook functions that will be executed at specific hook points:

```C
struct nf_hook_ops {
   struct list_head list;
   nf_hookfn *hook;
   pf_type pf;
   unsigned int hooknum;
   int priority;
};
```

- **hook**: Pointer to the function that will be called at the hook point.
- **pf**: Protocol family (e.g., PF_INET for IPv4).
- **hooknum**: The hook point (e.g., NF_INET_PRE_ROUTING).
- **priority**: Priority of the hook function, determining the order of execution when multiple functions are registered at the same point.

##### nf_conntrack

Connection tracking in Netfilter is managed using the `nf_conntrack` structure, which maintains state information for each connection.

```C
struct nf_conntrack {
   struct hlist_node hnode;
   spinlock_t lock;
   refcount_t refcnt;
   unsigned long timeout;
   struct nf_conntrack_tuple tuplehash[IP_CT_DIR_MAX];
};
```

- **hnode**: Hash table node for efficient lookups.
- **lock**: Spinlock for concurrency control.
- **refcnt**: Reference count for the structure.
- **timeout**: Timeout for the connection entry.
- **tuplehash**: Array of connection tuples representing source/destination address and ports.

#### Packet Filtering

Packet filtering is one of the core functionalities provided by Netfilter. It operates by applying a series of rules to each packet, determining whether the packet should be allowed, dropped, or modified.

##### iptables

Historically, iptables has been the primary interface for configuring Netfilter rules. An iptables rule specifies criteria for matching packets and actions to be taken on matching packets.

##### Chain and Table Structure

iptables organizes rules into chains, which are ordered sequences of rules. Chains are grouped into tables, each serving a particular purpose:

- **filter**: The default table, used for packet filtering.
- **nat**: Used for network address translation.
- **mangle**: Used for packet alteration.
- **raw**: Used for connection tracking exemptions.

##### Rule Definition

Rules specify criteria and actions. Criteria can be based on attributes such as source/destination IP address, port number, protocol, and interface. Actions include ACCEPT, DROP, REJECT, and others.

Here is an example of defining iptables rules in Bash:

```bash
# Allow incoming HTTP traffic
iptables -A INPUT -p tcp --dport 80 -j ACCEPT

# Drop all other incoming traffic
iptables -A INPUT -j DROP
```

In this example, the first rule allows incoming TCP traffic on port 80 (HTTP), while the second rule drops all other incoming traffic.

#### Network Address Translation (NAT)

NAT modifies packet headers to facilitate scenarios like IP masquerading, port forwarding, and load balancing. Netfilter supports various NAT types:

- **Source NAT (SNAT)**: Changes the source IP address of outgoing packets.
- **Destination NAT (DNAT)**: Changes the destination IP address of incoming packets.
- **Masquerading**: A form of SNAT where the source IP is dynamically assigned, often used for sharing a single public IP address among multiple devices.

##### Configuring SNAT and DNAT

Using iptables rules to configure SNAT:

```bash
# Source NAT for outbound traffic
iptables -t nat -A POSTROUTING -o eth0 -j SNAT --to-source 203.0.113.25
```

And for DNAT:

```bash
# Destination NAT for incoming traffic directed to an internal server
iptables -t nat -A PREROUTING -p tcp --dport 8080 -j DNAT --to-destination 192.168.1.100:80
```

The first command applies SNAT to outgoing traffic on the `eth0` interface, changing the source address to `203.0.113.25`. The second command applies DNAT to incoming TCP traffic on port 8080, redirecting it to an internal server at `192.168.1.100` on port 80.

#### Connection Tracking

Netfilter's connection tracking subsystem maintains the state of connections traversing the firewall. This stateful firewalling allows rules to be applied based on the state of the connection (e.g., new, established, related).

```bash
# Allow incoming traffic for established connections
iptables -A INPUT -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT

# Allow outgoing traffic for all connections
iptables -A OUTPUT -j ACCEPT
```

The `-m conntrack --ctstate` module matches packets based on their connection state. In the example, incoming traffic for existing connections is allowed, while all outgoing traffic is permitted.

#### Netfilter Implementation

##### Registering and Deregistering Hooks

Modules can register and deregister hook functions at specific Netfilter hook points using `nf_register_net_hook` and `nf_unregister_net_hook` respectively.

```C
static struct nf_hook_ops mynfho;

int __init my_module_init(void)
{
   mynfho.hook = my_hook_func;
   mynfho.hooknum = NF_INET_PRE_ROUTING;
   mynfho.pf = PF_INET;
   mynfho.priority = NF_IP_PRI_FIRST;
   nf_register_net_hook(&init_net, &mynfho);
   return 0;
}

void __exit my_module_exit(void)
{
   nf_unregister_net_hook(&init_net, &mynfho);
}

module_init(my_module_init);
module_exit(my_module_exit);
```

In this example, a module registers a hook function `my_hook_func` at the `NF_INET_PRE_ROUTING` stage for IPv4 packets. The hook function is executed with the highest priority (`NF_IP_PRI_FIRST`).

##### Hook Function Implementation

The hook function processes packets and returns a decision:

```C
unsigned int my_hook_func(void *priv,
               struct sk_buff *skb,
               const struct nf_hook_state *state)
{
   // Parse packet data and make a decision
   struct iphdr *ip_header = (struct iphdr *)skb_network_header(skb);
   if (ip_header->protocol == IPPROTO_TCP) {
      // Process TCP packets
   }
   return NF_ACCEPT; // Accept the packet
}
```

The hook function examines the protocol field in the IP header and processes TCP packets differently from other protocols. The final decision is returned using the appropriate Netfilter verdicts (e.g., `NF_ACCEPT`, `NF_DROP`, `NF_QUEUE`, `NF_STOLEN`).

#### Transition to Nftables

nftables is the modern replacement for iptables, offering a more efficient and flexible way to manage packet filtering and NAT. nftables leverages the Netfilter framework but introduces a new user-space utility and an improved kernel interface.

##### nftables Syntax and Usage

nftables simplifies rule definitions with a more consistent syntax:

```bash
# Define a table and chain
nft add table inet my_table
nft add chain inet my_table my_chain { type filter hook input priority 0 \; }

# Add rules
nft add rule inet my_table my_chain tcp dport 80 accept
nft add rule inet my_table my_chain drop
```

The commands define a table `my_table`, add an input chain `my_chain`, and add rules to accept TCP traffic on port 80 while dropping all other traffic.

##### Advantages of nftables

- **Unified API**: nftables provides a single API for packet filtering, NAT, and packet mangling.
- **Improved Performance**: nftables uses a more efficient bytecode interpreter, reducing overhead.
- **Extensible Syntax**: nftables supports complex constructs such as sets, maps, and concatenations, enabling more flexible rule definitions.

#### Firewalls with Netfilter

Firewalls built on Netfilter offer robust security measures for Linux systems by controlling inbound and outbound traffic based on predefined rules.

##### Basic Firewall Configuration

A simple firewall script using iptables might look like this:

```bash
#!/bin/bash
# Flush existing rules
iptables -F
iptables -t nat -F
iptables -t mangle -F
iptables -X

# Default policy to drop all traffic
iptables -P INPUT DROP
iptables -P FORWARD DROP
iptables -P OUTPUT ACCEPT

# Allow loopback traffic
iptables -A INPUT -i lo -j ACCEPT
iptables -A OUTPUT -o lo -j ACCEPT

# Allow established connections
iptables -A INPUT -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT

# Allow HTTP and HTTPS traffic
iptables -A INPUT -p tcp --dport 80 -j ACCEPT
iptables -A INPUT -p tcp --dport 443 -j ACCEPT

# Drop all other input traffic
iptables -A INPUT -j DROP
```

The script sets default policies, configures basic rules for loopback and established connections, and allows HTTP/HTTPS traffic.

##### Advanced Firewall Configuration

Advanced firewalls leverage features like rate limiting, logging, and custom chains:

```bash
# Create a custom chain for logging and dropping
iptables -N LOG_DROP
iptables -A LOG_DROP -m limit --limit 5/min -j LOG --log-prefix "DROP: "
iptables -A LOG_DROP -j DROP

# Example rule using the custom chain
iptables -A INPUT -p tcp --dport 22 -m state --state NEW -m recent --set
iptables -A INPUT -p tcp --dport 22 -m state --state NEW -m recent --update --seconds 60 --hitcount 4 -j LOG_DROP
```

In this example, a custom chain `LOG_DROP` logs and drops packets with a rate limit. A rule in the INPUT chain uses this custom chain to rate-limit SSH connections.

#### Summary

Netfilter is a comprehensive framework within the Linux kernel that provides robust capabilities for packet filtering, NAT, and packet mangling. By leveraging its architecture through hooks, connection tracking, and extensibility, administrators and developers can implement sophisticated firewall rules and network configurations. The transition from iptables to nftables marks a significant evolution in managing Linux firewall policies, offering improved performance and flexibility. Understanding the detailed workings of Netfilter is crucial for harnessing its full potential to secure and optimize network traffic in Linux systems.

