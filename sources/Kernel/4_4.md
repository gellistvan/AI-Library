\newpage

## 12. Advanced Memory Management

In this chapter, we delve into advanced memory management concepts that play a crucial role in optimizing system performance and resource utilization in the Linux kernel. We begin with an exploration of Non-Uniform Memory Access (NUMA), a memory architecture that enhances performance by optimizing memory access times based on the proximity of memory to CPUs. Following this, we examine Huge Pages and Transparent Huge Pages (THP), mechanisms that reduce the overhead of memory management by handling larger memory blocks in a more efficient manner. Finally, we address Memory Compaction and Defragmentation, techniques designed to alleviate fragmentation within the system’s memory, ensuring that large contiguous blocks of memory are available for allocation. These advanced topics are essential for anyone seeking to understand and leverage the full potential of the Linux kernel's memory management capabilities.

### Non-Uniform Memory Access (NUMA)

Non-Uniform Memory Access (NUMA) is a memory design used in multiprocessor systems where memory access times vary depending on the memory's location relative to the processor accessing it. In NUMA architectures, the system is divided into multiple “nodes,” each consisting of one or more processors and local memory. Memory located within a node can be accessed faster by the processors within that same node, compared to memory located in another node.

NUMA is a critical architectural consideration in modern multi-core and multi-processor systems because it influences memory access latency and bandwidth, directly impacting application performance. In this subchapter, we will explore the inner workings of NUMA, its advantages and disadvantages, how it is implemented in the Linux kernel, and the various techniques used to optimize performance in NUMA systems.

#### 1. NUMA Architecture

NUMA architectures essentially divide the entire memory pool into segments attached to individual processors or groups of processors. Each segment is called a "NUMA node." The key distinction in NUMA systems is that memory access time depends on the proximity of the memory to the processor. Local memory (memory within the same node as the processor) can be accessed faster than remote memory (memory in another node).

##### 1.1. Basic NUMA Model

In a basic NUMA model:
- Each processor has its own local memory.
- Each processor can access memory within its local node with lower latency.
- Accessing memory in a remote node entails higher latency due to additional coordination and communication overhead between nodes.

```text
+----------------+        +----------------+
|    Node 1      |        |    Node 2      |
|                |        |                |
| +------------+ |        | +------------+ |
| | Processor 1 | |        | | Processor 2 | |
| +------------+ |        | +------------+ |
| Example Memory| <-----> | Example Memory|
+----------------+        +----------------+
```

#### 2. NUMA in the Linux Kernel

The Linux kernel provides extensive support for NUMA, enabling efficient memory management and optimizing performance for applications running on NUMA architectures.

##### 2.1. NUMA Configuration

To fully utilize NUMA, you need to configure the kernel appropriately by enabling NUMA support. This is usually done during kernel compilation, ensuring that the system can identify and manage multiple NUMA nodes.

The kernel also provides a command `numactl` to control NUMA policy for processes or shared memory and obtain information about the system's NUMA topology.

```bash
numactl --hardware
```

This command displays the NUMA topology of the system, including the number of nodes, CPUs, and memory distribution across nodes.

##### 2.2. Memory Allocation in NUMA

The kernel uses various strategies for memory allocation in NUMA systems to improve performance:

- **Node-local Allocation**: The kernel attempts to allocate memory from the local node (node where the request originates from).
- **Interleaving**: Allocating memory pages across multiple nodes, distributing load and reducing contention.
- **Fallback Policy**: If the local node's memory is exhausted, the allocation may fall back to a remote node.

To influence memory allocation behavior, Linux provides several tunables via sysfs, such as:

```bash
/sys/devices/system/node/nodeX/meminfo
```

Here, `nodeX` corresponds to a specific NUMA node.

#### 3. Optimizing Performance on NUMA Systems

Performance optimization in NUMA systems centers around ensuring that memory access patterns are aligned with the NUMA topology. There are strategies and tools that can help in achieving this:

##### 3.1. Process Affinity

Binding processes to specific CPUs and memory nodes can significantly reduce remote memory access latency. The `numactl` command allows you to run a command with a specified NUMA policy.

```bash
numactl --cpunodebind=0 --membind=0 ./application
```

This example binds the application process to CPUs in node 0 and allocates memory from node 0.

##### 3.2. Memory Binding APIs

POSIX-compliant systems, such as Linux, offer APIs for more fine-grained control over memory allocation in NUMA environments:

- **mbind**: Sets memory bind policy for a given memory range.
- **set_mempolicy**: Sets the NUMA memory policy for the calling thread.
- **move_pages**: Moves individual memory pages to a different node.

Here is an example using `mbind` in C++:

```cpp
#include <numaif.h>
#include <errno.h>
#include <iostream>

void bind_memory_to_node(void* addr, size_t length, int node) {
   unsigned long nodemask = 1 << node;
   if (mbind(addr, length, MPOL_BIND, &nodemask, 8, 0) != 0) {
      std::cerr << "mbind error: " << strerror(errno) << std::endl;
   } else {
      std::cout << "Memory bound to node " << node << std::endl;
   }
}
```

##### 3.3. NUMA Balancing

NUMA balancing is a kernel feature that automatically moves tasks and memory pages across nodes to optimize performance.

- **AutoNUMA**: The kernel periodically scans a process’s address space and migrates pages to nodes closer to the executing CPU.
  
You can enable or disable NUMA balancing via sysctl:

```bash
sysctl -w kernel.numa_balancing=1
```

#### 4. NUMA-aware Algorithms and Data Structures

Algorithms and data structures can be designed to be NUMA-aware to improve performance:

- **Partitioning data based on NUMA nodes**: This minimizes cross-node memory access.
- **Replicating frequently accessed data**: Reduces contention and remote access latency.
- **Designing cache-friendly data structures**: Contiguous memory access patterns can significantly boost performance in NUMA systems.

#### 5. Challenges in NUMA Systems

While NUMA optimizations can bring substantial performance benefits, they also introduce complexities:

- **Code complexity**: Writing NUMA-aware code requires a deep understanding of the hardware and memory access patterns.
- **NUMA-induced contention**: Poorly managed NUMA policies can lead to performance degradation, especially under high load scenarios.
- **Debugging**: Diagnosing performance issues in NUMA systems can be challenging due to the intricate interactions between hardware and software.

#### Conclusion

NUMA represents a significant advancement in memory architecture, tailored for the increasing demands of multi-core and multi-processor systems. By understanding and leveraging the Linux kernel's NUMA capabilities, it is possible to optimize both system and application performance substantially. This chapter has provided a detailed examination of NUMA architecture, memory allocation strategies, performance optimization techniques, and the challenges associated with NUMA systems. Armed with this knowledge, you can effectively navigate the complexities of NUMA, ensuring efficient and performant memory management in your Linux environments.

### Huge Pages and Transparent Huge Pages (THP)

In modern computing systems, efficient memory management is critical to achieving high performance, especially as applications scale and require ever-larger memory allocations. Two pivotal techniques in the Linux kernel that significantly enhance memory management efficiency are Huge Pages and Transparent Huge Pages (THP). These mechanisms reduce memory management overhead and improve performance by handling larger memory blocks. This chapter provides an in-depth exploration of Huge Pages and Transparent Huge Pages, including their benefits, implementation, configuration, and practical usage in Linux.

#### 1. Huge Pages

Huge Pages are a feature in modern processors and operating systems that allow the allocation of memory in larger chunks than the standard page size. The standard page size on x86 systems is typically 4KB, but Huge Pages can be much larger, commonly 2MB or 1GB. By using larger page sizes, Huge Pages reduce the number of page table entries (PTEs) required for memory mapping, thereby decreasing the overhead of memory management.

##### 1.1. Benefits of Huge Pages

1. **Reduced Page Table Overhead**: With larger page sizes, fewer page table entries are needed to map the same amount of memory, which reduces the memory and computational overhead associated with maintaining the page tables.
  
2. **Improved TLB Efficiency**: The Translation Lookaside Buffer (TLB) caches page table entries to speed up virtual-to-physical address translation. Using Huge Pages means fewer entries need to be cached, increasing the probability of TLB hits and thereby reducing address translation latency.
  
3. **Enhanced Performance for Large Memory Applications**: Applications that require large memory allocations, such as databases and scientific computing applications, benefit significantly from Huge Pages due to reduced paging overhead.

##### 1.2. Configuring Huge Pages in Linux

To use Huge Pages, the Linux kernel must be configured to support them. The following steps outline the configuration process.

###### 1.2.1. Kernel Configuration

Ensure that Huge Page support is enabled in the kernel. This is typically the case in most modern Linux distributions, but you can verify it in the kernel configuration:

```bash
grep HUGETLB /boot/config-$(uname -r)
```

###### 1.2.2. Reserving Huge Pages

Huge Pages must be reserved by the system administrator. This can be done dynamically or at boot time.

**Reserving Huge Pages dynamically**:

```bash
echo 2048 > /proc/sys/vm/nr_hugepages
```

This command reserves 2048 Huge Pages of the default size (usually 2MB).

**Reserving Huge Pages at boot time**:

Add the following parameter to the kernel command line (e.g., in `/etc/default/grub`):

```bash
default_hugepagesz=2M hugepagesz=2M hugepages=2048
```

After modifying the GRUB configuration file, update the GRUB settings:

```bash
sudo update-grub
```

###### 1.2.3. Using Huge Pages in Applications

Applications need to be explicitly designed to use Huge Pages. For example, a C++ application can allocate Huge Pages using the `mmap` system call with the `MAP_HUGETLB` flag.

```cpp
#include <sys/mman.h>
#include <iostream>

int main() {
   const size_t length = 2 * 1024 * 1024; // 2MB
   void* addr = mmap(NULL, length, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS | MAP_HUGETLB, -1, 0);
   if (addr == MAP_FAILED) {
      std::cerr << "mmap failed" << std::endl;
      return 1;
   }
   
    // Use the allocated memory
   
    munmap(addr, length);
   return 0;
}
```

##### 1.3. Monitoring and Managing Huge Pages

Tools like `hugetlbfs` and `/proc/meminfo` provide insights into Huge Page usage.

```bash
grep Huge /proc/meminfo
```

This command outputs the current Huge Page statistics, such as the total number of Huge Pages, free Huge Pages, and reserved Huge Pages.

#### 2. Transparent Huge Pages (THP)

Transparent Huge Pages (THP) extend the concept of Huge Pages by automatically managing page allocation and promotion without requiring explicit application changes. With THP, the Linux kernel attempts to use Huge Pages transparently, reducing the need for manual intervention and simplifying application development.

##### 2.1. Benefits of Transparent Huge Pages

1. **Ease of Use**: THP abstracts the complexity of managing Huge Pages, allowing applications to benefit from larger page sizes without modification.
   
2. **Dynamic Management**: THP dynamically promotes and demotes memory regions to and from Huge Pages based on system usage patterns and performance heuristics.
   
3. **Performance Improvements**: THP can significantly improve performance for applications with large memory footprints by reducing page table overhead and improving TLB efficiency, similar to manually managed Huge Pages.

##### 2.2. Configuring Transparent Huge Pages

THP can be configured at runtime using the `/sys` filesystem. The default state of THP is usually enabled, but system administrators can adjust the settings based on workload requirements.

###### 2.2.1. Checking THP Status

To check the current status of THP:

```bash
cat /sys/kernel/mm/transparent_hugepage/enabled
cat /sys/kernel/mm/transparent_hugepage/defrag
```

The output will indicate the current mode, such as `always`, `madvise`, or `never`.

###### 2.2.2. Adjusting THP Settings

To enable or disable THP, you can write to the appropriate files:

**Enable THP**:

```bash
echo always > /sys/kernel/mm/transparent_hugepage/enabled
```

**Disable THP**:

```bash
echo never > /sys/kernel/mm/transparent_hugepage/enabled
```

**Using `madvise`**:

```bash
echo madvise > /sys/kernel/mm/transparent_hugepage/enabled
```

With `madvise`, the kernel uses Huge Pages only for memory regions explicitly marked by the application with the `madvise` system call.

##### 2.3. Utilizing Transparent Huge Pages in Applications

Applications can benefit from THP without any modification if the system is configured to use THP. However, developers can provide hints using `madvise` to control the behavior more precisely.

```cpp
#include <sys/mman.h>
#include <sys/types.h>
#include <unistd.h>
#include <iostream>

int main() {
   const size_t length = 2 * 1024 * 1024 * 10; // 20MB
   void* addr = mmap(NULL, length, PROT_READ | PROT_WRITE, MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
   if (addr == MAP_FAILED) {
      std::cerr << "mmap failed" << std::endl;
      return 1;
   }

   if (madvise(addr, length, MADV_HUGEPAGE) != 0) {
      std::cerr << "madvise failed" << std::endl;
      munmap(addr, length);
      return 1;
   }

   // Use the allocated memory
   
    munmap(addr, length);
   return 0;
}
```

##### 2.4. Monitoring and Managing Transparent Huge Pages

THP usage can be monitored via `/proc` and `/sys` interfaces. The `/proc/meminfo` file provides metrics related to THP usage.

```bash
grep -i huge /proc/meminfo
```

Kernel logs and debugging tools can provide additional insights into THP behavior and performance impact.

#### 3. Use Cases and Performance Considerations

##### 3.1. Suitable Workloads

Workloads that benefit most from Huge Pages and THP include:

- **Databases**: Large in-memory databases see reduced latency and improved throughput.
- **High-Performance Computing (HPC)**: Scientific applications with substantial memory footprints gain performance improvements.
- **Virtualization**: Guest operating systems can benefit from Huge Pages to minimize memory management overhead.
- **Large-scale Data Processing**: Applications like big data analytics that operate on vast datasets.

##### 3.2. Performance Impact

While Huge Pages and THP can improve performance, they are not a panacea. Proper configuration and tuning are required to maximize benefits. Potential downsides include:

- **Increased Memory Consumption**: In some cases, using Huge Pages can lead to higher memory consumption due to internal fragmentation.
- **Allocation Overhead**: The initial allocation of Huge Pages can incur overhead, affecting system performance if not managed properly.
- **Compatibility Issues**: Not all applications are suitable for Huge Pages, and improper use can lead to performance degradation.

##### 3.3. Best Practices

- **Profile Before and After**: Always profile application performance before and after enabling Huge Pages or THP to assess the impact.
- **Tune Based on Workload**: Different workloads have different memory access patterns. Tune Huge Pages and THP settings based on the specific needs of the application.
- **Monitor System Metrics**: Regularly monitor system metrics to detect any adverse effects or bottlenecks introduced by Huge Pages and THP.

#### Conclusion

Huge Pages and Transparent Huge Pages represent advanced techniques in memory management, offering substantial performance benefits by reducing the overhead associated with handling small memory pages. By configuring and using these features appropriately, system administrators and developers can enhance the efficiency and performance of applications with large memory requirements. This chapter has provided a thorough examination of Huge Pages and THP, including their benefits, configuration, and practical usage, as well as the associated performance considerations and best practices. Understanding and leveraging these features can significantly contribute to the optimization of memory management in Linux systems.

### Memory Compaction and Defragmentation

In the complex world of memory management, one of the critical challenges is dealing with memory fragmentation. Over time, memory allocations and deallocations can lead to a situation where, even though there is enough total free memory, it is split into small, non-contiguous blocks that are unusable for large allocations. To address this issue, the Linux kernel employs memory compaction and defragmentation techniques. This chapter provides a comprehensive exploration of these techniques, their benefits, implementation in the Linux kernel, configuration, and practical implications.

#### 1. Understanding Memory Fragmentation

Memory fragmentation occurs in two forms: **external fragmentation** and **internal fragmentation**.

1. **External Fragmentation**: This type occurs when free memory is scattered in small blocks across the system, preventing large contiguous memory allocations.
   
2. **Internal Fragmentation**: This type occurs when the allocated memory blocks are larger than required, leading to wasted space within allocated regions.

Both forms of fragmentation can severely impact system performance by preventing efficient memory utilization and causing higher latency for memory allocations.

#### 2. Memory Compaction

Memory compaction is a process whereby the kernel attempts to defragment physical memory by moving active pages closer together, thereby creating larger contiguous blocks of free memory. It is an essential mechanism to mitigate external fragmentation.

##### 2.1. Benefits of Memory Compaction

1. **Increased Availability of Contiguous Memory**: By defragmenting memory, compaction ensures that larger contiguous blocks are available for allocation, which is particularly beneficial for kernel allocations, huge pages, and I/O operations that require large buffers.
   
2. **Improved Performance**: Access patterns that benefit from contiguous memory blocks, such as large file I/O, streaming, and certain algorithmic operations, see performance improvements.
   
3. **Better Utilization of Available Memory**: Maximizing the utilization of available memory reduces the need for swapping or reclamation, thereby improving overall system performance.

##### 2.2. Implementation in the Linux Kernel

The Linux kernel implements memory compaction through two primary mechanisms: **proactive compaction** and **on-demand compaction**.

###### 2.2.1. Proactive Compaction

Proactive compaction runs periodically in the background, attempting to defragment memory even when there is no immediate request for large contiguous memory.

The kernel parameter `vm.compact_memory` triggers proactive compaction:

```bash
echo 1 > /proc/sys/vm/compact_memory
```

You can set proactive compaction to run periodically by adjusting the kernel tunables:

```bash
echo 300 > /proc/sys/vm/compact_proactiveness
```

Here, `300` represents the periodic frequency of proactive compaction. Setting it to `0` disables proactive compaction.

###### 2.2.2. On-demand Compaction

On-demand compaction occurs when the kernel needs to fulfill a request for contiguous memory but finds that memory is fragmented. The kernel triggers compaction at this point to free up the necessary space.

The kernel can be configured to set the thresholds for on-demand compaction using the following parameters:

```bash
echo 1 > /proc/sys/vm/compact_unevictable
```

This setting ensures that even "unevictable" pages (those that cannot be swapped out, such as mlocked pages) are considered during compaction to maximize contiguous memory availability.

##### 2.3. Memory Compaction Algorithms

The kernel employs sophisticated algorithms to achieve efficient compaction. These include:

- **Free Page Scanner**: This component scans memory zones to identify free pages that meet the criteria for compaction.
- **Migration Scanner**: It moves allocated pages to different locations to consolidate free space.
- **Allocation and Migration Policies**: These policies dictate when and how pages are moved, based on factors such as page activity, eviction cost, and system load.

###### 2.3.1. Free Page Scanner

The Free Page Scanner identifies areas of memory where compaction should occur. It looks for contiguous blocks of free memory that can be merged and areas where large allocations are needed.

```text
+-------------------+--------+          +---------------------+
|  Free Page Block  | Scanned|   ---->  |  Free Page Block    |
+-------------------+--------+          +---------------------+
|                   | Pages  |          |                     |
+-------------------+--------+          +---------------------+
| Uncompactible Area|--------+   +-->   | Uncompactible Area  |
+-------------------+--------+   |      +---------------------+
|                   | Scanned|   |      | Allocated Block     |
+-------------------+--------+   |      +---------------------+
               |      |   -> Moved          |
+-------------------+--------+   |      +---------------------+
|   Memory Zone     |--------+---+   -> |   Free Page Block   |
|                   | Scanned|          +---------------------+
+-------------------+--------+
```

###### 2.3.2. Migration Scanner

The Migration Scanner works by moving pages to new locations to consolidate free memory. It leverages the kernel's page migration mechanisms, which are also used in NUMA balancing and memory reclaim.

#### 3. Memory Defragmentation

Memory defragmentation aims to address internal fragmentation by reallocating and resizing memory blocks. This is particularly relevant for applications and data structures with dynamic memory allocation patterns.

##### 3.1. Benefits of Defragmentation

1. **Maximized Memory Utilization**: Defragmentation reduces waste within allocated memory blocks, ensuring higher overall memory utilization.
   
2. **Reduction in Memory Footprint**: By shrinking large blocks to fit actual usage, defragmentation lowers the overall memory footprint of applications.
   
3. **Improved Performance**: Optimized memory allocation and reduced overhead contribute to faster memory operations and better cache utilization.

##### 3.2. Implementation in the Linux Kernel

While the kernel does not perform defragmentation automatically for user-space memory allocations, certain tools and techniques can help mitigate internal fragmentation:

###### 3.2.1. Memory Reclamation

Memory reclamation techniques such as garbage collection and reference counting can help reduce internal fragmentation by reclaiming unused or stale memory blocks.

###### 3.2.2. Application-Level Defragmentation

Applications can periodically trigger defragmentation by reallocating and copying data to contiguous memory blocks. For instance, data structures like hash tables or heaps can be re-allocated periodically to shrink their size and remove fragmentation.

```cpp
#include <vector>
#include <algorithm>
#include <iostream>

template<typename T>
void defragment_vector(std::vector<T>& vec) {
   std::vector<T> new_vec(vec);
   std::swap(vec, new_vec);
   std::cout << "Vector defragmented. New size: " << vec.size() << std::endl;
}
```

#### 4. Configuring and Managing Compaction and Defragmentation

The Linux kernel provides several tunables to manage and customize memory compaction and defragmentation behavior. These tunables can be adjusted dynamically to optimize system performance based on workload characteristics.

##### 4.1. Proactive Compaction Settings

The `vm.compact_proactiveness` parameter controls the frequency of proactive compaction:

```bash
echo 300 > /proc/sys/vm/compact_proactiveness
```

Adjusting this setting impacts the balance between proactive compaction benefits and its performance overhead.

##### 4.2. On-demand Compaction

The `vm.compact_unevictable` parameter enables or disables the inclusion of unevictable pages in compaction decisions:

```bash
echo 1 > /proc/sys/vm/compact_unevictable
```

This setting can help increase the availability of contiguous memory by considering a broader range of pages during compaction.

##### 4.3. Monitoring Memory Compaction

Memory compaction activity can be monitored through kernel logs and metrics available in `/proc` and `/sys` filesystems. For example:

```bash
cat /proc/vmstat | grep compact
```

This command provides information on the success and failure rates of compaction attempts, helping administrators fine-tune settings for optimal performance.

#### 5. Use Cases and Performance Considerations

##### 5.1. Use Cases

Memory compaction and defragmentation are particularly beneficial for workloads that require large contiguous memory allocations, such as:

- **Database Management Systems**: Large databases benefit from reduced paging and improved query performance.
- **High-Performance Computing (HPC)**: Scientific simulations with substantial memory demands improve execution times.
- **Virtual Machines**: Hypervisors allocate large blocks of memory to guest virtual machines, necessitating contiguous memory.

##### 5.2. Performance Considerations

While memory compaction and defragmentation provide significant benefits, they also come with potential performance impacts:

- **Compaction Overhead**: Frequent compaction can introduce latency, especially during peak load periods.
- **Trade-offs in Memory Utilization**: Overzealous compaction and defragmentation may lead to higher memory overhead due to increased copy operations.

##### 5.3. Best Practices

To maximize the benefits of memory compaction and defragmentation:

- **Profile and Evaluate**: Continuously profile application performance and monitor system metrics before and after adjustments to compaction settings.
- **Tune for Workload**: Customize settings based on specific workload characteristics and performance goals.
- **Monitor Overhead**: Keep an eye on the overhead introduced by compaction activities and adjust parameters to balance performance gains and operational costs.

#### Conclusion

Memory compaction and defragmentation are vital techniques in the Linux kernel's arsenal for managing memory fragmentation and ensuring efficient utilization of available memory. By understanding these techniques and configuring them appropriately, system administrators and developers can achieve significant performance improvements, particularly for applications with large memory allocations and dynamic memory usage patterns. This chapter has provided an in-depth exploration of compaction and defragmentation, including their benefits, implementation details, configuration options, and best practices. Utilizing these techniques effectively can lead to a more robust and efficient memory management strategy in Linux systems.

